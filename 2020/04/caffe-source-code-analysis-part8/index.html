<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="111qqz的小窝"><meta property="og:type" content="article"><meta property="og:image" content="https://111qqz.github.io/img/2.png"><meta property="twitter:image" content="https://111qqz.github.io/img/2.png"><meta name=title content="caffe 源码学习笔记(8) loss function"><meta property="og:title" content="caffe 源码学习笔记(8) loss function"><meta property="twitter:title" content="caffe 源码学习笔记(8) loss function"><meta name=description content><meta property="og:description" content><meta property="twitter:description" content><meta property="twitter:card" content="summary"><meta name=keyword content="ACM,111qqz,商汤科技,hust,华中科技大学"><link rel="shortcut icon" href=/img/favicon.ico><title>caffe 源码学习笔记(8) loss function-111qqz的小窝</title><link rel=canonical href=/2020/04/caffe-source-code-analysis-part8><link rel=stylesheet href=/css/iDisqus.min.css><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hux-blog.min.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/zanshang.css><link href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css rel=stylesheet type=text/css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/docco.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/css/hux-blog.min.css><link rel=stylesheet href=/css/hux-blog.min-custom.css></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a class=navbar-brand href=/>111qqz的小窝</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>Home</a></li><li><a href=/categories/acm/>ACM-ICPC</a></li><li><a href=/categories/deep-learning/>深度学习</a></li><li><a href=/categories/mooc/>公开课</a></li><li><a href=/categories/%e5%85%b6%e4%bb%96/>其他</a></li><li><a href=/top/about/>ABOUT</a></li><li><a href=/search>SEARCH <img src=/img/search.png height=15 style=cursor:pointer alt=Search></a></li></ul></div></div></div></nav><script>var $body=document.body;var $toggle=document.querySelector('.navbar-toggle');var $navbar=document.querySelector('#huxblog_navbar');var $collapse=document.querySelector('.navbar-collapse');$toggle.addEventListener('click',handleMagic)
function handleMagic(e){if($navbar.className.indexOf('in')>0){$navbar.className=" ";setTimeout(function(){if($navbar.className.indexOf('in')<0){$collapse.style.height="0px"}},400)}else{$collapse.style.height="auto"
$navbar.className+=" in";}}</script><style type=text/css>header.intro-header{background-image:url(/img/2.png)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/caffe title=caffe>caffe</a></div><h1>caffe 源码学习笔记(8) loss function</h1><h2 class=subheading></h2><span class=meta>Posted by
111qqz
on
Saturday, April 18, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><header><h2>TOC</h2></header><nav id=TableOfContents><ol><li><a href=#背景>背景</a></li><li><a href=#euclidean-loss-l2-loss>Euclidean Loss (L2 loss)</a></li><li><a href=#multinomiallogisticloss>MultinomialLogisticLoss</a></li><li><a href=#hinge-loss>hinge-loss</a></li><li><a href=#参考资料>参考资料</a></li></ol></nav><h2 id=背景>背景</h2><p>虽然不太care 训练的过程，<del>但是由于容易看懂的layer都看得差不多了</del> 所以打算看一下这些loss function.</p><h2 id=euclidean-loss-l2-loss>Euclidean Loss (L2 loss)</h2><p><img src=https://i.loli.net/2020/04/18/voT7jGEF1BabmpL.png alt="L2 loss.png"></p><p>一般用于“real-valued regression tasks” 。　比如之前的项目上用的人脸年龄模型，就是用了这个Loss</p><p>这个loss没什么额外的参数，实现也很简单。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++>
<span style=color:#66d9ef>template</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>typename</span> Dtype<span style=color:#f92672>&gt;</span>
<span style=color:#66d9ef>void</span> EuclideanLossLayer<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>:</span><span style=color:#f92672>:</span>Reshape(
  <span style=color:#66d9ef>const</span> vector<span style=color:#f92672>&lt;</span>Blob<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>*</span><span style=color:#f92672>&gt;</span><span style=color:#f92672>&amp;</span> bottom, <span style=color:#66d9ef>const</span> vector<span style=color:#f92672>&lt;</span>Blob<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>*</span><span style=color:#f92672>&gt;</span><span style=color:#f92672>&amp;</span> top) {
  LossLayer<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>:</span><span style=color:#f92672>:</span>Reshape(bottom, top);
  CHECK_EQ(bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>count(<span style=color:#ae81ff>1</span>), bottom[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>count(<span style=color:#ae81ff>1</span>))
      <span style=color:#f92672>&lt;</span><span style=color:#f92672>&lt;</span> <span style=color:#e6db74></span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>Inputs must have the same dimension.</span><span style=color:#e6db74>&#34;</span>;
  diff_.ReshapeLike(<span style=color:#f92672>*</span>bottom[<span style=color:#ae81ff>0</span>]);
}

<span style=color:#66d9ef>template</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>typename</span> Dtype<span style=color:#f92672>&gt;</span>
<span style=color:#66d9ef>void</span> EuclideanLossLayer<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>:</span><span style=color:#f92672>:</span>Forward_cpu(<span style=color:#66d9ef>const</span> vector<span style=color:#f92672>&lt;</span>Blob<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>*</span><span style=color:#f92672>&gt;</span><span style=color:#f92672>&amp;</span> bottom,
    <span style=color:#66d9ef>const</span> vector<span style=color:#f92672>&lt;</span>Blob<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>*</span><span style=color:#f92672>&gt;</span><span style=color:#f92672>&amp;</span> top) {
  <span style=color:#66d9ef>int</span> count <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>count();
  caffe_sub(
      count,
      bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>cpu_data(),
      bottom[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>cpu_data(),
      diff_.mutable_cpu_data());
  Dtype dot <span style=color:#f92672>=</span> caffe_cpu_dot(count, diff_.cpu_data(), diff_.cpu_data());
  Dtype loss <span style=color:#f92672>=</span> dot <span style=color:#f92672>/</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>num() <span style=color:#f92672>/</span> Dtype(<span style=color:#ae81ff>2</span>);
  top[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>mutable_cpu_data()[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> loss;
}
</code></pre></div><p>注意要Reshape中，要求两个bottom blob（第一个为预测结果，第二个为gt)的batch维度可以不同,计算时按照预测结果的个数为准。</p><p>以及，Mean Square Error(MSE)　和L2 loss的差别只是一个系数1/2,可以放在一起说明。</p><p><img src=https://miro.medium.com/max/291/1*w9HQ9w30lpr_HYZqeRhm4A.png alt=MSE></p><h2 id=multinomiallogisticloss>MultinomialLogisticLoss</h2><p><img src=https://i.loli.net/2020/04/18/MdSGtml3JOYUzX6.png alt=MultinomialLogisticLoss.png></p><p>用于单标签多分类任务。　输入的预测blob是一个(通常是经过softmax后)得到的概率分布。</p><p>注意 <strong>&rdquo; The SoftmaxWithLossLayer should be preferred over separate SoftmaxLayer + MultinomialLogisticLossLayer as its gradient computation is more numerically stable."</strong></p><p>这个后面会提到。</p><p>同样，这个loss也没什么额外参数，
forward也很简单</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++>
<span style=color:#66d9ef>template</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>typename</span> Dtype<span style=color:#f92672>&gt;</span>
<span style=color:#66d9ef>void</span> MultinomialLogisticLossLayer<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>:</span><span style=color:#f92672>:</span>Forward_cpu(
    <span style=color:#66d9ef>const</span> vector<span style=color:#f92672>&lt;</span>Blob<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>*</span><span style=color:#f92672>&gt;</span><span style=color:#f92672>&amp;</span> bottom, <span style=color:#66d9ef>const</span> vector<span style=color:#f92672>&lt;</span>Blob<span style=color:#f92672>&lt;</span>Dtype<span style=color:#f92672>&gt;</span><span style=color:#f92672>*</span><span style=color:#f92672>&gt;</span><span style=color:#f92672>&amp;</span> top) {
  <span style=color:#66d9ef>const</span> Dtype<span style=color:#f92672>*</span> bottom_data <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>cpu_data();
  <span style=color:#75715e>//  predictions,ＮxCxHxW
</span><span style=color:#75715e></span>  <span style=color:#66d9ef>const</span> Dtype<span style=color:#f92672>*</span> bottom_label <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>cpu_data();
  <span style=color:#75715e>// labels, Nx1x1x1
</span><span style=color:#75715e></span>  <span style=color:#66d9ef>int</span> num <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>num();
  <span style=color:#66d9ef>int</span> dim <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>count() <span style=color:#f92672>/</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>num();
  <span style=color:#75715e>// dim 表示类别总数
</span><span style=color:#75715e></span>  Dtype loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
  <span style=color:#75715e>//  bottom_data[i * dim + label]表示的是第i张图的gt对应的位置的预测分数
</span><span style=color:#75715e></span>  <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> num; <span style=color:#f92672>+</span><span style=color:#f92672>+</span>i) {
    <span style=color:#66d9ef>int</span> label <span style=color:#f92672>=</span> <span style=color:#66d9ef>static_cast</span><span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span>(bottom_label[i]);
    Dtype prob <span style=color:#f92672>=</span> std<span style=color:#f92672>:</span><span style=color:#f92672>:</span>max(
        bottom_data[i <span style=color:#f92672>*</span> dim <span style=color:#f92672>+</span> label], Dtype(kLOG_THRESHOLD));
    loss <span style=color:#f92672>-</span><span style=color:#f92672>=</span> log(prob);
  }
  top[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>mutable_cpu_data()[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> loss <span style=color:#f92672>/</span> num;
}

</code></pre></div><p>需要注意的已经写在注释中了。
kLOG_THRESHOLD是一个很小的值，1E-20,防止log0炸掉</p><h2 id=hinge-loss>hinge-loss</h2><p><img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/a5f42d461f1a28b27438e8f1641e042ff2e40102 alt="hinge loss"></p><p>中文似乎叫"合页损失函数&rdquo;,因为很像一本书打开的样子(?</p><p><img src=https://pic1.zhimg.com/80/v2-3c6aa9626ee8e4609b0d7c5712baf624_720w.jpg alt=hinge-loss></p><p>主要用在svm中。。。所以其实没在工作中实际接触过这个。</p><p>这个loss的特点是更加严格。。严格的意思是说，分类的结果不仅需要正确，而且需要置信度足够高，损失才会是0.</p><p><img src=https://i.loli.net/2020/04/18/ZgKC2ONAmuTRYJ7.png alt=hinge_loss_caffe.png></p><p>forward代码稍微不是那么直接</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++>
  <span style=color:#66d9ef>const</span> Dtype<span style=color:#f92672>*</span> bottom_data <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>cpu_data();
  Dtype<span style=color:#f92672>*</span> bottom_diff <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>mutable_cpu_diff();
  <span style=color:#66d9ef>const</span> Dtype<span style=color:#f92672>*</span> label <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>cpu_data();
  <span style=color:#66d9ef>int</span> num <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>num();
  <span style=color:#66d9ef>int</span> count <span style=color:#f92672>=</span> bottom[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>count();
  <span style=color:#66d9ef>int</span> dim <span style=color:#f92672>=</span> count <span style=color:#f92672>/</span> num;
  <span style=color:#75715e>// dim是类别数
</span><span style=color:#75715e></span>
  caffe_copy(count, bottom_data, bottom_diff);
  <span style=color:#75715e>// 把预测值bottom_data拷贝到bottom_diff
</span><span style=color:#75715e></span>  <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> num; <span style=color:#f92672>+</span><span style=color:#f92672>+</span>i) {
    bottom_diff[i <span style=color:#f92672>*</span> dim <span style=color:#f92672>+</span> <span style=color:#66d9ef>static_cast</span><span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;</span>(label[i])] <span style=color:#f92672>*</span><span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>;
  }
  <span style=color:#75715e>// 把gt的那一类的预测分数取了相反数(这里相当于算了condition函数，只不过差个负号)
</span><span style=color:#75715e></span>
  <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> num; <span style=color:#f92672>+</span><span style=color:#f92672>+</span>i) {
    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; j <span style=color:#f92672>&lt;</span> dim; <span style=color:#f92672>+</span><span style=color:#f92672>+</span>j) {
      bottom_diff[i <span style=color:#f92672>*</span> dim <span style=color:#f92672>+</span> j] <span style=color:#f92672>=</span> std<span style=color:#f92672>:</span><span style=color:#f92672>:</span>max(
        Dtype(<span style=color:#ae81ff>0</span>), <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> bottom_diff[i <span style=color:#f92672>*</span> dim <span style=color:#f92672>+</span> j]);
    }


</code></pre></div><p>先算了一个bottom_diff。　这里其实是相当于算了condition function的结果，只不过差了一个负号。　后面我们也可以看到</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++>
bottom_diff[i <span style=color:#f92672>*</span> dim <span style=color:#f92672>+</span> j] <span style=color:#f92672>=</span> std<span style=color:#f92672>:</span><span style=color:#f92672>:</span>max(
        Dtype(<span style=color:#ae81ff>0</span>), <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> bottom_diff[i <span style=color:#f92672>*</span> dim <span style=color:#f92672>+</span> j]);

</code></pre></div><p>中是计算了<strong>bottom_diff[i * dim + j] = std::max(
Dtype(0), 1 + bottom_diff[i * dim + j]);</strong></p><p>还值得一提的是,caffe的hinge loss 同时支持L1 norm和L2 norm(在L2-SVM中使用)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>message</span> <span style=color:#a6e22e>HingeLossParameter</span> {<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>  <span style=color:#66d9ef>enum</span> Norm {<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    L1 <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>;<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    L2 <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>;<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>  }<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>  <span style=color:#75715e>// Specify the Norm to use L1 or L2
</span><span style=color:#75715e></span>  <span style=color:#66d9ef>optional</span> Norm norm <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> [<span style=color:#66d9ef>default</span> <span style=color:#f92672>=</span> L1];<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>}<span style=color:#960050;background-color:#1e0010>
</span></code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++>
  Dtype<span style=color:#f92672>*</span> loss <span style=color:#f92672>=</span> top[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>mutable_cpu_data();
  <span style=color:#66d9ef>switch</span> (<span style=color:#66d9ef>this</span><span style=color:#f92672>-</span><span style=color:#f92672>&gt;</span>layer_param_.hinge_loss_param().norm()) {
  <span style=color:#66d9ef>case</span> HingeLossParameter_Norm_L1:
    loss[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> caffe_cpu_asum(count, bottom_diff) <span style=color:#f92672>/</span> num;
    <span style=color:#66d9ef>break</span>;
  <span style=color:#66d9ef>case</span> HingeLossParameter_Norm_L2:
    loss[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> caffe_cpu_dot(count, bottom_diff, bottom_diff) <span style=color:#f92672>/</span> num;
    <span style=color:#66d9ef>break</span>;
  <span style=color:#66d9ef>default</span><span style=color:#f92672>:</span>
    LOG(FATAL) <span style=color:#f92672>&lt;</span><span style=color:#f92672>&lt;</span> <span style=color:#e6db74></span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>Unknown Norm</span><span style=color:#e6db74>&#34;</span>;
  }
}

</code></pre></div><h2 id=参考资料>参考资料</h2><ul><li><p><a href=https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0>5 Regression Loss Functions All Machine Learners Should Know</a></p></li><li><p><a href=https://en.wikipedia.org/wiki/Hinge_loss>Hinge loss</a></p></li></ul><div class="entry-shang text-center"><p>「真诚赞赏，手留余香」</p><button class="zs show-zs btn btn-bred">赞赏支持</button></div><div class=zs-modal-bg></div><div class=zs-modal-box><div class=zs-modal-head><button type=button class=close>×</button>
<span class=author><a href=https://111qqz.github.io><img src=/img/favicon.png>111qqz的小窝</a></span><p class=tip><i></i><span>真诚赞赏，手留余香</span></p></div><div class=zs-modal-body><div class=zs-modal-btns><button class="btn btn-blink" data-num=2>2元</button>
<button class="btn btn-blink" data-num=5>5元</button>
<button class="btn btn-blink" data-num=10>10元</button>
<button class="btn btn-blink" data-num=50>50元</button>
<button class="btn btn-blink" data-num=100>100元</button>
<button class="btn btn-blink" data-num=1>任意金额</button></div><div class=zs-modal-pay><button class="btn btn-bred" id=pay-text>2元</button><p>使用<span id=pay-type>微信</span>扫描二维码完成支付</p><img src=/img/reward/wechat-2.png id=pay-image></div></div><div class=zs-modal-footer><label><input type=radio name=zs-type value=wechat class=zs-type checked><span><span class=zs-wechat><img src=/img/reward/wechat-btn.png></span></label>
<label><input type=radio name=zs-type value=alipay class=zs-type class=zs-alipay><img src=/img/reward/alipay-btn.png></span></label></div></div><script type=text/javascript src=/js/reward.js></script><hr><ul class=pager><li class=previous><a href=/2020/04/caffe-source-code-analysis-part7 data-toggle=tooltip data-placement=top title="caffe 源码学习笔记(7) slice layer">&larr;
Previous Post</a></li><li class=next><a href=/2020/05/retinanet-notes data-toggle=tooltip data-placement=top title="Focal Loss for Dense Object Detection(RetinaNet) 学习笔记">Next
Post &rarr;</a></li></ul><div id=git-comments></div><link rel=stylesheet href=https://wzxjayce.github.io/gitment.css><script src=https://wzxjayce.github.io/gitment.js></script><script>var gitment=new Gitment({id:decodeURI(window.location.pathname),owner:'111qqz',repo:'111qqz.github.io',oauth:{client_id:'8839ce5e58d5197e2490',client_secret:'2d475a8a7e27a8b509847a6c60f692b8cbaa274e',}})
gitment.render('git-comments')</script></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/bfs title=bfs>bfs</a>
<a href=/tags/binary-search title=binary-search>binary-search</a>
<a href=/tags/brute-force title=brute-force>brute-force</a>
<a href=/tags/c++ title=c++>c++</a>
<a href=/tags/caffe title=caffe>caffe</a>
<a href=/tags/dfs title=dfs>dfs</a>
<a href=/tags/dp title=dp>dp</a>
<a href=/tags/greedy title=greedy>greedy</a>
<a href=/tags/hash title=hash>hash</a>
<a href=/tags/km title=km>km</a>
<a href=/tags/kmp title=kmp>kmp</a>
<a href=/tags/leetcode title=leetcode>leetcode</a>
<a href=/tags/math title=math>math</a>
<a href=/tags/number-theory title=number-theory>number-theory</a>
<a href=/tags/rmq title=rmq>rmq</a>
<a href=/tags/sg%E5%87%BD%E6%95%B0 title=sg函数>sg函数</a>
<a href=/tags/stl title=stl>stl</a>
<a href=/tags/tensorflow title=tensorflow>tensorflow</a>
<a href=/tags/%E4%BA%8C%E5%88%86%E5%9B%BE%E6%9C%80%E4%BD%B3%E5%8C%B9%E9%85%8D title=二分图最佳匹配>二分图最佳匹配</a>
<a href=/tags/%E5%88%86%E5%9D%97 title=分块>分块</a>
<a href=/tags/%E5%89%8D%E7%BC%80%E5%92%8C title=前缀和>前缀和</a>
<a href=/tags/%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95 title=匈牙利算法>匈牙利算法</a>
<a href=/tags/%E5%8C%BA%E9%97%B4dp title=区间dp>区间dp</a>
<a href=/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA title=博弈论>博弈论</a>
<a href=/tags/%E5%90%8E%E7%BC%80%E8%87%AA%E5%8A%A8%E6%9C%BA title=后缀自动机>后缀自动机</a>
<a href=/tags/%E5%9B%BE%E8%AE%BA title=图论>图论</a>
<a href=/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2 title=字符串>字符串</a>
<a href=/tags/%E5%BF%AB%E9%80%9F%E5%B9%82 title=快速幂>快速幂</a>
<a href=/tags/%E6%95%B0%E4%BD%8Ddp title=数位dp>数位dp</a>
<a href=/tags/%E6%9E%84%E9%80%A0 title=构造>构造</a>
<a href=/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84 title=树状数组>树状数组</a>
<a href=/tags/%E6%A6%82%E7%8E%87 title=概率>概率</a>
<a href=/tags/%E6%A8%A1%E6%8B%9F title=模拟>模拟</a>
<a href=/tags/%E6%AF%8D%E5%87%BD%E6%95%B0 title=母函数>母函数</a>
<a href=/tags/%E7%9F%A9%E9%98%B5 title=矩阵>矩阵</a>
<a href=/tags/%E7%A6%BB%E6%95%A3%E5%8C%96 title=离散化>离散化</a>
<a href=/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91 title=线段树>线段树</a>
<a href=/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95 title=计算几何>计算几何</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://111qqz.com>111qqz的wordpress博客</a></li></ul></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href rel=alternate type=application/rss+xml title=111qqz的小窝><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-rss fa-stack-1x fa-inverse"></i></span></a></li><li><a href=mailto:hust.111qqz@gmail.com><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat.jpg><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-wechat fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/111qqz/><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-github fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; 111qqz的小窝 2021<br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> by <a href=https://zhaohuabing.com>Huabing</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function async(u,c){var d=document,t='script',o=d.createElement(t),s=d.getElementsByTagName(t)[0];o.src=u;if(c){o.addEventListener('load',function(e){c(null,e);},false);}
s.parentNode.insertBefore(o,s);}</script><script>if($('#tag_cloud').length!==0){async("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:'#bbbbee',end:'#0085a1'},};$('#tag_cloud a').tagcloud();})}</script><script>async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.js",function(){var $nav=document.querySelector("nav");if($nav)FastClick.attach($nav);})</script></body></html>