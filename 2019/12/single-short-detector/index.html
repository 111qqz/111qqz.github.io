<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="111qqz的小窝">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="https://111qqz.github.io/img/2.png">
    <meta property="twitter:image" content="https://111qqz.github.io/img/2.png" />
    

    
    <meta name="title" content="SSD: Single Shot MultiBox Detector　学习笔记" />
    <meta property="og:title" content="SSD: Single Shot MultiBox Detector　学习笔记" />
    <meta property="twitter:title" content="SSD: Single Shot MultiBox Detector　学习笔记" />
    

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    <meta property="twitter:description" content="" />
    

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="ACM,111qqz,商汤科技,hust,华中科技大学">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>SSD: Single Shot MultiBox Detector　学习笔记-111qqz的小窝</title>

    <link rel="canonical" href="/2019/12/single-short-detector">

    <link rel="stylesheet" href="/css/iDisqus.min.css"/>
	
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    
    <link rel="stylesheet" href="/css/syntax.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">

    
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    
    
    <script src="/js/jquery.min.js"></script>
    
    
    <script src="/js/bootstrap.min.js"></script>
    
    
    <script src="/js/hux-blog.min.js"></script>
	
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/docco.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    <link rel="stylesheet" href="/css/hux-blog.min.css">
    <link rel="stylesheet" href="/css/hux-blog.min-custom.css">
</head>


<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">111qqz的小窝</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/categories/acm">acm</a>
                    </li>
                    
                    <li>
                        <a href="/categories/c&#43;&#43;">c&#43;&#43;</a>
                    </li>
                    
                    <li>
                        <a href="/categories/deep-learning">deep-learning</a>
                    </li>
                    
                    <li>
                        <a href="/categories/%E4%BC%98%E5%8C%96">优化</a>
                    </li>
                    
                    <li>
                        <a href="/categories/%E5%85%B6%E4%BB%96">其他</a>
                    </li>
                    
                    <li>
                        <a href="/categories/%E9%9A%8F%E7%AC%94%E6%9D%82%E8%B0%88">随笔杂谈</a>
                    </li>
                    
                    <li>
                        <a href="/categories/%E9%9D%A2%E8%AF%95">面试</a>
                    </li>
                    
                    
		    
                        <li><a href="/top/about/">ABOUT</a></li>
                    

                    
		    <li>
                        <a href="/search">SEARCH <img src="/img/search.png" height="15" style="cursor: pointer;" alt="Search"></a>
		    </li>
                    
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/img/2.png')
    }
</style>
<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/ssd" title="SSD">
                            SSD
                        </a>
                        
                        <a class="tag" href="/tags/single-state-detector" title="single state detector">
                            single state detector
                        </a>
                        
                    </div>
                    <h1>SSD: Single Shot MultiBox Detector　学习笔记</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
			Posted by 
			
			    111qqz
			 
			on 
			Sunday, December 8, 2019
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <header>
                    <h2>TOC</h2>
                </header>
                <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#概述">概述</a></li>
<li><a href="#基本概念">基本概念</a>
<ul>
<li><a href="#prior-box">prior box</a></li>
<li><a href="#loss">loss</a></li>
<li><a href="#different-scales-of-feature-maps">different scales of feature maps</a></li>
<li><a href="#prototxt">prototxt</a></li>
</ul></li>
<li><a href="#参考资料">参考资料</a></li>
</ul></li>
</ul>
</nav>
                
                

<h2 id="概述">概述</h2>

<p>SSD是一种单阶段目标检测算法．所谓单阶段，是指只使用了一个deep neural network,而不是像faster-rcnn这种两阶段网络．
为什么有了faster-rcnn还要使用SSD? 最主要是慢&hellip;
两阶段网络虽然准确率高，但是在嵌入式等算力不足的设备上做inference速度非常感人，很难达到real time的要求．
（实际业务上也是这样，公有云上的检测模型几乎都是faster-rcnn,而到了一些盒子之类的硬件设备，检测模型就全是SSD等single stage 模型了)</p>

<p>之前一直没有写SSD是因为相比faster rcnn的细节，SSD的问题似乎并不是很多．直到最近转模型的时候被FASF模型的一个细节卡了蛮久，因此决定还是记录一下．</p>

<h2 id="基本概念">基本概念</h2>

<p>这部分描述SSD中涉及到的一些想法．</p>

<h3 id="prior-box">prior box</h3>

<p>prior box的概念其实与faster-rcnn中anchor的概念是一样的，没有本质区别．
与faster-rcnn中的anchor不同的是，SSD会在多个feature map中的每个cell 都生成若干个prior_box.</p>

<p>对于一个特定的feature map,尺寸为m*n,假设有k个prior box,c种类别．
那么feature map的每个location会生成<code>k*(c+4)</code> 个结果，其中c代表每一类的confidence. ４代表相对prior_box中心点的offset.
整个feature_map会生成　<code>kmn(c+4)</code> 个结果．</p>

<p>prior_box的参数选择,以及一些训练有关的细节可以参考原论文,这里不再赘述.
这里主要想强调一下和priox box有关的inference 细节.
主要是decode box的部分.</p>

<p>由于模型输出的bbox其实是相对每个prior_box的offset,不是真正的bbox,因此需要由网络输出的box_pred和prior box得到真正的bbox 坐标.这部分通常称为decode box,其实已经算是后处理部分了.</p>

<p>pytorch中decode box的代码如下:</p>

<pre><code class="language-python"> variance1, variance2 = variance
        cx = box_prior[:, :,
                       0] + box_pred[:, :, 0] * variance1 * box_prior[:, :, 2]
        cy = box_prior[:, :,
                       1] + box_pred[:, :, 1] * variance1 * box_prior[:, :, 3]
        w = box_prior[:, :, 2] + torch.exp(box_pred[:, :, 2] * variance2)
        h = box_prior[:, :, 3] + torch.exp(box_pred[:, :, 3] * variance2)
        x1 = cx - w / 2
        y1 = cy - h / 2
        x2 = w + x1
        y2 = h + y1

</code></pre>

<p>不考虑variance的话,box_prior存储的四个数据按顺序分别为cx,cy,w,h
也就是prior_box的中心点坐标(cx,cy)以及宽和高.</p>

<p>而variance是一个原始paper中没有提到的实现细节.
按照<a href="https://github.com/rykov8/ssd_keras/issues/53"> What is the purpose of the variances?</a> 的说法,
&gt;  Probably, the naming comes from the idea, that the ground truth bounding boxes are not always precise, in other words, they vary from image to image probably for the same object in the same position just because human labellers cannot ideally repeat themselves. Thus, the encoded values are some random values, and we want them to have unit variance that is why we divide by some value.</p>

<p>可以理解成一个用来消除由标注引入的随机因素的手段.</p>

<p>更多bbox encoding/decoding的内容可以参考 <a href="https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/">Bounding Box Encoding and Decoding in Object Detection</a></p>

<p>对应的cuda代码</p>

<pre><code class="language-c++">template &lt;typename Dtype&gt;
__global__ void OneStageDecodeBBoxesSSDKernel_v2(const int nthreads, const Dtype* loc_data,
        const Dtype* prior_data, const Dtype variance1, const Dtype variance2,
        const int num_priors,const bool clip_bbox, Dtype* bbox_loc){

    CUDA_KERNEL_LOOP(index, nthreads) {
        // loc: Batch, num_priors, 4, 1
        // proir: 1, num_priors, 4, 1
        // nthreads = Batch * num_priors * 2
        const int x_or_y = index % 2;
        const int pri_idx = 4 * int( (index %(2*num_priors)) / 2) + x_or_y;
        const int loc_idx = 4 * int(index /2) + x_or_y;
        Dtype box0 =  prior_data[pri_idx];
        box0 = box0 + loc_data[loc_idx] * variance1 * box0; 
        Dtype box2 = prior_data[pri_idx + 2] * exp(variance2* loc_data[loc_idx+2]);
        // box0就是prior box的中心点坐标x或者y,
        // box2 就是prior box的w或者h. 
        // 下面的box0和box2复用了变量,但是实际上分别表示水平或者垂直方向的两个坐标.
        box0 -= box2/2; 
        box2 += box0;
        bbox_loc[loc_idx]  = clip_bbox ? min(max(box0, 0.0),1.0): box0;
        bbox_loc[loc_idx+2] = clip_bbox ? min(max(box2, 0.0),1.0): box2;
    }
}

</code></pre>

<p>如果涉及到部署,一个要注意的细节是,pytorch代码和caffe代码中prior_box 顺序的一致性.</p>

<h3 id="loss">loss</h3>

<p>由于不(wo)涉(bu)及(hui)训练,loss不是关注的重点,简单说一句.
loss function是localization loss 和confidence loss 的加权和.
前半部分用来衡量bbox的loss,后半部分是分数的loss.
其中localization loss 如下图:
<img src="https://i.loli.net/2019/12/08/qtaCuV5vRD3y67n.png" alt="ssd_loc_loss.png" /></p>

<p>而confidence loss就是  softmax loss.</p>

<h3 id="different-scales-of-feature-maps">different scales of feature maps</h3>

<p>神经网络越深的layer有着越大的感受野(receptive fileds),每个feature map cell包含着更抽象的信息. 我们可以用浅层的feature map来检测小物体,用深层的feture map来检测大物体.
<img src="https://i.loli.net/2019/12/08/ZioIuOMHesyCPGV.png" alt="Pyramidal feature hierarchy.png" /></p>

<p>这个想法后面会延伸到FPN,因此这里不详细讲了.</p>

<h3 id="prototxt">prototxt</h3>

<pre><code class="language-protobuf">input: &quot;Input&quot;
input_dim: 1
input_dim: 3
input_dim: 512
input_dim: 512

layer {
  name: &quot;PriorboxData&quot;
  type: &quot;CustomData&quot;
  bottom: &quot;Input&quot;
  top: &quot;PriorboxData&quot;
  window_data_param {
    source: &quot;ssd_prior_box.flat&quot;
  }
}
layer {
  name: &quot;Conv_0&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Input&quot;
  top: &quot;Conv_0&quot;
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Relu_1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_0&quot;
  top: &quot;Conv_0&quot;
}
layer {
  name: &quot;MaxPool_2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;Conv_0&quot;
  top: &quot;MaxPool_2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 0
  }
}
layer {
  name: &quot;Conv_3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;MaxPool_2&quot;
  top: &quot;Conv_3&quot;
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_4&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_3&quot;
  top: &quot;Conv_3&quot;
}
layer {
  name: &quot;Conv_5&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_3&quot;
  top: &quot;Conv_5&quot;
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_6&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_5&quot;
  top: &quot;Conv_5&quot;
}
layer {
  name: &quot;Conv_7&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_5&quot;
  top: &quot;Conv_7&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_8&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;MaxPool_2&quot;
  top: &quot;Conv_8&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_9&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_7&quot;
  bottom: &quot;Conv_8&quot;
  top: &quot;Add_9&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_10&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_9&quot;
  top: &quot;Add_9&quot;
}
layer {
  name: &quot;Conv_11&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_9&quot;
  top: &quot;Conv_11&quot;
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_12&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_11&quot;
  top: &quot;Conv_11&quot;
}
layer {
  name: &quot;Conv_13&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_11&quot;
  top: &quot;Conv_13&quot;
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_14&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_13&quot;
  top: &quot;Conv_13&quot;
}
layer {
  name: &quot;Conv_15&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_13&quot;
  top: &quot;Conv_15&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_16&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_15&quot;
  bottom: &quot;Add_9&quot;
  top: &quot;Add_16&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_17&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_16&quot;
  top: &quot;Add_16&quot;
}
layer {
  name: &quot;Conv_18&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_16&quot;
  top: &quot;Conv_18&quot;
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_19&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_18&quot;
  top: &quot;Conv_18&quot;
}
layer {
  name: &quot;Conv_20&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_18&quot;
  top: &quot;Conv_20&quot;
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_21&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_20&quot;
  top: &quot;Conv_20&quot;
}
layer {
  name: &quot;Conv_22&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_20&quot;
  top: &quot;Conv_22&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_23&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_22&quot;
  bottom: &quot;Add_16&quot;
  top: &quot;Add_23&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_24&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_23&quot;
  top: &quot;Add_23&quot;
}
layer {
  name: &quot;Conv_25&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_23&quot;
  top: &quot;Conv_25&quot;
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_26&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_25&quot;
  top: &quot;Conv_25&quot;
}
layer {
  name: &quot;Conv_27&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_25&quot;
  top: &quot;Conv_27&quot;
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Relu_28&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_27&quot;
  top: &quot;Conv_27&quot;
}
layer {
  name: &quot;Conv_29&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_27&quot;
  top: &quot;Conv_29&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_30&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_23&quot;
  top: &quot;Conv_30&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Add_31&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_29&quot;
  bottom: &quot;Conv_30&quot;
  top: &quot;Add_31&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_32&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_31&quot;
  top: &quot;Add_31&quot;
}
layer {
  name: &quot;Conv_33&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_31&quot;
  top: &quot;Conv_33&quot;
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_34&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_33&quot;
  top: &quot;Conv_33&quot;
}
layer {
  name: &quot;Conv_35&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_33&quot;
  top: &quot;Conv_35&quot;
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_36&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_35&quot;
  top: &quot;Conv_35&quot;
}
layer {
  name: &quot;Conv_37&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_35&quot;
  top: &quot;Conv_37&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_38&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_37&quot;
  bottom: &quot;Add_31&quot;
  top: &quot;Add_38&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_39&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_38&quot;
  top: &quot;Add_38&quot;
}
layer {
  name: &quot;Conv_40&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_38&quot;
  top: &quot;Conv_40&quot;
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_41&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_40&quot;
  top: &quot;Conv_40&quot;
}
layer {
  name: &quot;Conv_42&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_40&quot;
  top: &quot;Conv_42&quot;
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_43&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_42&quot;
  top: &quot;Conv_42&quot;
}
layer {
  name: &quot;Conv_44&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_42&quot;
  top: &quot;Conv_44&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_45&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_44&quot;
  bottom: &quot;Add_38&quot;
  top: &quot;Add_45&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_46&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_45&quot;
  top: &quot;Add_45&quot;
}
layer {
  name: &quot;Conv_47&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_45&quot;
  top: &quot;Conv_47&quot;
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_48&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_47&quot;
  top: &quot;Conv_47&quot;
}
layer {
  name: &quot;Conv_49&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_47&quot;
  top: &quot;Conv_49&quot;
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_50&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_49&quot;
  top: &quot;Conv_49&quot;
}
layer {
  name: &quot;Conv_51&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_49&quot;
  top: &quot;Conv_51&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_52&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_51&quot;
  bottom: &quot;Add_45&quot;
  top: &quot;Add_52&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_53&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_52&quot;
  top: &quot;Add_52&quot;
}
layer {
  name: &quot;Conv_54&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_52&quot;
  top: &quot;Conv_54&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_55&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_54&quot;
  top: &quot;Conv_54&quot;
}
layer {
  name: &quot;Conv_56&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_54&quot;
  top: &quot;Conv_56&quot;
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Relu_57&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_56&quot;
  top: &quot;Conv_56&quot;
}
layer {
  name: &quot;Conv_58&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_56&quot;
  top: &quot;Conv_58&quot;
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_59&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_52&quot;
  top: &quot;Conv_59&quot;
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Add_60&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_58&quot;
  bottom: &quot;Conv_59&quot;
  top: &quot;Add_60&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_61&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_60&quot;
  top: &quot;Add_60&quot;
}
layer {
  name: &quot;Conv_62&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_60&quot;
  top: &quot;Conv_62&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_63&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_62&quot;
  top: &quot;Conv_62&quot;
}
layer {
  name: &quot;Conv_64&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_62&quot;
  top: &quot;Conv_64&quot;
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_65&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_64&quot;
  top: &quot;Conv_64&quot;
}
layer {
  name: &quot;Conv_66&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_64&quot;
  top: &quot;Conv_66&quot;
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_67&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_66&quot;
  bottom: &quot;Add_60&quot;
  top: &quot;Add_67&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_68&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_67&quot;
  top: &quot;Add_67&quot;
}
layer {
  name: &quot;Conv_69&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_67&quot;
  top: &quot;Conv_69&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_70&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_69&quot;
  top: &quot;Conv_69&quot;
}
layer {
  name: &quot;Conv_71&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_69&quot;
  top: &quot;Conv_71&quot;
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_72&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_71&quot;
  top: &quot;Conv_71&quot;
}
layer {
  name: &quot;Conv_73&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_71&quot;
  top: &quot;Conv_73&quot;
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_74&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_73&quot;
  bottom: &quot;Add_67&quot;
  top: &quot;Add_74&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_75&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_74&quot;
  top: &quot;Add_74&quot;
}
layer {
  name: &quot;Conv_76&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_74&quot;
  top: &quot;Conv_76&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_77&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_76&quot;
  top: &quot;Conv_76&quot;
}
layer {
  name: &quot;Conv_78&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_76&quot;
  top: &quot;Conv_78&quot;
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_79&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_78&quot;
  top: &quot;Conv_78&quot;
}
layer {
  name: &quot;Conv_80&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_78&quot;
  top: &quot;Conv_80&quot;
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_81&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_80&quot;
  bottom: &quot;Add_74&quot;
  top: &quot;Add_81&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_82&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_81&quot;
  top: &quot;Add_81&quot;
}
layer {
  name: &quot;Conv_83&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_81&quot;
  top: &quot;Conv_83&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_84&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_83&quot;
  top: &quot;Conv_83&quot;
}
layer {
  name: &quot;Conv_85&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_83&quot;
  top: &quot;Conv_85&quot;
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_86&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_85&quot;
  top: &quot;Conv_85&quot;
}
layer {
  name: &quot;Conv_87&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_85&quot;
  top: &quot;Conv_87&quot;
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_88&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_87&quot;
  bottom: &quot;Add_81&quot;
  top: &quot;Add_88&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_89&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_88&quot;
  top: &quot;Add_88&quot;
}
layer {
  name: &quot;Conv_90&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_88&quot;
  top: &quot;Conv_90&quot;
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_91&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_90&quot;
  top: &quot;Conv_90&quot;
}
layer {
  name: &quot;Conv_92&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_90&quot;
  top: &quot;Conv_92&quot;
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_93&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_92&quot;
  top: &quot;Conv_92&quot;
}
layer {
  name: &quot;Conv_94&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_92&quot;
  top: &quot;Conv_94&quot;
  convolution_param {
    num_output: 512
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_95&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_94&quot;
  bottom: &quot;Add_88&quot;
  top: &quot;Add_95&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_96&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_95&quot;
  top: &quot;Add_95&quot;
}
layer {
  name: &quot;Conv_97&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_95&quot;
  top: &quot;Conv_97&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_98&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_97&quot;
  top: &quot;Conv_97&quot;
}
layer {
  name: &quot;Conv_99&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_97&quot;
  top: &quot;Conv_99&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Relu_100&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_99&quot;
  top: &quot;Conv_99&quot;
}
layer {
  name: &quot;Conv_101&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_99&quot;
  top: &quot;Conv_101&quot;
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_102&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_95&quot;
  top: &quot;Conv_102&quot;
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Add_103&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_101&quot;
  bottom: &quot;Conv_102&quot;
  top: &quot;Add_103&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_104&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_103&quot;
  top: &quot;Add_103&quot;
}
layer {
  name: &quot;Conv_105&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_103&quot;
  top: &quot;Conv_105&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_106&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_105&quot;
  top: &quot;Conv_105&quot;
}
layer {
  name: &quot;Conv_107&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_105&quot;
  top: &quot;Conv_107&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_108&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_107&quot;
  top: &quot;Conv_107&quot;
}
layer {
  name: &quot;Conv_109&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_107&quot;
  top: &quot;Conv_109&quot;
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_110&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_109&quot;
  bottom: &quot;Add_103&quot;
  top: &quot;Add_110&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_111&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_110&quot;
  top: &quot;Add_110&quot;
}
layer {
  name: &quot;Conv_112&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_110&quot;
  top: &quot;Conv_112&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_113&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_112&quot;
  top: &quot;Conv_112&quot;
}
layer {
  name: &quot;Conv_114&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_112&quot;
  top: &quot;Conv_114&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_115&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_114&quot;
  top: &quot;Conv_114&quot;
}
layer {
  name: &quot;Conv_116&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_114&quot;
  top: &quot;Conv_116&quot;
  convolution_param {
    num_output: 1024
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Add_117&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Conv_116&quot;
  bottom: &quot;Add_110&quot;
  top: &quot;Add_117&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Relu_118&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Add_117&quot;
  top: &quot;Add_117&quot;
}
layer {
  name: &quot;Conv_119&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_117&quot;
  top: &quot;Conv_119&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_120&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_119&quot;
  top: &quot;Conv_120&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Conv_121&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_120&quot;
  top: &quot;Conv_121&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 2
  }
}
layer {
  name: &quot;Conv_122&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_95&quot;
  top: &quot;Conv_122&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Upsample_123&quot;
  type: &quot;Interp&quot;
  bottom: &quot;Conv_119&quot;
  top: &quot;Upsample_123&quot;
  interp_param {
    height: 32
    width: 32
  }
}
layer {
  name: &quot;Add_124&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Upsample_123&quot;
  bottom: &quot;Conv_122&quot;
  top: &quot;Add_124&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Conv_125&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_52&quot;
  top: &quot;Conv_125&quot;
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Upsample_126&quot;
  type: &quot;Interp&quot;
  bottom: &quot;Add_124&quot;
  top: &quot;Upsample_126&quot;
  interp_param {
    height: 64
    width: 64
  }
}
layer {
  name: &quot;Add_127&quot;
  type: &quot;Eltwise&quot;
  bottom: &quot;Upsample_126&quot;
  bottom: &quot;Conv_125&quot;
  top: &quot;Add_127&quot;
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: &quot;Conv_128&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_119&quot;
  top: &quot;Conv_128&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_129&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_124&quot;
  top: &quot;Conv_129&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_130&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Add_127&quot;
  top: &quot;Conv_130&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_131&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_130&quot;
  top: &quot;Conv_131&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_132&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_131&quot;
  top: &quot;Conv_131&quot;
}
layer {
  name: &quot;Conv_133&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_131&quot;
  top: &quot;Conv_133&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_134&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_133&quot;
  top: &quot;Conv_133&quot;
}
layer {
  name: &quot;Conv_135&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_133&quot;
  top: &quot;Conv_135&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_136&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_135&quot;
  top: &quot;Conv_135&quot;
}
layer {
  name: &quot;Conv_137&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_135&quot;
  top: &quot;Conv_137&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_138&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_137&quot;
  top: &quot;Conv_137&quot;
}
layer {
  name: &quot;Conv_139&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_130&quot;
  top: &quot;Conv_139&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_140&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_139&quot;
  top: &quot;Conv_139&quot;
}
layer {
  name: &quot;Conv_141&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_139&quot;
  top: &quot;Conv_141&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_142&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_141&quot;
  top: &quot;Conv_141&quot;
}
layer {
  name: &quot;Conv_143&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_141&quot;
  top: &quot;Conv_143&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_144&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_143&quot;
  top: &quot;Conv_143&quot;
}
layer {
  name: &quot;Conv_145&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_143&quot;
  top: &quot;Conv_145&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_146&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_145&quot;
  top: &quot;Conv_145&quot;
}
layer {
  name: &quot;Conv_147&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_137&quot;
  top: &quot;Conv_147&quot;
  convolution_param {
    num_output: 4
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_148&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_145&quot;
  top: &quot;Conv_148&quot;
  convolution_param {
    num_output: 15
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Transpose_149&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_147&quot;
  top: &quot;Transpose_149&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Transpose_150&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_148&quot;
  top: &quot;Transpose_150&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Conv_151&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_129&quot;
  top: &quot;Conv_151&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_152&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_151&quot;
  top: &quot;Conv_151&quot;
}
layer {
  name: &quot;Conv_153&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_151&quot;
  top: &quot;Conv_153&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_154&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_153&quot;
  top: &quot;Conv_153&quot;
}
layer {
  name: &quot;Conv_155&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_153&quot;
  top: &quot;Conv_155&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_156&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_155&quot;
  top: &quot;Conv_155&quot;
}
layer {
  name: &quot;Conv_157&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_155&quot;
  top: &quot;Conv_157&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_158&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_157&quot;
  top: &quot;Conv_157&quot;
}
layer {
  name: &quot;Conv_159&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_129&quot;
  top: &quot;Conv_159&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_160&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_159&quot;
  top: &quot;Conv_159&quot;
}
layer {
  name: &quot;Conv_161&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_159&quot;
  top: &quot;Conv_161&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_162&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_161&quot;
  top: &quot;Conv_161&quot;
}
layer {
  name: &quot;Conv_163&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_161&quot;
  top: &quot;Conv_163&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_164&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_163&quot;
  top: &quot;Conv_163&quot;
}
layer {
  name: &quot;Conv_165&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_163&quot;
  top: &quot;Conv_165&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_166&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_165&quot;
  top: &quot;Conv_165&quot;
}
layer {
  name: &quot;Conv_167&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_157&quot;
  top: &quot;Conv_167&quot;
  convolution_param {
    num_output: 4
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_168&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_165&quot;
  top: &quot;Conv_168&quot;
  convolution_param {
    num_output: 15
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Transpose_169&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_167&quot;
  top: &quot;Transpose_169&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Transpose_170&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_168&quot;
  top: &quot;Transpose_170&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Conv_171&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_128&quot;
  top: &quot;Conv_171&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_172&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_171&quot;
  top: &quot;Conv_171&quot;
}
layer {
  name: &quot;Conv_173&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_171&quot;
  top: &quot;Conv_173&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_174&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_173&quot;
  top: &quot;Conv_173&quot;
}
layer {
  name: &quot;Conv_175&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_173&quot;
  top: &quot;Conv_175&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_176&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_175&quot;
  top: &quot;Conv_175&quot;
}
layer {
  name: &quot;Conv_177&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_175&quot;
  top: &quot;Conv_177&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_178&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_177&quot;
  top: &quot;Conv_177&quot;
}
layer {
  name: &quot;Conv_179&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_128&quot;
  top: &quot;Conv_179&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_180&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_179&quot;
  top: &quot;Conv_179&quot;
}
layer {
  name: &quot;Conv_181&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_179&quot;
  top: &quot;Conv_181&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_182&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_181&quot;
  top: &quot;Conv_181&quot;
}
layer {
  name: &quot;Conv_183&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_181&quot;
  top: &quot;Conv_183&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_184&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_183&quot;
  top: &quot;Conv_183&quot;
}
layer {
  name: &quot;Conv_185&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_183&quot;
  top: &quot;Conv_185&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_186&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_185&quot;
  top: &quot;Conv_185&quot;
}
layer {
  name: &quot;Conv_187&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_177&quot;
  top: &quot;Conv_187&quot;
  convolution_param {
    num_output: 4
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_188&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_185&quot;
  top: &quot;Conv_188&quot;
  convolution_param {
    num_output: 15
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Transpose_189&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_187&quot;
  top: &quot;Transpose_189&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Transpose_190&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_188&quot;
  top: &quot;Transpose_190&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Conv_191&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_120&quot;
  top: &quot;Conv_191&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_192&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_191&quot;
  top: &quot;Conv_191&quot;
}
layer {
  name: &quot;Conv_193&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_191&quot;
  top: &quot;Conv_193&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_194&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_193&quot;
  top: &quot;Conv_193&quot;
}
layer {
  name: &quot;Conv_195&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_193&quot;
  top: &quot;Conv_195&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_196&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_195&quot;
  top: &quot;Conv_195&quot;
}
layer {
  name: &quot;Conv_197&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_195&quot;
  top: &quot;Conv_197&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_198&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_197&quot;
  top: &quot;Conv_197&quot;
}
layer {
  name: &quot;Conv_199&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_120&quot;
  top: &quot;Conv_199&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_200&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_199&quot;
  top: &quot;Conv_199&quot;
}
layer {
  name: &quot;Conv_201&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_199&quot;
  top: &quot;Conv_201&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_202&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_201&quot;
  top: &quot;Conv_201&quot;
}
layer {
  name: &quot;Conv_203&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_201&quot;
  top: &quot;Conv_203&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_204&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_203&quot;
  top: &quot;Conv_203&quot;
}
layer {
  name: &quot;Conv_205&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_203&quot;
  top: &quot;Conv_205&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_206&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_205&quot;
  top: &quot;Conv_205&quot;
}
layer {
  name: &quot;Conv_207&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_197&quot;
  top: &quot;Conv_207&quot;
  convolution_param {
    num_output: 4
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_208&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_205&quot;
  top: &quot;Conv_208&quot;
  convolution_param {
    num_output: 15
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Transpose_209&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_207&quot;
  top: &quot;Transpose_209&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Transpose_210&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_208&quot;
  top: &quot;Transpose_210&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Conv_211&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_121&quot;
  top: &quot;Conv_211&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_212&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_211&quot;
  top: &quot;Conv_211&quot;
}
layer {
  name: &quot;Conv_213&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_211&quot;
  top: &quot;Conv_213&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_214&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_213&quot;
  top: &quot;Conv_213&quot;
}
layer {
  name: &quot;Conv_215&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_213&quot;
  top: &quot;Conv_215&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_216&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_215&quot;
  top: &quot;Conv_215&quot;
}
layer {
  name: &quot;Conv_217&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_215&quot;
  top: &quot;Conv_217&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_218&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_217&quot;
  top: &quot;Conv_217&quot;
}
layer {
  name: &quot;Conv_219&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_121&quot;
  top: &quot;Conv_219&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_220&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_219&quot;
  top: &quot;Conv_219&quot;
}
layer {
  name: &quot;Conv_221&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_219&quot;
  top: &quot;Conv_221&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_222&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_221&quot;
  top: &quot;Conv_221&quot;
}
layer {
  name: &quot;Conv_223&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_221&quot;
  top: &quot;Conv_223&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_224&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_223&quot;
  top: &quot;Conv_223&quot;
}
layer {
  name: &quot;Conv_225&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_223&quot;
  top: &quot;Conv_225&quot;
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Relu_226&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Conv_225&quot;
  top: &quot;Conv_225&quot;
}
layer {
  name: &quot;Conv_227&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_217&quot;
  top: &quot;Conv_227&quot;
  convolution_param {
    num_output: 4
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Conv_228&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;Conv_225&quot;
  top: &quot;Conv_228&quot;
  convolution_param {
    num_output: 15
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
  }
}
layer {
  name: &quot;Transpose_229&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_227&quot;
  top: &quot;Transpose_229&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Transpose_230&quot;
  type: &quot;Permute&quot;
  bottom: &quot;Conv_228&quot;
  top: &quot;Transpose_230&quot;
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: &quot;Reshape_232&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_149&quot;
  top: &quot;Reshape_232&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 4
    }
  }
}
layer {
  name: &quot;Reshape_234&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_169&quot;
  top: &quot;Reshape_234&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 4
    }
  }
}
layer {
  name: &quot;Reshape_236&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_189&quot;
  top: &quot;Reshape_236&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 4
    }
  }
}
layer {
  name: &quot;Reshape_238&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_209&quot;
  top: &quot;Reshape_238&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 4
    }
  }
}
layer {
  name: &quot;Reshape_240&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_229&quot;
  top: &quot;Reshape_240&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 4
    }
  }
}
layer {
  name: &quot;Concat_241&quot;
  type: &quot;Concat&quot;
  bottom: &quot;Reshape_232&quot;
  bottom: &quot;Reshape_234&quot;
  bottom: &quot;Reshape_236&quot;
  bottom: &quot;Reshape_238&quot;
  bottom: &quot;Reshape_240&quot;
  top: &quot;Concat_241&quot;
  concat_param {
    axis: 1
  }
}
layer {
  name: &quot;Reshape_243&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_150&quot;
  top: &quot;Reshape_243&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 15
    }
  }
}
layer {
  name: &quot;Reshape_245&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_170&quot;
  top: &quot;Reshape_245&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 15
    }
  }
}
layer {
  name: &quot;Reshape_247&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_190&quot;
  top: &quot;Reshape_247&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 15
    }
  }
}
layer {
  name: &quot;Reshape_249&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_210&quot;
  top: &quot;Reshape_249&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 15
    }
  }
}
layer {
  name: &quot;Reshape_251&quot;
  type: &quot;Reshape&quot;
  bottom: &quot;Transpose_230&quot;
  top: &quot;Reshape_251&quot;
  reshape_param {
    shape {
      dim:0
      dim: -1
      dim: 1
      dim: 15
    }
  }
}
layer {
  name: &quot;Concat_252&quot;
  type: &quot;Concat&quot;
  bottom: &quot;Reshape_243&quot;
  bottom: &quot;Reshape_245&quot;
  bottom: &quot;Reshape_247&quot;
  bottom: &quot;Reshape_249&quot;
  bottom: &quot;Reshape_251&quot;
  top: &quot;Concat_252&quot;
  concat_param {
    axis: 1
  }
}
layer {
  name: &quot;Sigmoid_253&quot;
  type: &quot;Sigmoid&quot;
  bottom: &quot;Concat_252&quot;
  top: &quot;Sigmoid_253&quot;
}
layer {
  name: &quot;Relu_254&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;Concat_241&quot;
  top: &quot;Concat_241&quot;
}

layer {
  name: &quot;output&quot;
  type: &quot;OneStageOutput&quot;
  bottom: &quot;Concat_241&quot;
  bottom: &quot;Sigmoid_253&quot;
  bottom: &quot;PriorboxData&quot;
  top: &quot;output&quot;
  one_stage_output_param {
    num_classes: 15
    background_label_id: 0
    model_type: FSAF
    normalize_scale: 4
    keep_top_k: 50
    confidence_threshold: 0.05
    clip_bbox: false
    nms_param {
      nms_threshold: 0.5
      top_k: 100
    }
  }
}

</code></pre>

<h2 id="参考资料">参考资料</h2>

<ul>
<li><a href="https://arxiv.org/pdf/1512.02325.pdf">原始论文</a></li>
<li><a href="https://arxiv.org/pdf/1612.03144.pdf">Feature Pyramid Networks for Object Detection</a></li>
<li><a href="https://medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad">Understand Single Shot MultiBox Detector (SSD) and Implement It in Pytorch</a></li>
</ul>


                
                
<div class="entry-shang text-center">
    
	    <p>「真诚赞赏，手留余香」</p>
	
	<button class="zs show-zs btn btn-bred">赞赏支持</button>
</div>
<div class="zs-modal-bg"></div>
<div class="zs-modal-box">
	<div class="zs-modal-head">
		<button type="button" class="close">×</button>
		<span class="author"><a href="https://111qqz.github.io"><img src="/img/favicon.png" />111qqz的小窝</a></span>
        
	        <p class="tip"><i></i><span>真诚赞赏，手留余香</span></p>
		
 
	</div>
	<div class="zs-modal-body">
		<div class="zs-modal-btns">
			<button class="btn btn-blink" data-num="2">2元</button>
			<button class="btn btn-blink" data-num="5">5元</button>
			<button class="btn btn-blink" data-num="10">10元</button>
			<button class="btn btn-blink" data-num="50">50元</button>
			<button class="btn btn-blink" data-num="100">100元</button>
			<button class="btn btn-blink" data-num="1">任意金额</button>
		</div>
		<div class="zs-modal-pay">
			<button class="btn btn-bred" id="pay-text">2元</button>
			<p>使用<span id="pay-type">微信</span>扫描二维码完成支付</p>
			<img src="/img/reward/wechat-2.png"  id="pay-image"/>
		</div>
	</div>
	<div class="zs-modal-footer">
		<label><input type="radio" name="zs-type" value="wechat" class="zs-type" checked="checked"><span ><span class="zs-wechat"><img src="/img/reward/wechat-btn.png"/></span></label>
		<label><input type="radio" name="zs-type" value="alipay" class="zs-type" class="zs-alipay"><img src="/img/reward/alipay-btn.png"/></span></label>
	</div>
</div>
<script type="text/javascript" src="/js/reward.js"></script>

                

                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2019/11/rankboost-Algorithm" data-toggle="tooltip" data-placement="top" title="rankboost 算法学习笔记">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2019/12/feature-pyramid-networks" data-toggle="tooltip" data-placement="top" title="FPN:Feature Pyramid Networks 学习笔记">Next
                            Post &rarr;</a>
                    </li>
                    
                </ul>

                
<div id="disqus-comment"></div>

<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "111qqz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



            </div>
            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/bfs" title="bfs">
                            bfs
                        </a>
                        
                        
                        
                        <a href="/tags/binary-search" title="binary-search">
                            binary-search
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/brute-force" title="brute-force">
                            brute-force
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/c&#43;&#43;" title="c&#43;&#43;">
                            c&#43;&#43;
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/dfs" title="dfs">
                            dfs
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/dp" title="dp">
                            dp
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/greedy" title="greedy">
                            greedy
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/hash" title="hash">
                            hash
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/km" title="km">
                            km
                        </a>
                        
                        
                        
                        <a href="/tags/kmp" title="kmp">
                            kmp
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/leetcode" title="leetcode">
                            leetcode
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/math" title="math">
                            math
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/number-theory" title="number-theory">
                            number-theory
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/rmq" title="rmq">
                            rmq
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/sg%E5%87%BD%E6%95%B0" title="sg函数">
                            sg函数
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/stl" title="stl">
                            stl
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/tensorflow" title="tensorflow">
                            tensorflow
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E4%BA%8C%E5%88%86%E5%9B%BE%E6%9C%80%E4%BD%B3%E5%8C%B9%E9%85%8D" title="二分图最佳匹配">
                            二分图最佳匹配
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E5%88%86%E5%9D%97" title="分块">
                            分块
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E5%89%8D%E7%BC%80%E5%92%8C" title="前缀和">
                            前缀和
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95" title="匈牙利算法">
                            匈牙利算法
                        </a>
                        
                        
                        
                        <a href="/tags/%E5%8C%BA%E9%97%B4dp" title="区间dp">
                            区间dp
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA" title="博弈论">
                            博弈论
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E5%90%8E%E7%BC%80%E8%87%AA%E5%8A%A8%E6%9C%BA" title="后缀自动机">
                            后缀自动机
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E5%9B%BE%E8%AE%BA" title="图论">
                            图论
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2" title="字符串">
                            字符串
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E5%BF%AB%E9%80%9F%E5%B9%82" title="快速幂">
                            快速幂
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E6%95%B0%E4%BD%8Ddp" title="数位dp">
                            数位dp
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E6%9E%84%E9%80%A0" title="构造">
                            构造
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84" title="树状数组">
                            树状数组
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E6%A6%82%E7%8E%87" title="概率">
                            概率
                        </a>
                        
                        
                        
                        <a href="/tags/%E6%A8%A1%E6%8B%9F" title="模拟">
                            模拟
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E6%AF%8D%E5%87%BD%E6%95%B0" title="母函数">
                            母函数
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E7%9F%A9%E9%98%B5" title="矩阵">
                            矩阵
                        </a>
                        
                        
                        
                        
                        
                        <a href="/tags/%E7%A6%BB%E6%95%A3%E5%8C%96" title="离散化">
                            离散化
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91" title="线段树">
                            线段树
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95" title="计算几何">
                            计算几何
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                    </div>
                </section>
                

                
                
                <section>
                    <hr>
                    <h5>FRIENDS</h5>
                    <ul class="list-inline">
                        
                        <li><a target="_blank" href="https://111qqz.com">111qqz的wordpress博客</a></li>
                        
                    </ul>
                </section>
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                   
                   <li>
                       <a href="" rel="alternate" type="application/rss+xml" title="111qqz的小窝" >
                           <span class="fa-stack fa-lg">
                               <i class="fa fa-circle fa-stack-2x"></i>
                               <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                           </span>
                       </a>
                   </li>
                   
                    
                    <li>
                        <a href="mailto:hust.111qqz@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		    
                    
                    
                    
                    

                    

		    
                    
                    <li>
                        <a target="_blank" href="/your%20wechat%20qr%20code%20image">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-wechat fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		    
                    
                    <li>
                        <a target="_blank" href="https://github.com/111qqz/">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		    
                    
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/yourlinkedinid">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		    
                    
                </ul>
		<p class="copyright text-muted">
                    Copyright &copy; 111qqz的小窝 2020
                    <br>
                    <a href="https://themes.gohugo.io/hugo-theme-cleanwhite">CleanWhite Hugo Theme</a> by <a href="https://zhaohuabing.com">Huabing</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        async("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>






</body>
</html>
