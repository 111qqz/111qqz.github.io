<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>end-to-end on 111qqz's blog</title><link>https://111qqz.com/tags/end-to-end/</link><description>Recent content in end-to-end on 111qqz's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Thu, 22 Feb 2018 02:53:01 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/end-to-end/index.xml" rel="self" type="application/rss+xml"/><item><title>end-to-end 神经网络</title><link>https://111qqz.com/2018/02/end-to-end-neural-network/</link><pubDate>Thu, 22 Feb 2018 02:53:01 +0000</pubDate><guid>https://111qqz.com/2018/02/end-to-end-neural-network/</guid><description>
所谓end-to-end 神经网络，更多是一种思想。
这种思想的核心是，比如对于图像处理，输入原始图像数据，输出的是直接有用的结果（有用取决于具体的任务，比如自动驾驶)
也就是尽可能少得减少人为干预，使训练是end (原始数据) to end (对应用问题直接有用的结果)
端到端指的是输入是原始数据，输出是最后的结果，原来输入端不是直接的原始数据，而是在原始数据中提取的特征，这一点在图像问题上尤为突出，因为图像像素数太多，数据维度高，会产生维度灾难，所以原来一个思路是手工提取图像的一些关键特征，这实际就是就一个降维的过程。 那么问题来了，特征怎么提？ 特征提取的好坏异常关键，甚至比学习算法还重要，举个例子，对一系列人的数据分类，分类结果是性别，如果你提取的特征是头发的颜色，无论分类算法如何，分类效果都不会好，如果你提取的特征是头发的长短，这个特征就会好很多，但是还是会有错误，如果你提取了一个超强特征，比如染色体的数据，那你的分类基本就不会错了。 这就意味着，特征需要足够的经验去设计，这在数据量越来越大的情况下也越来越困难。 于是就出现了端到端网络，特征可以自己去学习，所以特征提取这一步也就融入到算法当中，不需要人来干预了。
简单得说，符合end-to-end 的神经网络，特征应该是网络自己学习，而不是人为提取。
参考资料：
什么是 end-to-end 神经网络？</description></item></channel></rss>