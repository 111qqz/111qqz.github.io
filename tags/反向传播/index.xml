<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>反向传播 on 111qqz's blog</title><link>https://111qqz.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</link><description>Recent content in 反向传播 on 111qqz's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Tue, 05 Sep 2017 12:30:17 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/index.xml" rel="self" type="application/rss+xml"/><item><title>反向传播学习笔记</title><link>https://111qqz.com/2017/09/back-propagation-notes/</link><pubDate>Tue, 05 Sep 2017 12:30:17 +0000</pubDate><guid>https://111qqz.com/2017/09/back-propagation-notes/</guid><description>
先说下自己目前很笼统的理解：
反向传播是用来快速计算梯度的一种方法；
过程大概是把计算过程用计算图表示，这样每一个中间步骤都有一个节点，每一个local gradient都会比较容易计算；
思想涉及 chain rule + 计算图 + 记忆化
因为计算不同自变量的偏导数会存在很多共同路径，这部分就只计算了一次，因此可以加快计算速度。
所以核心的东西大概是两点：
* 用计算图表示计算，局部gradient 替代繁琐的微积分计算 * 共同部分只计算一次，类似一个记忆化。</description></item></channel></rss>