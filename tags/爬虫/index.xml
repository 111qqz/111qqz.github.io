<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>爬虫 on Clarity</title><link>https://111qqz.com/tags/%E7%88%AC%E8%99%AB/</link><description>Recent content in 爬虫 on Clarity</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright><lastBuildDate>Tue, 06 Nov 2018 13:33:00 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml"/><item><title>How to use Scrapy with Django Application（转自medium）</title><link>https://111qqz.com/2018/11/how-to-use-scrapy-with-django-application/</link><pubDate>Tue, 06 Nov 2018 13:33:00 +0000</pubDate><guid>https://111qqz.com/2018/11/how-to-use-scrapy-with-django-application/</guid><description>
在meidum上看到一篇很赞的文章...无奈关键部分一律无法加载出来...挂了梯子也不行，很心塞...刚刚突然发现加载出来了...以防之后再次无法访问，所以搬运过来．
There are couple of articles on how to integrate Scrapy into a Django Application (or vice versa?). But most of them don’t cover a full complete example that includes triggering spiders from Django views. Since this is a web application, that must be our main goal.
What do we need ? Before we start, it is better to specify what we want and how we want it. Check this diagram:</description></item><item><title>爬虫学习笔记</title><link>https://111qqz.com/2018/10/web-crawler-notes/</link><pubDate>Fri, 19 Oct 2018 08:18:53 +0000</pubDate><guid>https://111qqz.com/2018/10/web-crawler-notes/</guid><description>
再次迫于生计。。。
参考了面向新人的 Python 爬虫学习资料
大致的学习路线为:
一： 简单的定向脚本爬虫（ request --- bs4 --- re ） 二： 大型框架式爬虫（ Scrapy 框架为主）
三：浏览器模拟爬虫 （ Mechanize 模拟 和 Selenium 模拟）
有Python基础和一点html基础的话。。。貌似上手是0难度的
年轻人的第一个爬虫(虽然代码是直接copy的...
''' 抓取百度贴吧---生活大爆炸吧的基本内容 爬虫线路： requests - bs4 Python版本： 3.6 OS： mac os 12.12.4 ''' import requests import time from bs4 import BeautifulSoup # 首先我们写好抓取网页的函数 def get_html(url): try: r = requests.get(url, timeout=30) r.raise_for_status() # 这里我们知道百度贴吧的编码是utf-8，所以手动设置的。爬去其他的页面时建议使用： # r.endcodding = r.apparent_endconding r.encoding = 'utf-8' return r.text except: return &amp;quot; ERROR &amp;quot; def get_content(url): ''' 分析贴吧的网页文件，整理信息，保存在列表变量中 ''' # 初始化一个列表来保存所有的帖子信息： comments = [] # 首先，我们把需要爬取信息的网页下载到本地 html = get_html(url) # 我们来做一锅汤 soup = BeautifulSoup(html, 'lxml') # 按照之前的分析，我们找到所有具有‘ j_thread_list clearfix’属性的li标签。返回一个列表类型。 liTags = soup.</description></item></channel></rss>