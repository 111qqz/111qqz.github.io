<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Convolutional Neural Network on 111qqz的小窝</title><link>https://111qqz.com/tags/convolutional-neural-network/</link><description>Recent content in Convolutional Neural Network on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Mon, 17 Jul 2017 02:02:43 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/convolutional-neural-network/index.xml" rel="self" type="application/rss+xml"/><item><title>stanford CS231n notes：Linear classification</title><link>https://111qqz.com/2017/07/cs231n-linear-classification/</link><pubDate>Mon, 17 Jul 2017 02:02:43 +0000</pubDate><guid>https://111qqz.com/2017/07/cs231n-linear-classification/</guid><description>
课程链接 知乎翻译链接
之前看的原版，后来发现知乎上有翻译，正好想到之前看完没有整理总结，干脆就写一下自己的理解，顺便贴一下课程翻译（感觉翻译的质量好像还可以？
分类器就是一个函数,自变量是图像信息，因变量是类别信息。
比如线性分类器，SVM,softmax
不同的分类器有着不同的score function,对应着不同的cost function.
之所以选择不同的cost function的原因是，要保证cost funtion是凸函数，不然会存在很多局部极值。
分类器使得分类问题变成了一个优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。
然后，所谓overfit,就是参数太多而训练集太小，导致可以完美符合这些训练集，但是无法一般化。
解决overfit有很多方法，这里介绍了正则化（Regularization）
思路是在cost funtion中添加一项Regularization loss
,用作对参数值的惩罚，lambda为惩罚因子
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: /2017/07/cs231n-linear-classification/http://www.zhihu.com/equation?tex=Ldisplaystyle&amp;#43;underbrace&amp;#43;frac1Nsum_i&amp;#43;L_i_data&amp;#43;&amp;#43;&amp;#43;lossunderbracelambda&amp;#43;RW_regularization&amp;#43;&amp;#43;loss
链接到文件: /content/post/ACM-ICPC/http://www.zhihu.com/equation?tex=Ldisplaystyle&amp;#43;underbrace&amp;#43;frac1Nsum_i&amp;#43;L_i_data&amp;#43;&amp;#43;&amp;#43;lossunderbracelambda&amp;#43;RW_regularization&amp;#43;&amp;#43;loss
使用 Page Bundles: true
这样就可以使得每个参数尽可能小，这样每个参数对于cost function的贡献就比较小，可以改善overfit的问题
原文如下 内容列表：
* 线性分类器简介 * 线性评分函数 * 阐明线性分类器 _**译者注：上篇翻译截止处**_ * 损失函数 * 多类SVM * Softmax分类器 * SVM和Softmax的比较 * 基于Web的可交互线性分类器原型 * 小结 线性分类 上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。我们还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：</description></item></channel></rss>