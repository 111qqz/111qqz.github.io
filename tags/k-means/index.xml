<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>k-means on 111qqz's blog</title><link>https://111qqz.com/tags/k-means/</link><description>Recent content in k-means on 111qqz's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sun, 26 Nov 2017 11:05:50 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/k-means/index.xml" rel="self" type="application/rss+xml"/><item><title>PCA + kmeans</title><link>https://111qqz.com/2017/11/pca-kmeans/</link><pubDate>Sun, 26 Nov 2017 11:05:50 +0000</pubDate><guid>https://111qqz.com/2017/11/pca-kmeans/</guid><description>
先记录一下PCA实战需要用到的安装包(arch下,python2环境) python2-scikit-learn python2-numpy
python2-pandas
python2-matplotlib
python2-seaborn
pandas.DataFrame
pandas 数据结构介绍
几个和科学计算数据分析有关的重要的python库:Numpy、Matplotlib ,pandas
(之前数字图像处理课程都接触过了orz)
其中matplotlib 主要用于图像绘制
sklearn 是用于机器学习的python 模块
Seaborn也是用于图像绘制
str.fomat() 是 python2语法
format中的变量会按照str中{} 出现的顺序替换
import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.datasets import fetch_mldata from sklearn.decomposition import PCA import seaborn as sns mnist = fetch_mldata(&amp;quot;MNIST original&amp;quot;) X = mnist.data / 255.0 y = mnist.target #print X.shape, y.shape feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ] df = pd.</description></item><item><title>k-means clustering 学习笔记</title><link>https://111qqz.com/2017/08/k-means-clustering-notes/</link><pubDate>Thu, 03 Aug 2017 13:09:17 +0000</pubDate><guid>https://111qqz.com/2017/08/k-means-clustering-notes/</guid><description>
其实这算法巨简单。。。。让我想到了均分纸牌（noip200?
还是大致说一下：
对于有 features 但是 **没有 **labels 的数据，没办法用监督学习，但是可以使用非监督学习的聚类算法。
所谓聚类，简单理解，就是把相似的分成一组。。。
k-means就是一个常见的聚类算法。。。
k代表可以把数据分成k组。
举一个平面上二维点的例子，算法步骤如下：
1. 随机k个点当做k个点作为k组的中心。 2. 根据现在的k个中心，将数据集中的点，按照【距离哪个中心最近就属于哪个中心】的原则，分组。 3. 在每一个组内求点的二维平均数，作为新的中心。**如果存在一个组的数据中心改变，那么返回2，否则结束**。![](http://stanford.edu/~cpiech/cs221/img/kmeansViz.png) 可以很容易推广到高维度，就只是求平均数和算距离的时候有区别。
一般化的流程：
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } Web path: /2017/08/k-means-clustering-notes/http://stanford.edu/~cpiech/cs221/img/kmeansMath.png
Disk path: /content/post/ACM-ICPC/http://stanford.edu/~cpiech/cs221/img/kmeansMath.png
Using Page Bundles: true
然后该算法是Expectation Maximization的一个特例
该算法和KNN算法没有半毛钱关系。。。
参考资料：
k means 维基百科
CS221-kmeans</description></item></channel></rss>