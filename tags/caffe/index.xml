<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>caffe on 111qqz的小窝</title><link>https://111qqz.github.io/tags/caffe/</link><description>Recent content in caffe on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 30 Jun 2020 19:47:02 +0800</lastBuildDate><atom:link href="https://111qqz.github.io/tags/caffe/index.xml" rel="self" type="application/rss+xml"/><item><title>caffe 源码阅读笔记</title><link>https://111qqz.github.io/2020/06/caffe-notes/</link><pubDate>Tue, 30 Jun 2020 19:47:02 +0800</pubDate><guid>https://111qqz.github.io/2020/06/caffe-notes/</guid><description>blob layer net 激活函数 卷积 reshape slice loss function reduce eltwise argmax</description></item><item><title>caffe 源码学习笔记(11) argmax layer</title><link>https://111qqz.github.io/2020/05/caffe-source-code-analysis-part11/</link><pubDate>Wed, 06 May 2020 21:26:03 +0800</pubDate><guid>https://111qqz.github.io/2020/05/caffe-source-code-analysis-part11/</guid><description>背景 似乎没什么背景,继续看caffe代码 argmax的作用是返回一个blob某个维度或者batch_size之后的维度的top_k的inde</description></item><item><title>caffe 源码学习笔记(10) eltwise layer</title><link>https://111qqz.github.io/2020/05/caffe-source-code-analysis-part10/</link><pubDate>Sun, 03 May 2020 17:53:22 +0800</pubDate><guid>https://111qqz.github.io/2020/05/caffe-source-code-analysis-part10/</guid><description>背景 这个layer和reduce layer有一些相似,就干脆一起看了. 作用是输入至少两个blob,然后对每个blob中的元素所一些运算,最后</description></item><item><title>caffe 源码学习笔记(9) reduce layer</title><link>https://111qqz.github.io/2020/05/caffe-source-code-analysis-part9/</link><pubDate>Sun, 03 May 2020 15:16:53 +0800</pubDate><guid>https://111qqz.github.io/2020/05/caffe-source-code-analysis-part9/</guid><description>背景 其实没什么背景,继续啃caffe代码而已2333 reduce layer其实就是做reduce操作,把一个任意shape的blob通过某种运算变成一</description></item><item><title>caffe 源码学习笔记(8) loss function</title><link>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part8/</link><pubDate>Sat, 18 Apr 2020 18:33:29 +0800</pubDate><guid>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part8/</guid><description>背景 虽然不太care 训练的过程，但是由于容易看懂的layer都看得差不多了 所以打算看一下这些loss function. Euclidean Loss (L2 loss) 一般用于“real-value</description></item><item><title>caffe 源码学习笔记(7) slice layer</title><link>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part7/</link><pubDate>Mon, 13 Apr 2020 21:22:54 +0800</pubDate><guid>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part7/</guid><description>背景 ocr组那边有个shuffle net 的网络,里面有个pytorch op叫chunk,转成的onnx对应的op是 split 作用是: Split a tensor into a list of tensors, along the</description></item><item><title>caffe 源码学习笔记(6) reshape layer</title><link>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part6/</link><pubDate>Thu, 09 Apr 2020 21:03:06 +0800</pubDate><guid>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part6/</guid><description>背景 最近在魔改 tensorRT 的caffe parser 之前caffe模型转到trt模型时，有一个修改是需要将reshape layer的param末尾补1,比较繁琐，</description></item><item><title>caffe 源码学习笔记(5) 卷积</title><link>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part5/</link><pubDate>Wed, 08 Apr 2020 20:29:37 +0800</pubDate><guid>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part5/</guid><description>caffe中卷积运算的实现 暴力实现的卷积大概是这样子的 for w in 1..W for h in 1..H for x in 1..K for y in 1..K for m in 1..M for d in 1..D output(w, h, m) += input(w+x, h+y, d) * filter(m, x, y, d) end end end end end end 这</description></item><item><title>caffe 源码学习笔记(4) 激活函数</title><link>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part4/</link><pubDate>Tue, 07 Apr 2020 23:21:40 +0800</pubDate><guid>https://111qqz.github.io/2020/04/caffe-source-code-analysis-part4/</guid><description>在看过caffe代码的三个核心部分,blob,layer,net之后，陷入了不知道以什么顺序继续看的困境。 blob,layer,net只是三</description></item><item><title>caffe 源码学习笔记(3) Net</title><link>https://111qqz.github.io/2020/01/caffe-source-code-analysis-part3/</link><pubDate>Sun, 12 Jan 2020 19:18:15 +0800</pubDate><guid>https://111qqz.github.io/2020/01/caffe-source-code-analysis-part3/</guid><description>Net 基本介绍 网络通过组成和自微分共同定义一个函数及其梯度。 网络是一些Layer组成的DAG,也就是有向无环图，在caffe中通常由protot</description></item><item><title>caffe 源码学习笔记(2) Layer</title><link>https://111qqz.github.io/2020/01/caffe-source-code-analysis-part2/</link><pubDate>Sat, 11 Jan 2020 17:47:05 +0800</pubDate><guid>https://111qqz.github.io/2020/01/caffe-source-code-analysis-part2/</guid><description>layer 整体介绍 layer是模型计算的基本单元 类似于pytorch或者其他深度学习框架的op layer中的数据流向为,输入若干个blob，称之为&amp;</description></item><item><title>caffe 源码学习笔记(1) Blob</title><link>https://111qqz.github.io/2020/01/caffe-source-code-analysis-part1/</link><pubDate>Fri, 10 Jan 2020 11:24:23 +0800</pubDate><guid>https://111qqz.github.io/2020/01/caffe-source-code-analysis-part1/</guid><description>迫于生计，开始看caffe代码。 会侧重于分析inference部分。 blob 整体介绍 blob的含义及目的 blob在逻辑上表示的就是所谓的tenso</description></item><item><title>Eigen: C++开源矩阵学习笔记</title><link>https://111qqz.github.io/2018/04/eigen-notes/</link><pubDate>Thu, 05 Apr 2018 07:14:54 +0000</pubDate><guid>https://111qqz.github.io/2018/04/eigen-notes/</guid><description>接触Eigen的原因是最近在看caffe/caffe2源码,caffe2中使用了Eigen库. Eigen 是一个基于C++模板的线性代数库，直接将库下</description></item></channel></rss>