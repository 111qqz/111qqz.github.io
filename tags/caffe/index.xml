<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Caffe on 111qqz的小窝</title><link>https://111qqz.com/tags/caffe/</link><description>Recent content in Caffe on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sat, 06 Aug 2022 12:26:03 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/caffe/index.xml" rel="self" type="application/rss+xml"/><item><title>caffe 源码阅读笔记</title><link>https://111qqz.com/2020/06caffe-notes/</link><pubDate>Tue, 30 Jun 2020 19:47:02 +0800</pubDate><guid>https://111qqz.com/2020/06caffe-notes/</guid><description>
&lt;p>caffe做部署是YYDS!&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/01/caffe-source-code-analysis-part1/">blob&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/01/caffe-source-code-analysis-part2/">layer&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/01/caffe-source-code-analysis-part3/">net&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part4/">激活函数&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part5/">卷积&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part6/">reshape&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part7/">slice&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part8/">loss function&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/05/caffe-source-code-analysis-part9/">reduce&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/05/caffe-source-code-analysis-part10/">eltwise&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://111qqz.github.io/2020/05/caffe-source-code-analysis-part11/">argmax&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>[施工中]caffe 源码学习笔记(11) softmax</title><link>https://111qqz.com/2022/08caffe-source-code-analysis-part12/</link><pubDate>Sat, 06 Aug 2022 12:26:03 +0800</pubDate><guid>https://111qqz.com/2022/08caffe-source-code-analysis-part12/</guid><description>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>2022年惊讶的发现，当时竟然没有写关于softmax的笔记，因此来补充一下。&lt;/p>
&lt;h2 id="proto">proto&lt;/h2>
&lt;p>还是先看proto&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-protobuf" data-lang="protobuf">&lt;span class="line">&lt;span class="ln"> 1&lt;/span>&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 2&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="c1">// Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 3&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">message&lt;/span> &lt;span class="nc">SoftmaxParameter&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 4&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="kd">enum&lt;/span> &lt;span class="n">Engine&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 5&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">DEFAULT&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 6&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">CAFFE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 7&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="n">CUDNN&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 8&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="p">}&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 9&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="k">optional&lt;/span> &lt;span class="n">Engine&lt;/span> &lt;span class="n">engine&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="k">default&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">DEFAULT&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">10&lt;/span>&lt;span class="cl">&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">11&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span> &lt;span class="c1">// The axis along which to perform the softmax -- may be negative to index
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">12&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="c1">// from the end (e.g., -1 for the last axis).
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">13&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="c1">// Any other axes will be evaluated as independent softmaxes.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">14&lt;/span>&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">optional&lt;/span> &lt;span class="kt">int32&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="k">default&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">15&lt;/span>&lt;span class="cl">&lt;span class="err">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="err">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>axis表示在哪个维护进行softmax&lt;/p></description></item><item><title>caffe 源码学习笔记(11) argmax layer</title><link>https://111qqz.com/2020/05caffe-source-code-analysis-part11/</link><pubDate>Wed, 06 May 2020 21:26:03 +0800</pubDate><guid>https://111qqz.com/2020/05caffe-source-code-analysis-part11/</guid><description>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>似乎没什么背景,继续看caffe代码&lt;/p>
&lt;p>argmax的作用是返回一个blob某个维度或者batch_size之后的维度的top_k的index(或者pair(index,value))&lt;/p></description></item><item><title>caffe 源码学习笔记(10) eltwise layer</title><link>https://111qqz.com/2020/05caffe-source-code-analysis-part10/</link><pubDate>Sun, 03 May 2020 17:53:22 +0800</pubDate><guid>https://111qqz.com/2020/05caffe-source-code-analysis-part10/</guid><description>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>这个layer和reduce layer有一些相似,就干脆一起看了.
作用是输入至少两个blob,然后对每个blob中的元素所一些运算,最后得到一个blob.&lt;/p></description></item><item><title>caffe 源码学习笔记(9) reduce layer</title><link>https://111qqz.com/2020/05caffe-source-code-analysis-part9/</link><pubDate>Sun, 03 May 2020 15:16:53 +0800</pubDate><guid>https://111qqz.com/2020/05caffe-source-code-analysis-part9/</guid><description>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>其实没什么背景,继续啃caffe代码而已2333&lt;/p>
&lt;p>reduce layer其实就是做reduce操作,把一个任意shape的blob通过某种运算变成一个scalar.&lt;/p></description></item><item><title>caffe 源码学习笔记(8) loss function</title><link>https://111qqz.com/2020/04caffe-source-code-analysis-part8/</link><pubDate>Sat, 18 Apr 2020 18:33:29 +0800</pubDate><guid>https://111qqz.com/2020/04caffe-source-code-analysis-part8/</guid><description>
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>虽然不太care 训练的过程，&lt;del>但是由于容易看懂的layer都看得差不多了&lt;/del> 所以打算看一下这些loss function.&lt;/p>
&lt;h2 id="euclidean-loss-l2-loss">Euclidean Loss (L2 loss)&lt;/h2>
&lt;p>&lt;strong>L2 loss.png&lt;/strong> &lt;em>(图片已丢失: voT7jGEF1BabmpL.png)&lt;/em>&lt;/p>
&lt;p>一般用于“real-valued regression tasks” 。　比如之前的项目上用的人脸年龄模型，就是用了这个Loss&lt;/p></description></item><item><title>caffe 源码学习笔记(7) slice layer</title><link>https://111qqz.com/2020/04caffe-source-code-analysis-part7/</link><pubDate>Mon, 13 Apr 2020 21:22:54 +0800</pubDate><guid>https://111qqz.com/2020/04caffe-source-code-analysis-part7/</guid><description>
&lt;h2 id="背景">背景　&lt;/h2>
&lt;p>ocr组那边有个shuffle net 的网络,里面有个pytorch op叫chunk,转成的onnx对应的op是 &lt;a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Split">split&lt;/a>&lt;/p>
&lt;p>作用是:&lt;/p>
&lt;blockquote>
&lt;p>Split a tensor into a list of tensors, along the specified &amp;lsquo;axis&amp;rsquo;. Lengths of the parts can be specified using argument &amp;lsquo;split&amp;rsquo;. Otherwise, the tensor is split to equal sized parts.&lt;/p></description></item><item><title>caffe 源码学习笔记(6) reshape layer</title><link>https://111qqz.com/2020/04caffe-source-code-analysis-part6/</link><pubDate>Thu, 09 Apr 2020 21:03:06 +0800</pubDate><guid>https://111qqz.com/2020/04caffe-source-code-analysis-part6/</guid><description>
&lt;h2 id="背景">背景　&lt;/h2>
&lt;p>最近在魔改 tensorRT 的caffe parser
之前caffe模型转到trt模型时，有一个修改是需要将reshape　layer的param末尾补1,比较繁琐，于是看了下caffe的reshape layer的实现．&lt;/p></description></item><item><title>caffe 源码学习笔记(5) 卷积</title><link>https://111qqz.com/2020/04caffe-source-code-analysis-part5/</link><pubDate>Wed, 08 Apr 2020 20:29:37 +0800</pubDate><guid>https://111qqz.com/2020/04caffe-source-code-analysis-part5/</guid><description>
&lt;h2 id="caffe中卷积运算的实现">caffe中卷积运算的实现&lt;/h2>
&lt;p>暴力实现的卷积大概是这样子的&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="ln"> 1&lt;/span>&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 2&lt;/span>&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">W&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 3&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">H&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 4&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">K&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 5&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">K&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 6&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">M&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 7&lt;/span>&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">D&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 8&lt;/span>&lt;span class="cl"> &lt;span class="n">output&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nb">input&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="nb">filter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 9&lt;/span>&lt;span class="cl"> &lt;span class="n">end&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">10&lt;/span>&lt;span class="cl"> &lt;span class="n">end&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">11&lt;/span>&lt;span class="cl"> &lt;span class="n">end&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">12&lt;/span>&lt;span class="cl"> &lt;span class="n">end&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">13&lt;/span>&lt;span class="cl"> &lt;span class="n">end&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">14&lt;/span>&lt;span class="cl">&lt;span class="n">end&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这种方式的效率显然很低，不意外地,caffe中并不是这样实现的．&lt;/p>
&lt;p>注释里面说:&lt;/p></description></item><item><title>caffe 源码学习笔记(3) Net</title><link>https://111qqz.com/2020/01caffe-source-code-analysis-part3/</link><pubDate>Sun, 12 Jan 2020 19:18:15 +0800</pubDate><guid>https://111qqz.com/2020/01caffe-source-code-analysis-part3/</guid><description>
&lt;h2 id="net-基本介绍">Net 基本介绍&lt;/h2>
&lt;p>网络通过组成和自微分共同定义一个函数及其梯度。&lt;/p>
&lt;p>网络是一些Layer组成的DAG,也就是有向无环图，在caffe中通常由prototxt定义．&lt;/p></description></item><item><title>caffe 源码学习笔记(2) Layer</title><link>https://111qqz.com/2020/01caffe-source-code-analysis-part2/</link><pubDate>Sat, 11 Jan 2020 17:47:05 +0800</pubDate><guid>https://111qqz.com/2020/01caffe-source-code-analysis-part2/</guid><description>
&lt;h2 id="layer-整体介绍">layer 整体介绍&lt;/h2>
&lt;p>layer是模型计算的基本单元
类似于pytorch或者其他深度学习框架的op
layer中的数据流向为,输入若干个blob，称之为&amp;quot;bottom blob&amp;quot;,然后经过layer的计算，输出若干个blob,称之为&amp;quot;top blob&amp;quot;&lt;/p></description></item><item><title>caffe 源码学习笔记(1) Blob</title><link>https://111qqz.com/2020/01caffe-source-code-analysis-part1/</link><pubDate>Fri, 10 Jan 2020 11:24:23 +0800</pubDate><guid>https://111qqz.com/2020/01caffe-source-code-analysis-part1/</guid><description>
&lt;p>迫于生计，开始看caffe代码。
会侧重于分析inference部分。&lt;/p>
&lt;h2 id="blob-整体介绍">blob 整体介绍&lt;/h2>
&lt;h3 id="blob的含义及目的">blob的含义及目的&lt;/h3>
&lt;p>blob在逻辑上表示的就是所谓的tensor,blob是tensor在caffe中的叫法。
在框架层面上，blob的意义在于对数据进行封装，提供统一的接口。
这里的数据包含训练/inference时用的数据，也包含模型参数，导数等数据。
深度学习离不开在GPU上的计算。 blob对数据的封装使得用户不必关心和cuda有关的数据传输细节。&lt;/p></description></item><item><title>Eigen: C++开源矩阵学习笔记</title><link>https://111qqz.com/2018/04eigen-notes/</link><pubDate>Thu, 05 Apr 2018 07:14:54 +0000</pubDate><guid>https://111qqz.com/2018/04eigen-notes/</guid><description>
&lt;p>接触Eigen的原因是最近在看caffe/caffe2源码,caffe2中使用了Eigen库. &lt;a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen&lt;/a> 是一个基于C++模板的线性代数库，直接将库下载后放在项目目录下，然后包含头文件就能使用，非常方便。对于Linux用户,只需要把头文件放到/usr/include 下即可此外，Eigen的接口清晰，稳定高效。&lt;/p></description></item></channel></rss>