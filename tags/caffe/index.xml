<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>caffe on 111qqz的小窝</title><link>https://111qqz.com/tags/caffe/</link><description>Recent content in caffe on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Tue, 30 Jun 2020 19:47:02 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/caffe/index.xml" rel="self" type="application/rss+xml"/><item><title>caffe 源码阅读笔记</title><link>https://111qqz.com/2020/06/caffe-notes/</link><pubDate>Tue, 30 Jun 2020 19:47:02 +0800</pubDate><guid>https://111qqz.com/2020/06/caffe-notes/</guid><description>
blob
layer
net
激活函数
卷积
reshape
slice
loss function
reduce
eltwise
argmax</description></item><item><title>caffe 源码学习笔记(11) argmax layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part11/</link><pubDate>Wed, 06 May 2020 21:26:03 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part11/</guid><description>
背景 似乎没什么背景,继续看caffe代码
argmax的作用是返回一个blob某个维度或者batch_size之后的维度的top_k的index(或者pair(index,value))
proto 还是先看proto
12message ArgMaxParameter {3 // If true produce pairs (argmax, maxval) 4 optional bool out_max_val = 1 [default = false];5 optional uint32 top_k = 2 [default = 1];6 // The axis along which to maximise -- may be negative to index from the 7 // end (e.g., -1 for the last axis). 8 // By default ArgMaxLayer maximizes over the flattened trailing dimensions 9 // for each index of the first / num dimension.</description></item><item><title>caffe 源码学习笔记(10) eltwise layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part10/</link><pubDate>Sun, 03 May 2020 17:53:22 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part10/</guid><description>
背景 这个layer和reduce layer有一些相似,就干脆一起看了. 作用是输入至少两个blob,然后对每个blob中的元素所一些运算,最后得到一个blob.
caffe 支持的运算有&amp;quot;PROD&amp;quot;,&amp;quot;SUM&amp;quot;,&amp;quot;MAX&amp;quot;三种
顺便提一句,TensorRT支持的要多一些:
1 2enum class ElementWiseOperation : int 3{ 4 kSUM = 0, //!&amp;lt; Sum of the two elements. 5 kPROD = 1, //!&amp;lt; Product of the two elements. 6 kMAX = 2, //!&amp;lt; Maximum of the two elements. 7 kMIN = 3, //!&amp;lt; Minimum of the two elements. 8 kSUB = 4, //!&amp;lt; Substract the second element from the first. 9 kDIV = 5, //!</description></item><item><title>caffe 源码学习笔记(9) reduce layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part9/</link><pubDate>Sun, 03 May 2020 15:16:53 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part9/</guid><description>
背景 其实没什么背景,继续啃caffe代码而已2333
reduce layer其实就是做reduce操作,把一个任意shape的blob通过某种运算变成一个scalar.
caffe目前支持求和(SUM),绝对值的和(ASUM),平方和(SUMSQ),以及对得到的scalar的总数求平均的求和(MEAN).
说句题外话,TensorRT支持的操作是求和,求积,max,min和ave. 还是有一些gap的
proto 先看proto
12message ReductionParameter {3 enum ReductionOp {4 SUM = 1;5 ASUM = 2;6 SUMSQ = 3;7 MEAN = 4;8 }910 optional ReductionOp operation = 1 [default = SUM]; // reduction operation 1112 // The first axis to reduce to a scalar -- may be negative to index from the 13 // end (e.g., -1 for the last axis). 14 // (Currently, only reduction along ALL &amp;#34;tail&amp;#34; axes is supported; reduction 15 // of axis M through N, where N &amp;lt; num_axes - 1, is unsupported.</description></item><item><title>caffe 源码学习笔记(8) loss function</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part8/</link><pubDate>Sat, 18 Apr 2020 18:33:29 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part8/</guid><description>
背景 虽然不太care 训练的过程，但是由于容易看懂的layer都看得差不多了 所以打算看一下这些loss function.
Euclidean Loss (L2 loss) Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: /2020/04/caffe-source-code-analysis-part8/https://i.loli.net/2020/04/18/voT7jGEF1BabmpL.png
链接到文件: /content/post/深度学习/https://i.loli.net/2020/04/18/voT7jGEF1BabmpL.png
使用 Page Bundles: true
一般用于“real-valued regression tasks” 。　比如之前的项目上用的人脸年龄模型，就是用了这个Loss
这个loss没什么额外的参数，实现也很简单。
1 2template &amp;lt;typename Dtype&amp;gt; 3void EuclideanLossLayer&amp;lt;Dtype&amp;gt;::Reshape( 4 const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom, const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) { 5 LossLayer&amp;lt;Dtype&amp;gt;::Reshape(bottom, top); 6 CHECK_EQ(bottom[0]-&amp;gt;count(1), bottom[1]-&amp;gt;count(1)) 7 &amp;lt;&amp;lt; &amp;#34;Inputs must have the same dimension.</description></item><item><title>caffe 源码学习笔记(7) slice layer</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part7/</link><pubDate>Mon, 13 Apr 2020 21:22:54 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part7/</guid><description>
背景　 ocr组那边有个shuffle net 的网络,里面有个pytorch op叫chunk,转成的onnx对应的op是 split
作用是:
Split a tensor into a list of tensors, along the specified 'axis'. Lengths of the parts can be specified using argument 'split'. Otherwise, the tensor is split to equal sized parts.
然后发现这个op模型转换里不支持转到caffe的layer,于是想办法支持了一下. 发现是要转到caffe的slice layer.(caffe也有一个split layer,但是这个split layer是除了一个输出blob作为多个layer的输入时用的)
proto 12message SliceParameter {3 // The axis along which to slice -- may be negative to index from the end 4 // (e.g., -1 for the last axis).</description></item><item><title>caffe 源码学习笔记(6) reshape layer</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part6/</link><pubDate>Thu, 09 Apr 2020 21:03:06 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part6/</guid><description>
背景　 最近在魔改 tensorRT 的caffe parser 之前caffe模型转到trt模型时，有一个修改是需要将reshape　layer的param末尾补1,比较繁琐，于是看了下caffe的reshape layer的实现．
proto 12message ReshapeParameter {3 // Specify the output dimensions. If some of the dimensions are set to 0, 4 // the corresponding dimension from the bottom layer is used (unchanged). 5 // Exactly one dimension may be set to -1, in which case its value is 6 // inferred from the count of the bottom blob and the remaining dimensions. 7 // For example, suppose we want to reshape a 2D blob &amp;#34;input&amp;#34; with shape 2 x 8: 8 // 9 // layer { 10 // type: &amp;#34;Reshape&amp;#34; bottom: &amp;#34;input&amp;#34; top: &amp;#34;output&amp;#34; 11 // reshape_param { .</description></item><item><title>caffe 源码学习笔记(5) 卷积</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part5/</link><pubDate>Wed, 08 Apr 2020 20:29:37 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part5/</guid><description>
caffe中卷积运算的实现 暴力实现的卷积大概是这样子的
1 2for w in 1..W 3 for h in 1..H 4 for x in 1..K 5 for y in 1..K 6 for m in 1..M 7 for d in 1..D 8 output(w, h, m) += input(w+x, h+y, d) * filter(m, x, y, d) 9 end 10 end 11 end 12 end 13 end 14end 15 这种方式的效率显然很低，不意外地,caffe中并不是这样实现的．
注释里面说:
Caffe convolves by reduction to matrix multiplication. This achieves high-throughput and generality of input and filter dimensions but comes at the cost of memory for matrices.</description></item><item><title>caffe 源码学习笔记(4) 激活函数</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part4/</link><pubDate>Tue, 07 Apr 2020 23:21:40 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part4/</guid><description>
在看过caffe代码的三个核心部分,blob,layer,net之后，陷入了不知道以什么顺序继续看的困境。
blob,layer,net只是三个最基本的概念，关键还是在于各个layer. 但是layer这么多，要怎么看呢？ 想了一下决定把相同作用的layer放在一起分析。 今天打算先分析一下激活函数。
sigmoid Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: /2020/04/caffe-source-code-analysis-part4/https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Sigmoid_function_01.png/220px-Sigmoid_function_01.png
链接到文件: /content/post/深度学习/https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Sigmoid_function_01.png/220px-Sigmoid_function_01.png
使用 Page Bundles: true
表达式为 f(t) = 1/(1+e^-t)
caffe GPU实现，非常直接
1 2template &amp;lt;typename Dtype&amp;gt; 3__global__ void SigmoidForward(const int n, const Dtype* in, Dtype* out) { 4 CUDA_KERNEL_LOOP(index, n) { 5 out[index] = 1. / (1. + exp(-in[index])); 6 } 7} 8 sigmoid激活函数的一大优点是求导非常容易，因此backward函数其实也很简单。</description></item><item><title>caffe 源码学习笔记(3) Net</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part3/</link><pubDate>Sun, 12 Jan 2020 19:18:15 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part3/</guid><description>
Net 基本介绍 网络通过组成和自微分共同定义一个函数及其梯度。
网络是一些Layer组成的DAG,也就是有向无环图，在caffe中通常由prototxt定义．
比如
1name: &amp;#34;LogReg&amp;#34;2layer {3 name: &amp;#34;mnist&amp;#34;4 type: &amp;#34;Data&amp;#34;5 top: &amp;#34;data&amp;#34;6 top: &amp;#34;label&amp;#34;7 data_param {8 source: &amp;#34;input_leveldb&amp;#34;9 batch_size: 6410 }11}12layer {13 name: &amp;#34;ip&amp;#34;14 type: &amp;#34;InnerProduct&amp;#34;15 bottom: &amp;#34;data&amp;#34;16 top: &amp;#34;ip&amp;#34;17 inner_product_param {18 num_output: 219 }20}21layer {22 name: &amp;#34;loss&amp;#34;23 type: &amp;#34;SoftmaxWithLoss&amp;#34;24 bottom: &amp;#34;ip&amp;#34;25 bottom: &amp;#34;label&amp;#34;26 top: &amp;#34;loss&amp;#34;27}定义了
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: /2020/01/caffe-source-code-analysis-part3/https://caffe.</description></item><item><title>caffe 源码学习笔记(2) Layer</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part2/</link><pubDate>Sat, 11 Jan 2020 17:47:05 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part2/</guid><description>
layer 整体介绍 layer是模型计算的基本单元 类似于pytorch或者其他深度学习框架的op layer中的数据流向为,输入若干个blob，称之为&amp;quot;bottom blob&amp;quot;,然后经过layer的计算，输出若干个blob,称之为&amp;quot;top blob&amp;quot;
也就是数据是从“bottom”流向“top”
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: /2020/01/caffe-source-code-analysis-part2/https://caffe.berkeleyvision.org/tutorial/fig/layer.jpg
链接到文件: /content/post/深度学习/https://caffe.berkeleyvision.org/tutorial/fig/layer.jpg
使用 Page Bundles: true
layer通常会进行两种计算，forward和backward
forward是指，根据bottom blob计算得到top blob backward是指，根据top blob的结果和参数值计算得到的gradient,回传给前面的layer.
layer 实现细节 layer作为一个base class,实现了所有layer都需要的common的部分。 比如:
1 /** 2* @brief Implements common layer setup functionality. 3* 4* @param bottom the preshaped input blobs 5* @param top 6* the allocated but unshaped output blobs, to be shaped by Reshape 7* 8* Checks that the number of bottom and top blobs is correct.</description></item><item><title>caffe 源码学习笔记(1) Blob</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part1/</link><pubDate>Fri, 10 Jan 2020 11:24:23 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part1/</guid><description>
迫于生计，开始看caffe代码。 会侧重于分析inference部分。
blob 整体介绍 blob的含义及目的 blob在逻辑上表示的就是所谓的tensor,blob是tensor在caffe中的叫法。 在框架层面上，blob的意义在于对数据进行封装，提供统一的接口。 这里的数据包含训练/inference时用的数据，也包含模型参数，导数等数据。 深度学习离不开在GPU上的计算。 blob对数据的封装使得用户不必关心和cuda有关的数据传输细节。
blob的表示 对于图像数据，blob通常为４-dim，也就是N*C*H*W 其中N表示number,也就是batch_num C表示channel H表示height W表示width
在内存中，blob是按照&amp;quot;C-contiguous fashion&amp;quot;存储的，也就是&amp;quot;row-major &amp;quot;
因此，位于(n,c,h,w)的下标在OS中的位置是　((n * C + c) * H + h) * W + w.
在代码blob.hpp中，我们也可以看到名为offset的函数是其对应的实现。
1 inline int offset(const int n, const int c = 0, const int h = 0, 2 const int w = 0) const { 3 CHECK_GE(n, 0); 4 CHECK_LE(n, num()); 5 CHECK_GE(channels(), 0); 6 CHECK_LE(c, channels()); 7 CHECK_GE(height(), 0); 8 CHECK_LE(h, height()); 9 CHECK_GE(width(), 0); 10 CHECK_LE(w, width()); 11 return ((n * channels() + c) * height() + h) * width() + w; 12 } 13 // 给一个indices,计算这是第几个值。 14 inline int offset(const vector&amp;lt;int&amp;gt;&amp;amp; indices) const { 15 CHECK_LE(indices.</description></item><item><title>Eigen: C++开源矩阵学习笔记</title><link>https://111qqz.com/2018/04/eigen-notes/</link><pubDate>Thu, 05 Apr 2018 07:14:54 +0000</pubDate><guid>https://111qqz.com/2018/04/eigen-notes/</guid><description>
接触Eigen的原因是最近在看caffe/caffe2源码,caffe2中使用了Eigen库. Eigen 是一个基于C++模板的线性代数库，直接将库下载后放在项目目录下，然后包含头文件就能使用，非常方便。对于Linux用户,只需要把头文件放到/usr/include 下即可此外，Eigen的接口清晰，稳定高效。
之后会更新一些,Eigen中我使用过的函数.
ubuntu14.04LTS 下使用方式: sudo apt-get install libeigen3-dev cd /usr/include/eigen3 sudo cp -R Eigen /usr/include 然后尝试运行如下代码,直接编译即可.如果可以正常运行,表明安装完毕.
#include &amp;lt;iostream&amp;gt; #include &amp;lt;Eigen/Dense&amp;gt; //using Eigen::MatrixXd; using namespace Eigen; using namespace Eigen::internal; using namespace Eigen::Architecture; using namespace std; int main() { cout&amp;lt;&amp;lt;&amp;quot;*******************1D-object****************&amp;quot;&amp;lt;&amp;lt;endl; Vector4d v1; v1&amp;lt;&amp;lt; 1,2,3,4; cout&amp;lt;&amp;lt;&amp;quot;v1=\n&amp;quot;&amp;lt;&amp;lt;v1&amp;lt;&amp;lt;endl; VectorXd v2(3); v2&amp;lt;&amp;lt;1,2,3; cout&amp;lt;&amp;lt;&amp;quot;v2=\n&amp;quot;&amp;lt;&amp;lt;v2&amp;lt;&amp;lt;endl; Array4i v3; v3&amp;lt;&amp;lt;1,2,3,4; cout&amp;lt;&amp;lt;&amp;quot;v3=\n&amp;quot;&amp;lt;&amp;lt;v3&amp;lt;&amp;lt;endl; ArrayXf v4(3); v4&amp;lt;&amp;lt;1,2,3; cout&amp;lt;&amp;lt;&amp;quot;v4=\n&amp;quot;&amp;lt;&amp;lt;v4&amp;lt;&amp;lt;endl; } map的使用办法: double arr[9]={1,2,3,4,5,6,7,8,9}; Map A(arr,3,3); 得到 1 4 7 2 5 8 3 6 9</description></item></channel></rss>