<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Caffe on 111qqz的小窝</title><link>https://111qqz.com/tags/caffe/</link><description>Recent content in Caffe on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sat, 06 Aug 2022 12:26:03 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/caffe/index.xml" rel="self" type="application/rss+xml"/><item><title>caffe 源码阅读笔记</title><link>https://111qqz.com/2020/06/caffe-notes/</link><pubDate>Tue, 30 Jun 2020 19:47:02 +0800</pubDate><guid>https://111qqz.com/2020/06/caffe-notes/</guid><description>
&lt;p&gt;caffe做部署是YYDS!&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/01/caffe-source-code-analysis-part1/"&gt;blob&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/01/caffe-source-code-analysis-part2/"&gt;layer&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/01/caffe-source-code-analysis-part3/"&gt;net&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part4/"&gt;激活函数&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part5/"&gt;卷积&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part6/"&gt;reshape&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part7/"&gt;slice&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/04/caffe-source-code-analysis-part8/"&gt;loss function&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/05/caffe-source-code-analysis-part9/"&gt;reduce&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/05/caffe-source-code-analysis-part10/"&gt;eltwise&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://111qqz.github.io/2020/05/caffe-source-code-analysis-part11/"&gt;argmax&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description></item><item><title>[施工中]caffe 源码学习笔记(11) softmax</title><link>https://111qqz.com/2022/08/caffe-source-code-analysis-part12/</link><pubDate>Sat, 06 Aug 2022 12:26:03 +0800</pubDate><guid>https://111qqz.com/2022/08/caffe-source-code-analysis-part12/</guid><description>
&lt;h2 id="背景"&gt;背景&lt;/h2&gt;
&lt;p&gt;2022年惊讶的发现，当时竟然没有写关于softmax的笔记，因此来补充一下。&lt;/p&gt;
&lt;h2 id="proto"&gt;proto&lt;/h2&gt;
&lt;p&gt;还是先看proto&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-protobuf" data-lang="protobuf"&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 1&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 2&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt;&lt;span class="c1"&gt;// Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 3&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;&lt;/span&gt;&lt;span class="kd"&gt;message&lt;/span&gt; &lt;span class="nc"&gt;SoftmaxParameter&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 4&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt; &lt;span class="kd"&gt;enum&lt;/span&gt; &lt;span class="n"&gt;Engine&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 5&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt; &lt;span class="n"&gt;DEFAULT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 6&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt; &lt;span class="n"&gt;CAFFE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 7&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt; &lt;span class="n"&gt;CUDNN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 8&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 9&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt; &lt;span class="k"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;Engine&lt;/span&gt; &lt;span class="n"&gt;engine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DEFAULT&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;10&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;11&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt; &lt;span class="c1"&gt;// The axis along which to perform the softmax -- may be negative to index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;12&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;&lt;/span&gt; &lt;span class="c1"&gt;// from the end (e.g., -1 for the last axis).
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;13&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;&lt;/span&gt; &lt;span class="c1"&gt;// Any other axes will be evaluated as independent softmaxes.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;14&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;&lt;/span&gt; &lt;span class="k"&gt;optional&lt;/span&gt; &lt;span class="kt"&gt;int32&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;15&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;axis表示在哪个维护进行softmax&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(11) argmax layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part11/</link><pubDate>Wed, 06 May 2020 21:26:03 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part11/</guid><description>
&lt;h2 id="背景"&gt;背景&lt;/h2&gt;
&lt;p&gt;似乎没什么背景,继续看caffe代码&lt;/p&gt;
&lt;p&gt;argmax的作用是返回一个blob某个维度或者batch_size之后的维度的top_k的index(或者pair(index,value))&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(10) eltwise layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part10/</link><pubDate>Sun, 03 May 2020 17:53:22 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part10/</guid><description>
&lt;h2 id="背景"&gt;背景&lt;/h2&gt;
&lt;p&gt;这个layer和reduce layer有一些相似,就干脆一起看了.
作用是输入至少两个blob,然后对每个blob中的元素所一些运算,最后得到一个blob.&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(9) reduce layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part9/</link><pubDate>Sun, 03 May 2020 15:16:53 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part9/</guid><description>
&lt;h2 id="背景"&gt;背景&lt;/h2&gt;
&lt;p&gt;其实没什么背景,继续啃caffe代码而已2333&lt;/p&gt;
&lt;p&gt;reduce layer其实就是做reduce操作,把一个任意shape的blob通过某种运算变成一个scalar.&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(8) loss function</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part8/</link><pubDate>Sat, 18 Apr 2020 18:33:29 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part8/</guid><description>
&lt;h2 id="背景"&gt;背景&lt;/h2&gt;
&lt;p&gt;虽然不太care 训练的过程，&lt;del&gt;但是由于容易看懂的layer都看得差不多了&lt;/del&gt; 所以打算看一下这些loss function.&lt;/p&gt;
&lt;h2 id="euclidean-loss-l2-loss"&gt;Euclidean Loss (L2 loss)&lt;/h2&gt;
&lt;p&gt;
&lt;img class="image_figure image_external" loading="lazy" src="https://111qqz.com/images/migrated/loli/voT7jGEF1BabmpL.png" alt="L2 loss.png" /&gt;
&lt;/p&gt;
&lt;p&gt;一般用于“real-valued regression tasks” 。　比如之前的项目上用的人脸年龄模型，就是用了这个Loss&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(7) slice layer</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part7/</link><pubDate>Mon, 13 Apr 2020 21:22:54 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part7/</guid><description>
&lt;h2 id="背景"&gt;背景　&lt;/h2&gt;
&lt;p&gt;ocr组那边有个shuffle net 的网络,里面有个pytorch op叫chunk,转成的onnx对应的op是 &lt;a href="https://github.com/onnx/onnx/blob/master/docs/Operators.md#Split"&gt;split&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作用是:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Split a tensor into a list of tensors, along the specified 'axis'. Lengths of the parts can be specified using argument 'split'. Otherwise, the tensor is split to equal sized parts.&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(6) reshape layer</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part6/</link><pubDate>Thu, 09 Apr 2020 21:03:06 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part6/</guid><description>
&lt;h2 id="背景"&gt;背景　&lt;/h2&gt;
&lt;p&gt;最近在魔改 tensorRT 的caffe parser
之前caffe模型转到trt模型时，有一个修改是需要将reshape　layer的param末尾补1,比较繁琐，于是看了下caffe的reshape layer的实现．&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(5) 卷积</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part5/</link><pubDate>Wed, 08 Apr 2020 20:29:37 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part5/</guid><description>
&lt;h2 id="caffe中卷积运算的实现"&gt;caffe中卷积运算的实现&lt;/h2&gt;
&lt;p&gt;暴力实现的卷积大概是这样子的&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 1&lt;/span&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 2&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 3&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 4&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 5&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 6&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 7&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 8&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt; 9&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;10&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;11&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;12&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;13&lt;/span&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="ln"&gt;14&lt;/span&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种方式的效率显然很低，不意外地,caffe中并不是这样实现的．&lt;/p&gt;
&lt;p&gt;注释里面说:&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(4) 激活函数</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part4/</link><pubDate>Tue, 07 Apr 2020 23:21:40 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part4/</guid><description>
&lt;p&gt;在看过caffe代码的三个核心部分,blob,layer,net之后，陷入了不知道以什么顺序继续看的困境。&lt;/p&gt;
&lt;p&gt;blob,layer,net只是三个最基本的概念，关键还是在于各个layer. 但是layer这么多，要怎么看呢？ 想了一下决定把相同作用的layer放在一起分析。 今天打算先分析一下激活函数。&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(3) Net</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part3/</link><pubDate>Sun, 12 Jan 2020 19:18:15 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part3/</guid><description>
&lt;h2 id="net-基本介绍"&gt;Net 基本介绍&lt;/h2&gt;
&lt;p&gt;网络通过组成和自微分共同定义一个函数及其梯度。&lt;/p&gt;
&lt;p&gt;网络是一些Layer组成的DAG,也就是有向无环图，在caffe中通常由prototxt定义．&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(2) Layer</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part2/</link><pubDate>Sat, 11 Jan 2020 17:47:05 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part2/</guid><description>
&lt;h2 id="layer-整体介绍"&gt;layer 整体介绍&lt;/h2&gt;
&lt;p&gt;layer是模型计算的基本单元
类似于pytorch或者其他深度学习框架的op
layer中的数据流向为,输入若干个blob，称之为&amp;quot;bottom blob&amp;quot;,然后经过layer的计算，输出若干个blob,称之为&amp;quot;top blob&amp;quot;&lt;/p&gt;</description></item><item><title>caffe 源码学习笔记(1) Blob</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part1/</link><pubDate>Fri, 10 Jan 2020 11:24:23 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part1/</guid><description>
&lt;p&gt;迫于生计，开始看caffe代码。
会侧重于分析inference部分。&lt;/p&gt;
&lt;h2 id="blob-整体介绍"&gt;blob 整体介绍&lt;/h2&gt;
&lt;h3 id="blob的含义及目的"&gt;blob的含义及目的&lt;/h3&gt;
&lt;p&gt;blob在逻辑上表示的就是所谓的tensor,blob是tensor在caffe中的叫法。
在框架层面上，blob的意义在于对数据进行封装，提供统一的接口。
这里的数据包含训练/inference时用的数据，也包含模型参数，导数等数据。
深度学习离不开在GPU上的计算。 blob对数据的封装使得用户不必关心和cuda有关的数据传输细节。&lt;/p&gt;</description></item><item><title>Eigen: C++开源矩阵学习笔记</title><link>https://111qqz.com/2018/04/eigen-notes/</link><pubDate>Thu, 05 Apr 2018 07:14:54 +0000</pubDate><guid>https://111qqz.com/2018/04/eigen-notes/</guid><description>
&lt;p&gt;接触Eigen的原因是最近在看caffe/caffe2源码,caffe2中使用了Eigen库. &lt;a href="http://eigen.tuxfamily.org/index.php?title=Main_Page"&gt;Eigen&lt;/a&gt; 是一个基于C++模板的线性代数库，直接将库下载后放在项目目录下，然后包含头文件就能使用，非常方便。对于Linux用户,只需要把头文件放到/usr/include 下即可此外，Eigen的接口清晰，稳定高效。&lt;/p&gt;</description></item></channel></rss>