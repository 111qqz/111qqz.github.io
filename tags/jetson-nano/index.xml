<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jetson Nano on 111qqz的小窝</title><link>https://111qqz.com/tags/jetson-nano/</link><description>Recent content in Jetson Nano on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Fri, 18 Sep 2020 11:02:50 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/jetson-nano/index.xml" rel="self" type="application/rss+xml"/><item><title>【施工中】torch2trt　学习笔记</title><link>https://111qqz.com/2020/09torch2trt/</link><pubDate>Fri, 18 Sep 2020 11:02:50 +0800</pubDate><guid>https://111qqz.com/2020/09torch2trt/</guid><description>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>偶然发现了 &lt;a href="https://github.com/NVIDIA-AI-IOT/torch2trt">torch2trt&lt;/a> 的模型转换方案，思路是直接将pytorch op映射到TensorRT的python api. 在pytorch进行每个op　forward的时候，tensorrt也相应往network上添加op.
这里会先涉及torch2trt的使用，后面会补充这个转换工具的代码学习&lt;/p></description></item><item><title>Jetson Nano踩坑记录</title><link>https://111qqz.com/2020/09jetson-nano/</link><pubDate>Tue, 08 Sep 2020 14:41:34 +0800</pubDate><guid>https://111qqz.com/2020/09jetson-nano/</guid><description>
&lt;h2 id="写在前面">写在前面&lt;/h2>
&lt;p>主要是需要在jetson nano做模型转换，来记录下踩的坑
目前有两条路径，一条是我们现有的转换路径，也就是pytorch-&amp;gt;onnx(-&amp;gt;caffe)-&amp;gt;trt的路径
在这条路径上踩了比较多的坑，最终暂时放弃，最直接的原因是&lt;strong>cudnn8.0升级接口发生改动，编译caffe遇到较多问题&lt;/strong>
这里其实仍然采用了两条平行的路径，一条是直接在nano上构建环境，另外一种是基于docker(包括构建交叉编译环境用于加快编译速度)&lt;/p></description></item></channel></rss>