<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DL-SYS on 111qqz的小窝</title><link>https://111qqz.com/tags/dl-sys/</link><description>Recent content in DL-SYS on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Mon, 05 Apr 2021 14:44:25 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/dl-sys/index.xml" rel="self" type="application/rss+xml"/><item><title>(CSE 599W)Reverse Mode Autodiff</title><link>https://111qqz.com/2021/04/reverse-mode-autodiff/</link><pubDate>Mon, 05 Apr 2021 14:44:25 +0800</pubDate><guid>https://111qqz.com/2021/04/reverse-mode-autodiff/</guid><description>
背景 怎么算微分。。通常有三种方法。
Symbolic Differentiation Numerical Differentiation Automatic Differentiation (auto diff)
auto diff中两种主流的方式分别是forward-mode和reverse-mode 由于forward-mode的方法中，计算的时间复杂度是O(n),n是输入的参数个数。而reverse-mode中，计算的时间复杂度是O(m),m是输出节点的个数。在dnn中，n往往很大，远大于m，因此这里主要介绍reverse-mode auto diff方法。
backprop和reverse mode auto diff的区别 看了reverse mode auto diff的过程，感觉和backprop是一回事呀。。。 实际上，backprop指的是训练神经网络根据loss的gradient来更新weight的过程，而auto diff是backprop使用的一个用来计算gradient的 technique.
Bakpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode auto diff is simply a technique used to compute gradients efficiently and it happens to be used by backpropagation.</description></item></channel></rss>