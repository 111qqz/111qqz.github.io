<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>pipline on 111qqz's blog</title><link>https://111qqz.com/tags/pipline/</link><description>Recent content in pipline on 111qqz's blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Thu, 24 Aug 2017 09:12:58 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/pipline/index.xml" rel="self" type="application/rss+xml"/><item><title>tensorflow input pipline 学习笔记</title><link>https://111qqz.com/2017/08/tensorflow-input-pipline-notes/</link><pubDate>Thu, 24 Aug 2017 09:12:58 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-input-pipline-notes/</guid><description>
参考资料：
tf_doc_Reading data
TENSORFLOW INPUT PIPELINE EXAMPLE
tensorflow：理解tensorflow中的输入管道
第二个参考资料是第一个的翻译版本，翻译的水平一般，建议看原文，不是很长。
下面是我挑了文章中重点的部分+自己的理解。
TL;DR; 一个适用于不是很大的数据集的pipline input 的例子。
Load Data in Tensorflow input pipline 可以理解称一种load data的方式。 一般有两种方式load data,一种是比较传统的，使用feed 的方式。如果数据集比较大，这种方式就不适用了，因为这种方式需要将数据全部导入到memory中。因此tf提供了pipline input的读入数据的方式。
input pipline 会处理 csv file,解码文件格式，重构数据结构，打乱数据顺序，做数据扩充或者其他预处理，然后使用线程(threads)将数据导进batch.
Load the Label Data 确保使用正确的dataset,csv文件路径。
然后处理 得到train和test 的label
由于我们只是读数据而没有真的打算训练，所以没有使用one-hot的编码方式，而是直接将（本来也是由数字字符组成的）字符串，转化成int.
def encode_label(label): return int(label) def read_label_file(file): f = open(file, &amp;quot;r&amp;quot;) filepaths = [] labels = [] for line in f: filepath, label = line.split(&amp;quot;,&amp;quot;) filepaths.append(filepath) labels.append(encode_label(label)) return filepaths, labels # reading labels and file path train_filepaths, train_labels = read_label_file(dataset_path + train_labels_file) test_filepaths, test_labels = read_label_file(dataset_path + test_labels_file) Do Some Optional Processing on Our String Lists # transform relative path into full path train_filepaths = [ dataset_path + fp for fp in train_filepaths] test_filepaths = [ dataset_path + fp for fp in test_filepaths] # for this example we will create or own test partition all_filepaths = train_filepaths + test_filepaths all_labels = train_labels + test_labels # we limit the number of files to 20 to make the output more clear!</description></item></channel></rss>