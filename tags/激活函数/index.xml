<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>激活函数 on 111qqz的小窝</title><link>https://111qqz.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link><description>Recent content in 激活函数 on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sat, 22 Jul 2017 08:56:08 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>stanford cs 231n:常用激活函数</title><link>https://111qqz.com/2017/07/common-activation-functions/</link><pubDate>Sat, 22 Jul 2017 08:56:08 +0000</pubDate><guid>https://111qqz.com/2017/07/common-activation-functions/</guid><description>
其实我觉得这部分可以直接黑箱。。。直接无脑上Leaky ReLU或者Maxou？不过对这些激活函数的特点有个high-level的了解应该总是没坏处的，只要别太纠结细节就好了把。。 每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数： ————————————————————————————————————————
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: /2017/07/common-activation-functions/https://pic3.zhimg.com/677187e96671a4cac9c95352743b3806_b.png
链接到文件: /content/post/ACM-ICPC/https://pic3.zhimg.com/677187e96671a4cac9c95352743b3806_b.png
使用 Page Bundles: true
左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。
————————————————————————————————————————
**Sigmoid。**sigmoid非线性函数的数学公式是 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: /2017/07/common-activation-functions/http://www.zhihu.com/equation?tex=displaystylesigmax11e-x
链接到文件: /content/post/ACM-ICPC/http://www.zhihu.com/equation?tex=displaystylesigmax11e-x
使用 Page Bundles: true
，函数图像如上图的左边所示。在前一节中已经提到过，它输入实数值并将其“挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和（saturated）的激活（1）。然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：
* _Sigmoid函数饱和使梯度消失_。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。 * _Sigmoid函数的输出不是零中心的_。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在!</description></item></channel></rss>