<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PCA on Clarity</title><link>https://111qqz.com/tags/pca/</link><description>Recent content in PCA on Clarity</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright><lastBuildDate>Sun, 26 Nov 2017 11:05:50 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/pca/index.xml" rel="self" type="application/rss+xml"/><item><title>PCA + kmeans</title><link>https://111qqz.com/2017/11/pca-kmeans/</link><pubDate>Sun, 26 Nov 2017 11:05:50 +0000</pubDate><guid>https://111qqz.com/2017/11/pca-kmeans/</guid><description>
先记录一下PCA实战需要用到的安装包(arch下,python2环境) python2-scikit-learn python2-numpy
python2-pandas
python2-matplotlib
python2-seaborn
pandas.DataFrame
pandas 数据结构介绍
几个和科学计算数据分析有关的重要的python库:Numpy、Matplotlib ,pandas
(之前数字图像处理课程都接触过了orz)
其中matplotlib 主要用于图像绘制
sklearn 是用于机器学习的python 模块
Seaborn也是用于图像绘制
str.fomat() 是 python2语法
format中的变量会按照str中{} 出现的顺序替换
import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.datasets import fetch_mldata from sklearn.decomposition import PCA import seaborn as sns mnist = fetch_mldata(&amp;quot;MNIST original&amp;quot;) X = mnist.data / 255.0 y = mnist.target #print X.shape, y.shape feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ] df = pd.</description></item><item><title>Deep Learning Tutorial - PCA and Whitening</title><link>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</link><pubDate>Thu, 06 Jul 2017 08:35:51 +0000</pubDate><guid>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</guid><description>
说下我自己的理解
PCA：主成分分析，是一种预处理手段。对于n维的数据，通过一些手段，把变化显著的k个维度保留，舍弃另外n-k个维度。对于一些非监督学习算法，降低维度可以有效加快运算速度。而n-k个最次要方向的丢失带来的误差不会很大。
PCA的思想是将n维特征映射到k维上（k&amp;lt;n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。
whitening:是一种预处理手段，为了解决数据的冗余问题。比如如果数据是一个16_16的图像，raw data 有16_16=256维度，但是实际上这256个维度不是相互独立的，相邻的像素位置实际上有大关联！
Principal Component Analysis PCA is a method for reducing the number of dimensions in the vectors in a dataset. Essentially, you’re compressing the data by exploiting correlations between some of the dimensions.
Covariance Matrix PCA starts with computing the covariance matrix. I found this tutorial helpful for getting a basic understanding of covariance matrices (I only read a little bit of it to get the basic idea).</description></item></channel></rss>