<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Resnet on 111qqz的小窝</title><link>https://111qqz.com/tags/resnet/</link><description>Recent content in Resnet on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sun, 05 Apr 2020 16:49:44 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/resnet/index.xml" rel="self" type="application/rss+xml"/><item><title>resnet 学习笔记</title><link>https://111qqz.com/2020/04/resnet-learning-notes/</link><pubDate>Sun, 05 Apr 2020 16:49:44 +0800</pubDate><guid>https://111qqz.com/2020/04/resnet-learning-notes/</guid><description>
&lt;h2 id="背景"&gt;背景&lt;/h2&gt;
&lt;p&gt;基于Conv的方法在某年的ImageNet比赛上又重新被人想起之后，大家发现网络堆叠得越深，似乎在cv的各个任务上表现的越好。&lt;/p&gt;
&lt;p&gt;然而事情当然没有无脑退跌深度那么简单，人们发现，当网络深到一定程度时，结果还不如浅一些的网络结构。&lt;/p&gt;</description></item><item><title>Inception-v4,Inception-ResNet 和残差连接对学习的影响</title><link>https://111qqz.com/2017/07/inception-resnet-notes/</link><pubDate>Tue, 18 Jul 2017 02:42:50 +0000</pubDate><guid>https://111qqz.com/2017/07/inception-resnet-notes/</guid><description>
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1602.07261"&gt;原始论文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ask.julyedu.com/question/7711"&gt;翻译链接&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;**——前言：**作者认为残差连接在训练深度卷积模型是很有必要的。至少在图像识别上，我们的研究似乎并不支持这一观点。
&lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;
近年来，深度卷积神经网络对图像识别性能的巨大提升发挥着关键作用。以Inception网络为例，其以相对较低的计算代价取得出色的表现。最近，与传统结构相结合的残差连接网络在2015ILSVRC挑战赛上取得非常优异的成绩；它的性能跟最新的Inception-v3 网络非常接近。因此也就引出了结合残差连接的Inception结构能否对性能进行提高的问题。本文给出实验证明，残差连接可以明显加速Inception网络的训练。同时实验也证明，相比没有残差连接的消耗相似的Inception网络，残差Inception网络在性能上具有微弱的优势。针对是否包含残差连接的Inception网络，本文同时提出了一些新的简化网络。这些网络的变体在ILSVRC2012分类任务上很明显的改善了单一框架的识别性能。本文进一步展示了适当的激活缩放如何使得很宽的残差Inception网络的训练更加稳定。本文通过对三个残差和一个Inception-v4进行组合，在top-5错误率上达到了 3.08%。&lt;/p&gt;</description></item></channel></rss>