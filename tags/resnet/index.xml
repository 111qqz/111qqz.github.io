<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>resnet on 111qqz的小窝</title><link>https://111qqz.com/tags/resnet/</link><description>Recent content in resnet on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sun, 05 Apr 2020 16:49:44 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/resnet/index.xml" rel="self" type="application/rss+xml"/><item><title>resnet 学习笔记</title><link>https://111qqz.com/2020/04/resnet-learning-notes/</link><pubDate>Sun, 05 Apr 2020 16:49:44 +0800</pubDate><guid>https://111qqz.com/2020/04/resnet-learning-notes/</guid><description>
背景 基于Conv的方法在某年的ImageNet比赛上又重新被人想起之后，大家发现网络堆叠得越深，似乎在cv的各个任务上表现的越好。
然而事情当然没有无脑退跌深度那么简单，人们发现，当网络深到一定程度时，结果还不如浅一些的网络结构。
可能第一反应是，网路那么深，多了那么多参数，有那么多数据吗？ overfit了吧
然而情况没有那么简单。如果只是单纯得overfit，那么应该只有test error很高才对。然而现在的情况是training error也很高。
那这是怎么回事呢？ Resnet的团队认为，是因为深层的网络在训练的时候很难收敛。
这个想法是有依据的，因为我们可以通过构造一个较深的网络结构，使得后面的layer学成一个&amp;quot;identity mapping&amp;quot;的函数。这样training error和test error应该至少和一个浅层网络的结果一样好才对。
那么问题很可能就出在，深层的网络没办法学到这样的函数。
基于这样的想法，resnet团队提出了一种新的结构，称之为&amp;quot;skip connection&amp;quot;,来验证该假设。
resnet网络结构 我们可以看到，该结构把原来网络要学的H(x)，变成了F(X)+X的形势。 因此网络只需要学习F(X),也就是在 &amp;quot;identity mapping&amp;quot;上学习一个偏移。
实验表明，这种结构对于深层的网络是非常有效的，因为这种结构将默认设置变为了&amp;quot;identity mapping&amp;quot;,整个网络变得更加容易收敛。
resnet也成了目前工业界各种网络结构的标准backbone
resnet 结构的caffe prototxt 放了resnet50的部分结构，截止到第一个resnet block
12name: &amp;#34;ResNet-50&amp;#34;3input: &amp;#34;data&amp;#34;4input_dim: 15input_dim: 36input_dim: 2247input_dim: 22489layer {10 bottom: &amp;#34;data&amp;#34;11 top: &amp;#34;conv1&amp;#34;12 name: &amp;#34;conv1&amp;#34;13 type: &amp;#34;Convolution&amp;#34;14 convolution_param {15 num_output: 6416 kernel_size: 717 pad: 318 stride: 219 }20}2122layer {23 bottom: &amp;#34;conv1&amp;#34;24 top: &amp;#34;conv1&amp;#34;25 name: &amp;#34;bn_conv1&amp;#34;26 type: &amp;#34;BatchNorm&amp;#34;27 batch_norm_param {28 use_global_stats: true29 }30}3132layer {33 bottom: &amp;#34;conv1&amp;#34;34 top: &amp;#34;conv1&amp;#34;35 name: &amp;#34;scale_conv1&amp;#34;36 type: &amp;#34;Scale&amp;#34;37 scale_param {38 bias_term: true39 }40}4142layer {43 bottom: &amp;#34;conv1&amp;#34;44 top: &amp;#34;conv1&amp;#34;45 name: &amp;#34;conv1_relu&amp;#34;46 type: &amp;#34;ReLU&amp;#34;47}4849layer {50 bottom: &amp;#34;conv1&amp;#34;51 top: &amp;#34;pool1&amp;#34;52 name: &amp;#34;pool1&amp;#34;53 type: &amp;#34;Pooling&amp;#34;54 pooling_param {55 kernel_size: 356 stride: 257 pool: MAX58 }59}6061layer {62 bottom: &amp;#34;pool1&amp;#34;63 top: &amp;#34;res2a_branch1&amp;#34;64 name: &amp;#34;res2a_branch1&amp;#34;65 type: &amp;#34;Convolution&amp;#34;66 convolution_param {67 num_output: 25668 kernel_size: 169 pad: 070 stride: 171 bias_term: false72 }73}7475layer {76 bottom: &amp;#34;res2a_branch1&amp;#34;77 top: &amp;#34;res2a_branch1&amp;#34;78 name: &amp;#34;bn2a_branch1&amp;#34;79 type: &amp;#34;BatchNorm&amp;#34;80 batch_norm_param {81 use_global_stats: true82 }83}8485layer {86 bottom: &amp;#34;res2a_branch1&amp;#34;87 top: &amp;#34;res2a_branch1&amp;#34;88 name: &amp;#34;scale2a_branch1&amp;#34;89 type: &amp;#34;Scale&amp;#34;90 scale_param {91 bias_term: true92 }93}9495layer {96 bottom: &amp;#34;pool1&amp;#34;97 top: &amp;#34;res2a_branch2a&amp;#34;98 name: &amp;#34;res2a_branch2a&amp;#34;99 type: &amp;#34;Convolution&amp;#34;100 convolution_param {101 num_output: 64102 kernel_size: 1103 pad: 0104 stride: 1105 bias_term: false106 }107}108109layer {110 bottom: &amp;#34;res2a_branch2a&amp;#34;111 top: &amp;#34;res2a_branch2a&amp;#34;112 name: &amp;#34;bn2a_branch2a&amp;#34;113 type: &amp;#34;BatchNorm&amp;#34;114 batch_norm_param {115 use_global_stats: true116 }117}118119layer {120 bottom: &amp;#34;res2a_branch2a&amp;#34;121 top: &amp;#34;res2a_branch2a&amp;#34;122 name: &amp;#34;scale2a_branch2a&amp;#34;123 type: &amp;#34;Scale&amp;#34;124 scale_param {125 bias_term: true126 }127}128129layer {130 bottom: &amp;#34;res2a_branch2a&amp;#34;131 top: &amp;#34;res2a_branch2a&amp;#34;132 name: &amp;#34;res2a_branch2a_relu&amp;#34;133 type: &amp;#34;ReLU&amp;#34;134}135136layer {137 bottom: &amp;#34;res2a_branch2a&amp;#34;138 top: &amp;#34;res2a_branch2b&amp;#34;139 name: &amp;#34;res2a_branch2b&amp;#34;140 type: &amp;#34;Convolution&amp;#34;141 convolution_param {142 num_output: 64143 kernel_size: 3144 pad: 1145 stride: 1146 bias_term: false147 }148}149150layer {151 bottom: &amp;#34;res2a_branch2b&amp;#34;152 top: &amp;#34;res2a_branch2b&amp;#34;153 name: &amp;#34;bn2a_branch2b&amp;#34;154 type: &amp;#34;BatchNorm&amp;#34;155 batch_norm_param {156 use_global_stats: true157 }158}159160layer {161 bottom: &amp;#34;res2a_branch2b&amp;#34;162 top: &amp;#34;res2a_branch2b&amp;#34;163 name: &amp;#34;scale2a_branch2b&amp;#34;164 type: &amp;#34;Scale&amp;#34;165 scale_param {166 bias_term: true167 }168}169170layer {171 bottom: &amp;#34;res2a_branch2b&amp;#34;172 top: &amp;#34;res2a_branch2b&amp;#34;173 name: &amp;#34;res2a_branch2b_relu&amp;#34;174 type: &amp;#34;ReLU&amp;#34;175}176177layer {178 bottom: &amp;#34;res2a_branch2b&amp;#34;179 top: &amp;#34;res2a_branch2c&amp;#34;180 name: &amp;#34;res2a_branch2c&amp;#34;181 type: &amp;#34;Convolution&amp;#34;182 convolution_param {183 num_output: 256184 kernel_size: 1185 pad: 0186 stride: 1187 bias_term: false188 }189}190191layer {192 bottom: &amp;#34;res2a_branch2c&amp;#34;193 top: &amp;#34;res2a_branch2c&amp;#34;194 name: &amp;#34;bn2a_branch2c&amp;#34;195 type: &amp;#34;BatchNorm&amp;#34;196 batch_norm_param {197 use_global_stats: true198 }199}200201layer {202 bottom: &amp;#34;res2a_branch2c&amp;#34;203 top: &amp;#34;res2a_branch2c&amp;#34;204 name: &amp;#34;scale2a_branch2c&amp;#34;205 type: &amp;#34;Scale&amp;#34;206 scale_param {207 bias_term: true208 }209}210211layer {212 bottom: &amp;#34;res2a_branch1&amp;#34;213 bottom: &amp;#34;res2a_branch2c&amp;#34;214 top: &amp;#34;res2a&amp;#34;215 name: &amp;#34;res2a&amp;#34;216 type: &amp;#34;Eltwise&amp;#34;217}218219layer {220 bottom: &amp;#34;res2a&amp;#34;221 top: &amp;#34;res2a&amp;#34;222 name: &amp;#34;res2a_relu&amp;#34;223 type: &amp;#34;ReLU&amp;#34;224}可视化的结构为: 重点要关注的是 res2a 这个layer，把两个分支的结果直接加在了一起。 eltwise的layer是按照元素操作的，支持乘积，相加，或者取最大值，默认是相加。 可以参考caffe的proto文件</description></item><item><title>Inception-v4,Inception-ResNet 和残差连接对学习的影响</title><link>https://111qqz.com/2017/07/inception-resnet-notes/</link><pubDate>Tue, 18 Jul 2017 02:42:50 +0000</pubDate><guid>https://111qqz.com/2017/07/inception-resnet-notes/</guid><description>
原始论文
翻译链接
**——前言：**作者认为残差连接在训练深度卷积模型是很有必要的。至少在图像识别上，我们的研究似乎并不支持这一观点。 摘要： 近年来，深度卷积神经网络对图像识别性能的巨大提升发挥着关键作用。以Inception网络为例，其以相对较低的计算代价取得出色的表现。最近，与传统结构相结合的残差连接网络在2015ILSVRC挑战赛上取得非常优异的成绩；它的性能跟最新的Inception-v3 网络非常接近。因此也就引出了结合残差连接的Inception结构能否对性能进行提高的问题。本文给出实验证明，残差连接可以明显加速Inception网络的训练。同时实验也证明，相比没有残差连接的消耗相似的Inception网络，残差Inception网络在性能上具有微弱的优势。针对是否包含残差连接的Inception网络，本文同时提出了一些新的简化网络。这些网络的变体在ILSVRC2012分类任务上很明显的改善了单一框架的识别性能。本文进一步展示了适当的激活缩放如何使得很宽的残差Inception网络的训练更加稳定。本文通过对三个残差和一个Inception-v4进行组合，在top-5错误率上达到了 3.08%。
前言： 自从Krizhevsky等人于2012年赢得Image Net比赛，其网络“AlexNet”已在越来越多的机器视觉任务中得到成功应用，比如目标检测、分割、人体姿态估计、视频分类、目标跟踪、超分辨率等。这些都是使用深度卷积网络的成功案例。 本研究结合最近的两个想法：残差连接和最近的Inception网络结构除了直接的融合，我们也研究了Inception本身通过变得更深更宽能否能变得更加高效。为了实现这个目的，我们设计了一个新版本的Inception-v4，相比Inception-v3，它有更加统一简化的网络结构和更多的inception模块。从历史观点来看，Inception-v3继承了之前的很多方法。技术性局限主要在于使用DistBelief对分布式训练进行模型划分。 如今，将训练迁移到TensorFlow上以后，这些问题也就随之解决，这样就允许我们对结构进行简化。简化的网络结构详见第三节。 在本文中，我们将两个单一Inception变体——Inception-v3和v4与消耗相似的 InceptionResNet混合版本进行比较。这些模型的挑选主要满足以下约束条件，即和非残差模型具有相似的参数和计算复杂度。事实上我们对更深更宽的Inception-ResNet变体也进行测试，它们在ImageNet分类任务上表现性能相似。 最新的实验对组合模型的性能进行了评估。结果显示Inception-v4和Inception-ResNetv2的性能都很好，在ImageNet验证集上其性能已超过业界领先的单个框架模型，我们想看这种结合如何将业界领先水准继续推进，令人惊讶的是，我们发现单个框架性能的提升不会引起组合性能大幅的提高。尽管如此，我们仍然用四个模型组合在验证集上取得了top-5上3.1%的错误率。 在最后一部分，我们分析了分类任务失败的原因，并总结出组合模型在标注数据上的类标噪声上仍然没有达到很好的效果，同时对于预测还有很有大的提升空间。
近期工作： 卷积网络在大规模图像识别任务上的运用非常广泛。主要的模型有Network-in-network、VGGNet、GoogleLeNet(Inception-v1)。残差连接在引文5中提出，并指出附加残差网络对于图像识别尤其是目标检测具有很大的优势，并给出理论和实验验证。作者认为残差连接在训练深度卷积模型是很有必要的。至少在图像识别上，我们的研究似乎并不支持这一观点。然而，残差连接所带来的潜在优势可能需要在更深网络结构中来展现。在实验部分，我们展示了不使用残差连接时深度网络的训练并不难做到。然而，使用残差连接能够极大的提高训练速度，单单这一点就值的肯定。 Inception深度卷积网络被称为GoogleLeNet或Incention-v1。继而我们通过各种方法对 Inception结构进行优化，首先引入batch normalization(Inception-v2)。后来在第三代中增加factorization因子，即本文中提到的Inception-v3。
图1.残差连接
图2.优化版本的ResNet连接 3.结构选择 3.1纯净的Inception模块 我们对以前的Inception模型通过分布式进行训练，将每个副本被划分成一个含多个子网络的模型，以达到在内存中对整个模型进行拟合的目的。然而，Inception结构是高度可调的，这就意味着各层滤波器（filter）的数量可以有多种变化，而整个训练网络的质量不会受到影响。为了优化训练速度，我们对层大小进行调整以平衡不同子网络的计算。 相反，随着TensorFlow的引入，大部分最新的模型无需分布式的对副本进行训练。它通过反向传播（back propagation）进行内存优化，并仔细考虑梯度计算需要的tensors，以及通过结构化计算减少这类tensors的数量。从历史观点来讲，我们对网络结构的更迭已经做得非常保守，并限制实验改变独立网络的组分，同时保持其余网络的稳定性。 由于之前没有对网络进行简化，导致网络看起来更加复杂。在最新的实验中，针对 Inception-v4网络，我们决定丢掉不必要的包袱，对于inception块的每个网格大小进行统一。如图9，展示了大尺寸的Inceptionv4网络结构。图3至8是每个部分的详细结构。所有图中没有标记“V”的卷积使用same的填充原则，意即其输出网格与输入的尺寸正好匹配。使用“V”标记的卷积使用valid的填充原则，意即每个单元输入块全部包含在前几层中，同时输出激活图（output activation map）的网格尺寸也相应会减少。
图3 Inception-v4网络和Inception-ResNet-v2网络的结构。这是网络的输入部分。
图 4 Inception-v4网络35×35网格的框架。对应图9中Inception-A块。
图 5 Incep-v4网络17×17网格块的框架。对应图9中Inception-B块。
图 6 Inception-v4网络的8×8网格模块的框架。对应图9中Inception-C块。
图7 35×35到17×17减少模块的框架。这个块不同的变化（不同滤波器）在图9和15中使用，同时在网络Inception(-v4,-ResNet-v1,-ResNet-v2).k,l,m,n数量表示滤波器尺寸大小，如表1所示。
图 8 17×17到8×8网格缩减框架。减少的模块在图9中Inception-v4网络。</description></item></channel></rss>