<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Whitening on Clarity</title><link>https://111qqz.com/tags/whitening/</link><description>Recent content in Whitening on Clarity</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright><lastBuildDate>Thu, 06 Jul 2017 08:35:51 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/whitening/index.xml" rel="self" type="application/rss+xml"/><item><title>Deep Learning Tutorial - PCA and Whitening</title><link>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</link><pubDate>Thu, 06 Jul 2017 08:35:51 +0000</pubDate><guid>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</guid><description>
说下我自己的理解
PCA：主成分分析，是一种预处理手段。对于n维的数据，通过一些手段，把变化显著的k个维度保留，舍弃另外n-k个维度。对于一些非监督学习算法，降低维度可以有效加快运算速度。而n-k个最次要方向的丢失带来的误差不会很大。
PCA的思想是将n维特征映射到k维上（k&amp;lt;n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。
whitening:是一种预处理手段，为了解决数据的冗余问题。比如如果数据是一个16_16的图像，raw data 有16_16=256维度，但是实际上这256个维度不是相互独立的，相邻的像素位置实际上有大关联！
Principal Component Analysis PCA is a method for reducing the number of dimensions in the vectors in a dataset. Essentially, you’re compressing the data by exploiting correlations between some of the dimensions.
Covariance Matrix PCA starts with computing the covariance matrix. I found this tutorial helpful for getting a basic understanding of covariance matrices (I only read a little bit of it to get the basic idea).</description></item></channel></rss>