<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cuda on 111qqz的小窝</title><link>https://111qqz.com/tags/cuda/</link><description>Recent content in Cuda on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Tue, 24 Mar 2020 12:26:01 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/cuda/index.xml" rel="self" type="application/rss+xml"/><item><title>tensorRT 模型兼容性说明</title><link>https://111qqz.com/2020/03/tensorrt-model-compatibility-notes/</link><pubDate>Tue, 24 Mar 2020 12:26:01 +0800</pubDate><guid>https://111qqz.com/2020/03/tensorrt-model-compatibility-notes/</guid><description>
&lt;h2 id="名词说明"&gt;名词说明&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CUDA. 一般来说指的是CUDA SDK. 目前经常使用的是CUDA 8.0和CUDA 10.1两个版本. 8.0和10.1都是SDK的版本号.&lt;/li&gt;
&lt;li&gt;CUDNN. The NVIDIA CUDA® Deep Neural Network library (cuDNN). 是一个可以为神经网络提供GPU加速的库&lt;/li&gt;
&lt;li&gt;compute capability. 是GPU的固有参数,可以理解为GPU的版本.越新的显卡该数值往往越高.&lt;/li&gt;
&lt;li&gt;tensorRT.NVIDIA TensorRT™ is an SDK for high-performance deep learning inference. 是一个深度学习推理库,旨在提供高性能的推理速度.&lt;/li&gt;
&lt;li&gt;plan file,也称为 engine plan. 是生成的tensorRT 模型文件.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="兼容性说明"&gt;兼容性说明&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Engine plan 的兼容性依赖于GPU的compute capability 和 TensorRT 版本, 不依赖于CUDA和CUDNN版本.&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>CUDA C Best Practices Guide 阅读笔记（二） Heterogeneous Computing</title><link>https://111qqz.com/2018/02/cuda-c-best-practices-guide-heterogeneous-computing/</link><pubDate>Tue, 13 Feb 2018 06:38:38 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-c-best-practices-guide-heterogeneous-computing/</guid><description>
&lt;p&gt;CUDA 编程涉及到在不同的平台上同时运行代码:包含CPU的host 和包含GPU的device.&lt;/p&gt;
&lt;p&gt;所以了解host和device的对性能优化是非常重要的。&lt;/p&gt;</description></item><item><title>CUDA C Best Practices Guide 阅读笔记（1） 并行计算方法论(APOD)</title><link>https://111qqz.com/2018/02/cuda-c-best-practices-parallel-computing-methodology/</link><pubDate>Mon, 12 Feb 2018 04:58:31 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-c-best-practices-parallel-computing-methodology/</guid><description>
&lt;p&gt;APOD指的是Assess, Parallelize, Optimize, Deploy&lt;/p&gt;
&lt;p&gt;
&lt;img class="image_figure image_external" loading="lazy" src="http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/graphics/apod-cycle.png" alt="Assess, Parallelize, Optimize, Deploy." /&gt;
&lt;/p&gt;
&lt;p&gt;如图所示，APOD过程是一个循环的过程，每次只进行一部分，从A到P到O到D,然后再进行下一轮的APOD&lt;/p&gt;</description></item><item><title>cuda c++ 基础算法库 thrust 学习笔记</title><link>https://111qqz.com/2018/02/cuda-thrust-notes/</link><pubDate>Sat, 10 Feb 2018 08:43:54 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-thrust-notes/</guid><description>
&lt;p&gt;可以了解成并行版的STL(?&lt;/p&gt;
&lt;p&gt;过了一遍&lt;a href="http://docs.nvidia.com/cuda/thrust/index.html"&gt;nvidia的官方网文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发现如果熟悉STL的话,thrust没什么太多好说的,看起来很简单...&lt;/p&gt;
&lt;p&gt;不过还是开一篇记录一下,一段时间内估计要和cuda c++ 打交道,就当记录使用过程中遇到的问题吧.&lt;/p&gt;</description></item><item><title>cuda error checking 学习笔记</title><link>https://111qqz.com/2018/02/cuda-error-checking-notes/</link><pubDate>Fri, 09 Feb 2018 06:55:00 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-error-checking-notes/</guid><description>
&lt;p&gt;由于发现cuda c++ 的 debug方式和c++ 差别很大,因此打算再开一篇,专门记录一些和error checking 以及debug有关的内容.&lt;/p&gt;
&lt;p&gt;Error checks in CUDA code can help catch CUDA errors at their source. There are 2 sources of errors in CUDA source code:&lt;/p&gt;</description></item><item><title>cuda 学习笔记</title><link>https://111qqz.com/2018/02/cuda-notes/</link><pubDate>Thu, 01 Feb 2018 07:20:04 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-notes/</guid><description>
&lt;p&gt;uodate:有毒吧。kernel中出问题原来是不会报错的。。。。&lt;/p&gt;
&lt;p&gt;请教了组里的hust学长orz..、&lt;/p&gt;
&lt;p&gt;学到了cuda-memcheck命令和cudaGetLastError来查看问题。。可以参考&lt;a href="https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api"&gt;What is the canonical way to check for errors using the CUDA runtime API?&lt;/a&gt;&lt;/p&gt;</description></item></channel></rss>