<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>reid on Clarity</title><link>https://111qqz.com/tags/reid/</link><description>Recent content in reid on Clarity</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright><lastBuildDate>Sat, 24 Feb 2018 04:34:02 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/reid/index.xml" rel="self" type="application/rss+xml"/><item><title>reid 相关任务记录</title><link>https://111qqz.com/2018/02/reid-task-notes/</link><pubDate>Sat, 24 Feb 2018 04:34:02 +0000</pubDate><guid>https://111qqz.com/2018/02/reid-task-notes/</guid><description>
被师兄（同事？）普及了一番实验规范orz...
我还是太年轻了
所谓的一个fc的版本是右边的．一个放着不动，另一个在sequence_len（１０）的维度上做ave,然后再expand成原来的维度．如下图．
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } Web path: https://111qqz.com/wordpress/wp-content/uploads/2018/02/433671689.jpg
Disk path: /static/https://111qqz.com/wordpress/wp-content/uploads/2018/02/433671689.jpg
Using Page Bundles: false
任务命名规则：
如D1V2_a_1,D1表示使用第一个数据集，V2表示是第二个大版本，ａ表示在V２大版本上的微调，最后的数字表示这是第几次运行该任务（跑三次以减少波动的影响）
logdir的地址为:/mnt/lustre/renkuanze/Data_t1/reid/log/｛$jobname｝
* D1:使用ilivids 数据集 * D1V1表示最初始的　baseline model * D1V2表示改为使用一个fc * D1V2_a是一个在一个FC上，不添加光流的修改版本 * D1V2_b是在一个FC上的baseline版本（也就是有光流） * D1V2_c是在一个FC上，有光流，batchsize从３２改为６４，gpu数目从4改为8的版本 * D1V3表示将softmax改为sigmod * D1V3_b表示将softmax改为sigmod的baseline版本 * D2:使用prid2011数据集 * D2V1表示初始的baseline model * D2V2表示改为使用一个fc * D2V2_b是在一个FC上的baseline版本 *</description></item><item><title>Pose-driven Deep Convolutional Model for Person Re-identification 阅读笔记</title><link>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification/</link><pubDate>Thu, 22 Feb 2018 02:25:00 +0000</pubDate><guid>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification/</guid><description>
1709.08325
Reid问题指的是判断一个probe person 是否在被不同的camera捕获的gallery person 中出现。
通常是如下情景：给出一个特定camera下某个特定人的probe image 或者 video sequence，从其他camera处询问这个人的图像，地点，时间戳。
ReID问题至今没有很好得解决，主要原因是，不同camera下，人的姿势（pose),观察的视角(viewpoint) 变化太大了。
传统方法主要在两个大方向上努力:
1. **用一些在图像上抽取的不变量来作为人的特征feture** 2. **去学习一个不同的距离度量方式，以至于同一个人在不同camera之间的距离尽可能小。** 但是由于在实际中，行人的pose和 摄像机的viewpoint不可控，因此与之相关的feture可能不够健壮。
学习新的不同的距离度量方式需要每对camera分别计算距离，然而这是O(n^2)的时间复杂度，凉凉。
近些年Deep learning发展迅猛，并且在很多CV任务上表现良好。所以自然有人想把Deep learning 方法应用到Reid任务上。
目前Deep learning的做法一般分为两部分：
* 使用softmax loss 结合person ID labels得到一个global representation * 首先用预定义好的body 刚体模型去得到local representation,然后将global 和local representation 融合。 目前用deep learning的方法效果已经不错了，比传统方法要好。但是目前的deep learning方法没有考虑到人的姿势(pose)的变化。
虽然目前也有些deep learning的办法在处理Reid问题时使用pose estimation algorithms 来预测行人的pose,
但是这种办法是手动完成而不是一个end-to-end（什么是end-to-end 神经网络） 的过程
所以考虑pose的潜力还没有被完全发掘。
这篇paper主要做了以下工作：
* 提出了一种新的深层结构，将身体部分转化成相应的特征表示，以此来克服pose变化带来的问题 * 提出了一个用来自动学习各部分权值的sub-network 这两部分工作都是end-to-end的</description></item><item><title>persion reid 论文列表</title><link>https://111qqz.com/2018/02/persion-reid-paper-list/</link><pubDate>Sat, 17 Feb 2018 07:17:49 +0000</pubDate><guid>https://111qqz.com/2018/02/persion-reid-paper-list/</guid><description>
Key:
(1). Pose-driven, body part alignment, combine whole feature and body part feature, focus on alignment of part model,
(2). Combine image label and human attributes classes, do classification with attributes and identity learning
(3). Based on triplet loss, improve metric learning for an end to end learning
(4). Post-process, re-ranking
AlignedReID: Surpassing Human-Level Performance in Person Re-Identification
Hydraplus-net: Attentive deep features for pedestrian analysis.
Darkrank: Accelerating deep metric learning via cross sample similarities transfer.</description></item></channel></rss>