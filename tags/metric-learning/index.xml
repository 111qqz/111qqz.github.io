<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Metric learning on 111qqz的小窝</title><link>https://111qqz.com/tags/metric-learning/</link><description>Recent content in Metric learning on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sun, 18 Feb 2018 08:14:10 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/metric-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Similarity learning 和Metric learning</title><link>https://111qqz.com/2018/02/similarity-learning-metric-learning/</link><pubDate>Sun, 18 Feb 2018 08:14:10 +0000</pubDate><guid>https://111qqz.com/2018/02/similarity-learning-metric-learning/</guid><description>
Similarity_learning 相似性学习（Similarity learning ）有监督机器学习，它与回归和分类密切相关，但目标是从实例中学习一个相似函数，以衡量两个对象的相似程度或相关程度。
Similarity learning通常有四种setups:
* regression similarity learning 在这种方式中，给出的数据是 ![(x_{i}^{1},x_{i}^{2})](https://wikimedia.org/api/rest_v1/media/math/render/svg/cfa249357a1b4a7baf332041d67e480d6bb1f8fb)  和他们的相似度 . 目标是学习到一个函数 ，对于 给出yi的近似值。 * Classification similarity learning 给出数据 和他们是否相似 。目标是训练出一个分类器，能够完成对一组 是否相似的二分类判断。 * Ranking similarity learning 给出有序三元组 ，其中 is known to be more similar to than to 。目标是训练出一个函数，使得对于新的三元组 ，满足 。容易看出，这种方式采取了比回归更弱的监督形式，因为不需要提供精确的相似性度量，只需要提供相似性的相对顺序。因此，这种ranking-based的相似性学习更容易应用于实际的大规模应用 * Locality sensitive hashing (LSH) 局部敏感哈希和普通哈希的不同就是，相似的项有更大的概率被放到同一个桶中。
顺便一提，这里有一个叫triplet loss 的概念，
如上图所示，triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本和不同类的样本,这两个样本对应的称为Positive (记为x_p)和Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。
我们发现，triplet loss 其实就是在ranking similarity learning 问题中，学习similarity function时的loss
Metric learning 相似性学习与距离度量学习密切相关。度量学习的目标是在对象上学习一个距离函数。度量或距离函数必须遵循四个公理:非负性、不可分辨的恒等式、对称性和次可加性/三角形不等式。在实际应用中，度量学习算法忽略了不可分辨物体的身份条件，学习了一个伪度量。</description></item></channel></rss>