<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>梯度下降 on Clarity</title><link>https://111qqz.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</link><description>Recent content in 梯度下降 on Clarity</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright><lastBuildDate>Mon, 10 Jul 2017 01:49:04 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/index.xml" rel="self" type="application/rss+xml"/><item><title>几种梯度下降(GD)法的比较（转载）</title><link>https://111qqz.com/2017/07/Gradient-descent-methods/</link><pubDate>Mon, 10 Jul 2017 01:49:04 +0000</pubDate><guid>https://111qqz.com/2017/07/Gradient-descent-methods/</guid><description>
参考资料
机器学习中梯度下降（Gradient Descent， GD）算法只需要计算损失函数的一阶导数，计算代价小，非常适合训练数据非常大的应用。
梯度下降法的物理意义很好理解，就是沿着当前点的梯度方向进行线搜索，找到下一个迭代点。但是，为什么有会派生出 batch、mini-batch、online这些GD算法呢？
原来，batch、mini-batch、SGD、online的区别在于训练数据的选择上：
** ** **batch** **mini-batch** **Stochastic** **Online** **训练集** 固定 固定 固定 实时更新 **单次迭代样本数** 整个训练集 训练集的子集 单个样本 根据具体算法定 **算法复杂度** 高 一般 低 低 **时效性** 低 一般（delta 模型） 一般（delta 模型） 高 **收敛性** 稳定 较稳定 不稳定 不稳定 1.</description></item></channel></rss>