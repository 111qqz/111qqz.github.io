<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>计算机视觉 on 111qqz的小窝</title><link>https://111qqz.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link><description>Recent content in 计算机视觉 on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sun, 05 Apr 2020 20:38:28 +0800</lastBuildDate><atom:link href="https://111qqz.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/index.xml" rel="self" type="application/rss+xml"/><item><title>Faster Rcnn 目标检测算法</title><link>https://111qqz.com/2020/04/faster-rcnn/</link><pubDate>Sun, 05 Apr 2020 20:38:28 +0800</pubDate><guid>https://111qqz.com/2020/04/faster-rcnn/</guid><description>
背景 2019年对了好几次faster rcnn，第一次是赛事之窗项目和北京的同事，对齐sdk和训练的实现。 第二次是被tensorRT4和tensorRT5之间默认参数不一致的问题坑了一下。 第三次是被caffe proto中roi align 的默认参数坑了。
虽然debug了这么多次，踩了一堆坑，但是一段时间不用，细节就会慢慢不记得了。因此来记录一下。
faster rcnn，是一种&amp;quot;two stage&amp;quot;的目标检测算法。
所谓&amp;quot;two stage&amp;quot;，是说在实际进行目标检测之前，先会通过某种&amp;quot;region proposals&amp;quot; algorithm，来获得一定数量的RoI(Regions of Interest),我们下一阶段要检测的obejct有极大可能被包含在这些RoI. 这种&amp;quot;Region based&amp;quot;的方法是对基于&amp;quot;sliding windows&amp;quot;方法的极大改进，因为不需要遍历每一个可能的位置以及crop大小，只需要对这些RoI进行检测，有效地减小了计算量。
下面简单说一下这一类&amp;quot;Region based&amp;quot;方法的历史脉络
rcnn RCNN的做法是通过一种传统方法&amp;quot;selective search&amp;quot;来得到若干RoI,然后把每一个RoI，后面接CNN进行后续的检测。 显然，这个方法的问题在于计算量非常大。
selective search的策略是，既然是不知道尺度是怎样的，那我们就尽可能遍历所有的尺度，但是不同于暴力穷举，我们可以先得到小尺度的区域，然后一次次合并得到大的尺寸.
fast rcnn 明眼人可以看出，rcnn计算量过大的原因之一是做了非常多的重复计算。
因此fast rcnn做的改进是，与其把每一个通过&amp;quot;selective search&amp;quot;得到的RoI在原图上crop出来送进CNN,不如先让整张图过一段CNN,然后把通过&amp;quot;selective search&amp;quot;在原图上得到的RoI先映射到这段CNN的某个conv feature map. 相当于这部分CNN只做了一次，与RoI数量无关，极大地减小了计算量。
faster rcnn 终于轮到主角登场了。 fast rcnn极大提高了检测的速度。 然后发现，速度的瓶颈已经不在后续的检测部分了，而是在于“region proposals” algorithm.
于是，faster-rcnn提出&amp;quot;Region proposal network&amp;quot;来替代&amp;quot;selective search&amp;quot;,进一步提高了检测速度。
放一张结构图，非常清楚。 Region proposal network(RPN) anchor 介绍RPN网络首先就要介绍一下anchor.
（被坑过一次，某个足球项目上，training和inference用的anchor竟然是不一致的。。）
其实anchor这个概念很简单，用大白话说就是，根据要检测的物体的形状（高矮胖瘦等），预先 设置一些不同尺寸（高矮胖瘦）的粗略的框，然后对这些框做一个二分类，判断前景还是背景，同时做bbox regression 来微调坐标，最终得到proposals.
设置anchor的思路其实就是修改了proposals的默认位置为生成的anchors的位置。对这些anchors进行微调总要比从零开始生成容易得多。
要注意的是，anchors是在进入网络前预先生成的。 实际项目中，通常设置长宽比为[1:1,2:1,1:2]三种比例，然后通过 generate_anchors.py 来生成anchors.</description></item><item><title>resnet 学习笔记</title><link>https://111qqz.com/2020/04/resnet-learning-notes/</link><pubDate>Sun, 05 Apr 2020 16:49:44 +0800</pubDate><guid>https://111qqz.com/2020/04/resnet-learning-notes/</guid><description>
背景 基于Conv的方法在某年的ImageNet比赛上又重新被人想起之后，大家发现网络堆叠得越深，似乎在cv的各个任务上表现的越好。
然而事情当然没有无脑退跌深度那么简单，人们发现，当网络深到一定程度时，结果还不如浅一些的网络结构。
可能第一反应是，网路那么深，多了那么多参数，有那么多数据吗？ overfit了吧
然而情况没有那么简单。如果只是单纯得overfit，那么应该只有test error很高才对。然而现在的情况是training error也很高。
那这是怎么回事呢？ Resnet的团队认为，是因为深层的网络在训练的时候很难收敛。
这个想法是有依据的，因为我们可以通过构造一个较深的网络结构，使得后面的layer学成一个&amp;quot;identity mapping&amp;quot;的函数。这样training error和test error应该至少和一个浅层网络的结果一样好才对。
那么问题很可能就出在，深层的网络没办法学到这样的函数。
基于这样的想法，resnet团队提出了一种新的结构，称之为&amp;quot;skip connection&amp;quot;,来验证该假设。
resnet网络结构 我们可以看到，该结构把原来网络要学的H(x)，变成了F(X)+X的形势。 因此网络只需要学习F(X),也就是在 &amp;quot;identity mapping&amp;quot;上学习一个偏移。
实验表明，这种结构对于深层的网络是非常有效的，因为这种结构将默认设置变为了&amp;quot;identity mapping&amp;quot;,整个网络变得更加容易收敛。
resnet也成了目前工业界各种网络结构的标准backbone
resnet 结构的caffe prototxt 放了resnet50的部分结构，截止到第一个resnet block
12name: &amp;#34;ResNet-50&amp;#34;3input: &amp;#34;data&amp;#34;4input_dim: 15input_dim: 36input_dim: 2247input_dim: 22489layer {10 bottom: &amp;#34;data&amp;#34;11 top: &amp;#34;conv1&amp;#34;12 name: &amp;#34;conv1&amp;#34;13 type: &amp;#34;Convolution&amp;#34;14 convolution_param {15 num_output: 6416 kernel_size: 717 pad: 318 stride: 219 }20}2122layer {23 bottom: &amp;#34;conv1&amp;#34;24 top: &amp;#34;conv1&amp;#34;25 name: &amp;#34;bn_conv1&amp;#34;26 type: &amp;#34;BatchNorm&amp;#34;27 batch_norm_param {28 use_global_stats: true29 }30}3132layer {33 bottom: &amp;#34;conv1&amp;#34;34 top: &amp;#34;conv1&amp;#34;35 name: &amp;#34;scale_conv1&amp;#34;36 type: &amp;#34;Scale&amp;#34;37 scale_param {38 bias_term: true39 }40}4142layer {43 bottom: &amp;#34;conv1&amp;#34;44 top: &amp;#34;conv1&amp;#34;45 name: &amp;#34;conv1_relu&amp;#34;46 type: &amp;#34;ReLU&amp;#34;47}4849layer {50 bottom: &amp;#34;conv1&amp;#34;51 top: &amp;#34;pool1&amp;#34;52 name: &amp;#34;pool1&amp;#34;53 type: &amp;#34;Pooling&amp;#34;54 pooling_param {55 kernel_size: 356 stride: 257 pool: MAX58 }59}6061layer {62 bottom: &amp;#34;pool1&amp;#34;63 top: &amp;#34;res2a_branch1&amp;#34;64 name: &amp;#34;res2a_branch1&amp;#34;65 type: &amp;#34;Convolution&amp;#34;66 convolution_param {67 num_output: 25668 kernel_size: 169 pad: 070 stride: 171 bias_term: false72 }73}7475layer {76 bottom: &amp;#34;res2a_branch1&amp;#34;77 top: &amp;#34;res2a_branch1&amp;#34;78 name: &amp;#34;bn2a_branch1&amp;#34;79 type: &amp;#34;BatchNorm&amp;#34;80 batch_norm_param {81 use_global_stats: true82 }83}8485layer {86 bottom: &amp;#34;res2a_branch1&amp;#34;87 top: &amp;#34;res2a_branch1&amp;#34;88 name: &amp;#34;scale2a_branch1&amp;#34;89 type: &amp;#34;Scale&amp;#34;90 scale_param {91 bias_term: true92 }93}9495layer {96 bottom: &amp;#34;pool1&amp;#34;97 top: &amp;#34;res2a_branch2a&amp;#34;98 name: &amp;#34;res2a_branch2a&amp;#34;99 type: &amp;#34;Convolution&amp;#34;100 convolution_param {101 num_output: 64102 kernel_size: 1103 pad: 0104 stride: 1105 bias_term: false106 }107}108109layer {110 bottom: &amp;#34;res2a_branch2a&amp;#34;111 top: &amp;#34;res2a_branch2a&amp;#34;112 name: &amp;#34;bn2a_branch2a&amp;#34;113 type: &amp;#34;BatchNorm&amp;#34;114 batch_norm_param {115 use_global_stats: true116 }117}118119layer {120 bottom: &amp;#34;res2a_branch2a&amp;#34;121 top: &amp;#34;res2a_branch2a&amp;#34;122 name: &amp;#34;scale2a_branch2a&amp;#34;123 type: &amp;#34;Scale&amp;#34;124 scale_param {125 bias_term: true126 }127}128129layer {130 bottom: &amp;#34;res2a_branch2a&amp;#34;131 top: &amp;#34;res2a_branch2a&amp;#34;132 name: &amp;#34;res2a_branch2a_relu&amp;#34;133 type: &amp;#34;ReLU&amp;#34;134}135136layer {137 bottom: &amp;#34;res2a_branch2a&amp;#34;138 top: &amp;#34;res2a_branch2b&amp;#34;139 name: &amp;#34;res2a_branch2b&amp;#34;140 type: &amp;#34;Convolution&amp;#34;141 convolution_param {142 num_output: 64143 kernel_size: 3144 pad: 1145 stride: 1146 bias_term: false147 }148}149150layer {151 bottom: &amp;#34;res2a_branch2b&amp;#34;152 top: &amp;#34;res2a_branch2b&amp;#34;153 name: &amp;#34;bn2a_branch2b&amp;#34;154 type: &amp;#34;BatchNorm&amp;#34;155 batch_norm_param {156 use_global_stats: true157 }158}159160layer {161 bottom: &amp;#34;res2a_branch2b&amp;#34;162 top: &amp;#34;res2a_branch2b&amp;#34;163 name: &amp;#34;scale2a_branch2b&amp;#34;164 type: &amp;#34;Scale&amp;#34;165 scale_param {166 bias_term: true167 }168}169170layer {171 bottom: &amp;#34;res2a_branch2b&amp;#34;172 top: &amp;#34;res2a_branch2b&amp;#34;173 name: &amp;#34;res2a_branch2b_relu&amp;#34;174 type: &amp;#34;ReLU&amp;#34;175}176177layer {178 bottom: &amp;#34;res2a_branch2b&amp;#34;179 top: &amp;#34;res2a_branch2c&amp;#34;180 name: &amp;#34;res2a_branch2c&amp;#34;181 type: &amp;#34;Convolution&amp;#34;182 convolution_param {183 num_output: 256184 kernel_size: 1185 pad: 0186 stride: 1187 bias_term: false188 }189}190191layer {192 bottom: &amp;#34;res2a_branch2c&amp;#34;193 top: &amp;#34;res2a_branch2c&amp;#34;194 name: &amp;#34;bn2a_branch2c&amp;#34;195 type: &amp;#34;BatchNorm&amp;#34;196 batch_norm_param {197 use_global_stats: true198 }199}200201layer {202 bottom: &amp;#34;res2a_branch2c&amp;#34;203 top: &amp;#34;res2a_branch2c&amp;#34;204 name: &amp;#34;scale2a_branch2c&amp;#34;205 type: &amp;#34;Scale&amp;#34;206 scale_param {207 bias_term: true208 }209}210211layer {212 bottom: &amp;#34;res2a_branch1&amp;#34;213 bottom: &amp;#34;res2a_branch2c&amp;#34;214 top: &amp;#34;res2a&amp;#34;215 name: &amp;#34;res2a&amp;#34;216 type: &amp;#34;Eltwise&amp;#34;217}218219layer {220 bottom: &amp;#34;res2a&amp;#34;221 top: &amp;#34;res2a&amp;#34;222 name: &amp;#34;res2a_relu&amp;#34;223 type: &amp;#34;ReLU&amp;#34;224}可视化的结构为: 重点要关注的是 res2a 这个layer，把两个分支的结果直接加在了一起。 eltwise的layer是按照元素操作的，支持乘积，相加，或者取最大值，默认是相加。 可以参考caffe的proto文件</description></item></channel></rss>