<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Latent Factor Model on 111qqz的小窝</title><link>https://111qqz.com/tags/latent-factor-model/</link><description>Recent content in Latent Factor Model on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Fri, 09 Feb 2018 13:02:06 +0000</lastBuildDate><atom:link href="https://111qqz.com/tags/latent-factor-model/index.xml" rel="self" type="application/rss+xml"/><item><title>推荐系统之 LFM (Latent Factor Model) 隐因子模型 学习笔记</title><link>https://111qqz.com/2018/02/Latent-Factor-Model-notes/</link><pubDate>Fri, 09 Feb 2018 13:02:06 +0000</pubDate><guid>https://111qqz.com/2018/02/Latent-Factor-Model-notes/</guid><description>
起因是被assgin了一个新的任务.....要死.
参考资料:
推荐系统学习笔记之三 LFM (Latent Factor Model) 隐因子模型 + SVD (singular value decomposition) 奇异值分解
基于矩阵分解的隐因子模型
实时推荐系统的三种方式
先说下我的理解...
隐因子模型(LFM)是一种推荐算法,&amp;quot;隐&amp;quot;可以理解成用户喜欢某个item的间接原因.
该算法的核心思想是转化成一个矩阵分解问题..
然后用传统机器学习算法去优化分解得到的矩阵...
主要的优势如下： * 比较容易编程实现，随机梯度下降方法依次迭代即可训练出模型。 * 预测的精度比较高，预测准确率要高于基于领域的协同过滤以及基于内容CBR等方法。 * 比较低的时间和空间复杂度，高维矩阵映射为两个低维矩阵节省了存储空间，训练过程比较费时，但是可以离线完成；评分预测一般在线计算，直接使用离线训练得到的参数，可以实时推荐。 * 非常好的扩展性，如由SVD拓展而来的SVD++和 TIME SVD++。 矩阵分解的不足主要有：
* 训练模型较为费时。 * 推荐结果不具有很好的可解释性，无法用现实概念给分解出来的用户和物品矩阵的每个维度命名，只能理解为潜在语义空间。 我们用user1,2,3表示用户，item 1,2,3表示物品，Rij表示用户i对于物品j的评分，也就是喜好度。那么我们需要得到一个关于用户-物品的二维矩阵，如下面的R。
常见的系统中，R是一个非常稀疏的矩阵，因为我们不可能得到所有用户对于所有物品的评分。于是利用稀疏的R，填充得到一个满矩阵R’就是我们的目的。
下面我们就来看看LFM是如何解决上面的问题的？对于一个给定的用户行为数据集（数据集包含的是所有的user, 所有的item，以及每个user有过行为的item列表），使用LFM对其建模后，我们可以得到如下图所示的模型：（假设数据集中有3个user, 4个item, LFM建模的分类数为4）
 R矩阵是user-item矩阵，矩阵值Rij表示的是user i 对item j的兴趣度，这正是我们要求的值。对于一个user来说，当计算出他对所有item的兴趣度后，就可以进行排序并作出推荐。LFM算法从数据集中抽取出若干主题，作为user和item之间连接的桥梁，将R矩阵表示为P矩阵和Q矩阵相乘。其中P矩阵是user-class矩阵，矩阵值Pij表示的是user i对class j的兴趣度；Q矩阵式class-item矩阵，矩阵值Qij表示的是item j在class i中的权重，权重越高越能作为该类的代表。所以LFM根据如下公式来计算用户U对物品I的兴趣度
我们发现使用LFM后，
1. 我们不需要关心分类的角度，结果都是基于用户行为统计自动聚类的，全凭数据自己说了算。 2. 不需要关心分类粒度的问题，通过设置LFM的最终分类数就可控制粒度，分类数越大，粒度约细。 3. 对于一个item，并不是明确的划分到某一类，而是计算其属于每一类的概率，是一种标准的软分类。 4.</description></item></channel></rss>