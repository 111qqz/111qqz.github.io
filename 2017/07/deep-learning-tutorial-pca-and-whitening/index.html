<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="111qqz的小窝"><meta property="og:type" content="article"><meta property="og:image" content="https://111qqz.github.io/img/2.png"><meta property="twitter:image" content="https://111qqz.github.io/img/2.png"><meta name=title content="Deep Learning Tutorial - PCA and Whitening"><meta property="og:title" content="Deep Learning Tutorial - PCA and Whitening"><meta property="twitter:title" content="Deep Learning Tutorial - PCA and Whitening"><meta name=description content><meta property="og:description" content><meta property="twitter:description" content><meta property="twitter:card" content="summary"><meta name=keyword content="ACM,111qqz,商汤科技,hust,华中科技大学"><link rel="shortcut icon" href=/img/favicon.ico><title>Deep Learning Tutorial - PCA and Whitening-111qqz的小窝</title><link rel=canonical href=/2017/07/deep-learning-tutorial-pca-and-whitening/><link rel=stylesheet href=/css/iDisqus.min.css><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hux-blog.min.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/zanshang.css><link href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css rel=stylesheet type=text/css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/docco.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/css/hux-blog.min.css><link rel=stylesheet href=/css/hux-blog.min-custom.css></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a class=navbar-brand href=/>111qqz的小窝</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>Home</a></li><li><a href=/categories/acm/>ACM-ICPC</a></li><li><a href=/categories/deep-learning/>深度学习</a></li><li><a href=/categories/mooc/>公开课</a></li><li><a href=/categories/%e5%85%b6%e4%bb%96/>其他</a></li><li><a href=/top/about/>ABOUT</a></li><li><a href=/search>SEARCH <img src=/img/search.png height=15 style=cursor:pointer alt=Search></a></li></ul></div></div></div></nav><script>var $body=document.body;var $toggle=document.querySelector('.navbar-toggle');var $navbar=document.querySelector('#huxblog_navbar');var $collapse=document.querySelector('.navbar-collapse');$toggle.addEventListener('click',handleMagic)
function handleMagic(e){if($navbar.className.indexOf('in')>0){$navbar.className=" ";setTimeout(function(){if($navbar.className.indexOf('in')<0){$collapse.style.height="0px"}},400)}else{$collapse.style.height="auto"
$navbar.className+=" in";}}</script><style type=text/css>header.intro-header{background-image:url(/img/2.png)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/pca title=PCA>PCA</a>
<a class=tag href=/tags/whitening title=Whitening>Whitening</a>
<a class=tag href=/tags/%E9%A2%84%E5%A4%84%E7%90%86 title=预处理>预处理</a></div><h1>Deep Learning Tutorial - PCA and Whitening</h1><h2 class=subheading></h2><span class=meta>Posted by
111qqz
on
Thursday, July 6, 2017
<span id=/2017/07/deep-learning-tutorial-pca-and-whitening/ class="leancloud_visitors meta_data_item" data-flag-title><span class=post-meta-item-icon><span class="octicon octicon-eye"></span></span><i class="fa fa-eye"></i><span class=old-visitors-count style=display:none></span><span class=leancloud-visitors-count></span></span><script src=https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js></script><script>AV.initialize("2dzJwxGKq4hbtg5R5NM8NTzJ-gzGzoHsz","RaYu8uGTiuiIjLISQppPVYWw");</script><script type=text/javascript>function showTime(Counter){var query=new AV.Query(Counter);var entries=[];var $visitors=$(".leancloud_visitors");$visitors.each(function(){entries.push($(this).attr("id").trim());});query.containedIn('url',entries);query.find().done(function(results){var COUNT_CONTAINER_REF='.leancloud-visitors-count';var OLD_COUNT_CONTAINER_REF='.old-visitors-count';for(var i=0;i<results.length;i++){var item=results[i];var url=item.get('url');var time=item.get('time');var element=document.getElementById(url);$(element).find(COUNT_CONTAINER_REF).text(time);}
for(var i=0;i<entries.length;i++){var url=entries[i];var element=document.getElementById(url);var countSpan=$(element).find(COUNT_CONTAINER_REF);if(countSpan.text()==''){var oldCountSpan=$(element).find(OLD_COUNT_CONTAINER_REF).text();if(oldCountSpan!=''){countSpan.text(0+parseInt(oldCountSpan));}else{countSpan.text(0);}}}}).fail(function(object,error){console.log("Error: "+error.code+" "+error.message);});}
function addCount(Counter){var $visitors=$(".leancloud_visitors");var url=$visitors.attr('id').trim();var title=$visitors.attr('data-flag-title').trim();var query=new AV.Query(Counter);query.equalTo("url",url);query.find({success:function(results){if(results.length>0){var counter=results[0];counter.fetchWhenSave(true);counter.increment("time");counter.save(null,{success:function(counter){var $element=$(document.getElementById(url));$element.find('.leancloud-visitors-count').text(counter.get('time'));},error:function(counter,error){console.log('Failed to save Visitor num, with error message: '+error.message);}});}else{var newcounter=new Counter();var acl=new AV.ACL();acl.setPublicReadAccess(true);acl.setPublicWriteAccess(true);newcounter.setACL(acl);newcounter.set("title",title);newcounter.set("url",url);var OLD_COUNT_CONTAINER_REF='.old-visitors-count';var $element=$(document.getElementById(url));var oldCountSpan=$element.find(OLD_COUNT_CONTAINER_REF).text();if(oldCountSpan!=''){newcounter.set("time",parseInt(oldCountSpan)+1);}else{newcounter.set("time",1);}
newcounter.save(null,{success:function(newcounter){var $element=$(document.getElementById(url));$element.find('.leancloud-visitors-count').text(newcounter.get('time'));},error:function(newcounter,error){console.log('Failed to create');}});}},error:function(error){console.log('Error:'+error.code+" "+error.message);}});}
$(function(){var Counter=AV.Object.extend("Counter");if($('.leancloud_visitors').length==1){addCount(Counter);}else{showTime(Counter);}});</script></span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><header><h2>TOC</h2></header><nav id=TableOfContents><ol><li><ol><li><a href=#principal-component-analysis>Principal Component Analysis</a></li><li><a href=#covariance-matrix>Covariance Matrix</a></li><li><a href=#projecting-onto-an-eigenvector>Projecting onto an eigenvector</a></li><li><a href=#whitening>Whitening</a></li><li><a href=#pca-in-2d-exercise>PCA in 2D Exercise</a></li><li><a href=#pca-and-whitening-exercise>PCA and Whitening Exercise</a></li></ol></li></ol></nav><p>说下我自己的理解</p><p>PCA：主成分分析，是一种预处理手段。对于n维的数据，通过一些手段，把变化显著的k个维度保留，舍弃另外n-k个维度。对于一些非监督学习算法，降低维度可以有效加快运算速度。而n-k个最次要方向的丢失带来的误差不会很大。</p><p>PCA的思想是将n维特征映射到k维上（k&lt;n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。</p><p>whitening:是一种预处理手段，为了解决数据的冗余问题。比如如果数据是一个16_16的图像，raw data 有16_16=256维度，但是实际上这256个维度不是相互独立的，相邻的像素位置实际上有大关联！</p><blockquote><blockquote><h3 id=principal-component-analysis>Principal Component Analysis</h3></blockquote><p>PCA is a method for reducing the number of dimensions in the vectors in a dataset. Essentially, you’re compressing the data by exploiting correlations between some of the dimensions.</p><blockquote><h3 id=covariance-matrix>Covariance Matrix</h3></blockquote><p>PCA starts with computing the covariance matrix. I found <a href=http://stattrek.com/matrix-algebra/covariance-matrix.aspx>this tutorial</a> helpful for getting a basic understanding of covariance matrices (I only read a little bit of it to get the basic idea).</p><p>The following equation is presented for computing the covariance matrix.</p><p><a href=https://chrisjmccormick.files.wordpress.com/2014/06/covariance.png><img src=http://chrisjmccormick.files.wordpress.com/2014/06/covariance.png alt=Covariance></a></p><p>Note that the placement of the transpose operator creates a matrix here, not a single value.</p><p>Note that this function only computes the covariance matrix if the mean is zero. The proper function would be based on (x - mu)(x - mu)^T. For the tutorial example, the dataset has been adjusted to have a mean of 0.</p><p>For 2D data, here are the equations for each individual cell of the 2x2 covariance matrix, so that you can get more of a feel for what each element represents.</p><p><a href=https://chrisjmccormick.files.wordpress.com/2014/06/covariancematrix.png><img src=http://chrisjmccormick.files.wordpress.com/2014/06/covariancematrix.png alt=CovarianceMatrix></a></p><p>If you subtract the means from the dataset ahead of time, then you can drop the “minus mu” terms from these equations. Subtracting the means causes the dataset to be centered around (0, 0).</p><p>The top-left corner is just a measure of how much the data varies along the x_1 dimension. Similarly, the bottom-right corner is the variance in the x_2 dimension.</p><p>The bottom-left and top-right corners are identical. These indicate the correlation between x_1 and x_2.</p><p>If the data is mainly in quadrants one and three, then all of the x_1 * x_2 products are going to be positive, so there’s a <em>positive</em> correlation between x_1 and x_2.</p><p><a href=https://chrisjmccormick.files.wordpress.com/2014/06/positivecorrelation.png><img src=http://chrisjmccormick.files.wordpress.com/2014/06/positivecorrelation.png alt=PositiveCorrelation></a></p><p>If the data is all in quadrants two and four, then the all of the products will be negative, so there’s a <em>negative</em> correlation between x_1 and x_2.</p><p><a href=https://chrisjmccormick.files.wordpress.com/2014/06/negativecorrelation.png><img src=http://chrisjmccormick.files.wordpress.com/2014/06/negativecorrelation.png alt=NegativeCorrelation></a></p><p>If the data is evenly dispersed in all four quadrants, then the positive and negative products will cancel out, and the covariance will be roughly zero. This indicates that there is _no _correlation.</p><p><a href=https://chrisjmccormick.files.wordpress.com/2014/06/nocorrelation.png><img src=http://chrisjmccormick.files.wordpress.com/2014/06/nocorrelation.png alt=NoCorrelation></a></p><p><strong>Eigenvectors</strong></p><p>After calculating the covariance matrix for the dataset, the next step is to compute the  eigenvectors of the covariance matrix. I found <a href=http://www.math.hmc.edu/calculus/tutorials/eigenstuff/>this tutorial</a>helpful. Again, I only skimmed it and got a high level understanding.</p><p>The eigenvectors of the covariance matrix have the property that they point along the major directions of variation in the data.</p><p><img src=http://ufldl.stanford.edu/wiki/images/thumb/b/b4/PCA-u1.png/600px-PCA-u1.png alt=PCA-u1.png></p><p>Why this is the case is beyond me. I suspect you’d have to be more intimately acquainted with how the eigenvectors are found in order to understand why they have this property. So I’m just taking it as a given for now.</p><blockquote><h3 id=projecting-onto-an-eigenvector>Projecting onto an eigenvector</h3></blockquote><p>The tutorial explains that taking the dot product between a data point, x, and an eigen vector, u_1, gives you “the length (magnitude) of the projection of<img src=http://deeplearning.stanford.edu/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png alt="\textstyle x">
onto the vector<img src=http://deeplearning.stanford.edu/wiki/images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png alt="\textstyle u_1">
. “</p><p>Take a look at the Wikipedia article on <a href=http://en.wikipedia.org/wiki/Scalar_projection>Scalar Projection</a> to help understand what this means.</p><p>The resulting scalar value is a point along the line formed by the eigen vector.</p><p>What’s actually involved, then, in reducing a 256 dimensional vector down to 50 dimensional vector? You will be taking the dot product between your 256-dimensional vector x and each of the top 50 eigen vectors.</p><p>It was interesting to me to note that this is equivalent to evaluating a neural network with 256 inputs and 50 hidden units, but with no activation function on the hidden units (i.e., no sigmoid function).</p><blockquote><h3 id=whitening>Whitening</h3></blockquote><p>There are two things we are trying to accomplish with whitening:</p><blockquote></blockquote><pre><code>  1. Make the features less correlated with one another.
  2. Give all of the features the same variance.
</code></pre><p>Whitening has two simple steps:</p><pre><code>  1. Project the dataset onto the eigenvectors. This rotates the dataset so that there is no correlation between the components.
  2. Normalize the the dataset to have a variance of 1 for all components. This is done by simply dividing each component by the square root of its eigenvalue.
</code></pre><p>I asked a Neural Network expect I’m connected with, <a href=http://www.pawlin.com/>Pavel Skribtsov</a>, for more of an explanation on why this technique is beneficial:</p><p>&ldquo;This is a common trick to simplify optimization process to find weights. If the input signal has correlating inputs (some linear dependency) then the [cost] function will tend to have &ldquo;river-like&rdquo; minima regions rather than minima points in weights space. As to input whitening - similar thing - if you don't do it - error function will tend to have non-symmetrical minima &ldquo;caves&rdquo; and since some training algorithms have equal speed of update for all weights - the minimization may tend to skip good places in narrow dimensions of the minima while trying to please the wider ones. So it does not directly relate to deep learning. If your optimization process converges well - you can skip this pre-processing.&rdquo;</p><blockquote><h3 id=pca-in-2d-exercise>PCA in 2D Exercise</h3></blockquote><p>This exercise is pretty straightforward. A few notes, though:</p><blockquote></blockquote><pre><code>  * Note that you don’t need to adjust the data to have a mean of 0, it’s already close enough.
  * In step 1a, where it plots the eigen vectors, your plot area needs to be square in order for it to look right. Mine was a rectangle at first (from a previous plot) and it threw me off–it made the second eigen vector look wrong.

    * The command axis(“square”) is supposed to do this, but for seem reason it gives my plot a 5:4 ratio, not 1:1. What a pain!
</code></pre><blockquote><h3 id=pca-and-whitening-exercise>PCA and Whitening Exercise</h3></blockquote><blockquote></blockquote><pre><code>  * If you are using Octave instead of Matlab, there’s a modification you’ll need to make to line 93 of display_network.m. Remove the arguments ‘EraseMode’ and ‘none’.
  * When subtracting the mean, the instructions say to calculate the mean per image, but the code says to calculate it per row (per pixel). This [section](http://ufldl.stanford.edu/wiki/index.php/PCA#PCA_on_Images) of the tutorial describes why they compute it per image for natural images.
  * You can use the command “colorbar” to add a color legend to the plot for the imagesc command.
</code></pre><p>Using 116 out of 144 principal components preserved 99% of the variance.</p><p>Here is the final output of my code, showing the original image patches and the whitened images.</p><p>Before whitening:</p><p><a href=https://chrisjmccormick.files.wordpress.com/2014/06/imagepatches_prewhitening.png><img src=http://chrisjmccormick.files.wordpress.com/2014/06/imagepatches_prewhitening.png alt=imagePatches_preWhitening></a></p><p>After whitening:</p><p><a href=https://chrisjmccormick.files.wordpress.com/2014/06/imagepatches_withwhitening.png><img src=http://chrisjmccormick.files.wordpress.com/2014/06/imagepatches_withwhitening.png alt=imagePatches_withWhitening></a></blockquote></p><p>提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。</p><p><strong>常见错误：进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong></p><hr><ul class=pager><li class=previous><a href=/2017/06/install-qq-on-manjaro/ data-toggle=tooltip data-placement=top title="archlinux/manjaro 下 安装 qq/tim">&larr;
Previous Post</a></li><li class=next><a href=/2017/07/Gradient-descent-methods/ data-toggle=tooltip data-placement=top title=几种梯度下降(GD)法的比较（转载）>Next
Post &rarr;</a></li></ul><div class=post-comment><span id=/2017/07/deep-learning-tutorial-pca-and-whitening/ class=leancloud_visitors data-flag-title="Deep Learning Tutorial - PCA and Whitening"><span class=post-meta-item-text>访问量</span>
<span class=leancloud-visitors-count></span><p></p></span><div id=vcomments></div><script src=//cdn1.lncld.net/static/js/3.0.4/av-min.js></script><script src=//unpkg.com/valine/dist/Valine.min.js></script><script type=text/javascript>new Valine({el:'#vcomments',appId:'2dzJwxGKq4hbtg5R5NM8NTzJ-gzGzoHsz',appKey:'RaYu8uGTiuiIjLISQppPVYWw',notify:true,verify:false,avatar:'retro',placeholder:'说点什么吧...',visitor:true});</script></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/bfs title=bfs>bfs</a>
<a href=/tags/binary-search title=binary-search>binary-search</a>
<a href=/tags/brute-force title=brute-force>brute-force</a>
<a href=/tags/dfs title=dfs>dfs</a>
<a href=/tags/dp title=dp>dp</a>
<a href=/tags/greedy title=greedy>greedy</a>
<a href=/tags/kmp title=kmp>kmp</a>
<a href=/tags/leetcode title=leetcode>leetcode</a>
<a href=/tags/math title=math>math</a>
<a href=/tags/number-theory title=number-theory>number-theory</a>
<a href=/tags/rmq title=rmq>rmq</a>
<a href=/tags/stl title=stl>stl</a>
<a href=/tags/%E5%89%8D%E7%BC%80%E5%92%8C title=前缀和>前缀和</a>
<a href=/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA title=博弈论>博弈论</a>
<a href=/tags/%E5%9B%BE%E8%AE%BA title=图论>图论</a>
<a href=/tags/%E5%BF%AB%E9%80%9F%E5%B9%82 title=快速幂>快速幂</a>
<a href=/tags/%E6%95%B0%E4%BD%8Ddp title=数位dp>数位dp</a>
<a href=/tags/%E6%9E%84%E9%80%A0 title=构造>构造</a>
<a href=/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84 title=树状数组>树状数组</a>
<a href=/tags/%E6%A8%A1%E6%8B%9F title=模拟>模拟</a>
<a href=/tags/%E6%AF%8D%E5%87%BD%E6%95%B0 title=母函数>母函数</a>
<a href=/tags/%E7%9F%A9%E9%98%B5 title=矩阵>矩阵</a>
<a href=/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91 title=线段树>线段树</a>
<a href=/tags/%E8%AE%A1%E7%AE%97%E5%87%A0%E4%BD%95 title=计算几何>计算几何</a></div></section><section><hr><h5>FRIENDS</h5><ul class=list-inline><li><a target=_blank href=https://111qqz.com>111qqz的wordpress博客</a></li></ul></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href rel=alternate type=application/rss+xml title=111qqz的小窝><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-rss fa-stack-1x fa-inverse"></i></span></a></li><li><a href=mailto:hust.111qqz@gmail.com><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/img/wechat.jpg><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-wechat fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/111qqz/><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-github fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; 111qqz的小窝 2021<br><a href=https://beian.miit.gov.cn/>粤ICP备18103363号</a><br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> by <a href=https://zhaohuabing.com>Huabing</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function async(u,c){var d=document,t='script',o=d.createElement(t),s=d.getElementsByTagName(t)[0];o.src=u;if(c){o.addEventListener('load',function(e){c(null,e);},false);}
s.parentNode.insertBefore(o,s);}</script><script>if($('#tag_cloud').length!==0){async("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:'#bbbbee',end:'#0085a1'},};$('#tag_cloud a').tagcloud();})}</script><script>async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.js",function(){var $nav=document.querySelector("nav");if($nav)FastClick.attach($nav);})</script></body></html>