<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>计算机视觉 on 111qqz的小窝</title><link>https://111qqz.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link><description>Recent content in 计算机视觉 on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sat, 02 May 2020 18:50:06 +0800</lastBuildDate><atom:link href="https://111qqz.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/index.xml" rel="self" type="application/rss+xml"/><item><title>Focal Loss for Dense Object Detection(RetinaNet) 学习笔记</title><link>https://111qqz.com/2020/05/retinanet-notes/</link><pubDate>Sat, 02 May 2020 18:50:06 +0800</pubDate><guid>https://111qqz.com/2020/05/retinanet-notes/</guid><description>
先写个简略版的笔记..看之后的情况要不要读得更精细一点..
背景 two stage的检测比one stage的检测效果好,原因是啥?
作者认为是正负样本不平衡导致的. two stage的方法在proposal 的时候干掉了大部分负样本,所以效果好.
因为作者提出了一种新的loss,称为Focal Loss 是对交叉熵loss的改进,作用是提高没有正确分类的样本的权重,降低正确分类的样本的权重.
然后设计了个retinaNet 来验证效果. 主要是用了Focal Loss 作为损失函数,以及backbone比起之前的one stage的检测用上了FPN.
Focal Loss 一图胜千言
Focal loss是在交叉熵loss的基础上增加了一个指数衰减. 对正确分类的样本影响很小,对错误分类的样本影响很大.
从图上我们可以看出,CE对正确分类的样本仍然有不小的loss,这样的样本数很多的话,就会被训练的时候模型被带歪... 因此需要减少这些正确分类的样本对loss的影响.
RetinaNet 单阶段检测,主要在于使用Focal Loss训练,以及backbone用上了FPN</description></item><item><title>记一次faster-rcnn debug记录</title><link>https://111qqz.com/2019/12/debug-faster-rcnn-once-again/</link><pubDate>Fri, 13 Dec 2019 16:32:14 +0800</pubDate><guid>https://111qqz.com/2019/12/debug-faster-rcnn-once-again/</guid><description>
问题描述 一年debug 三次faster rcnn,每次都有新感觉（不
接到一个bug report,现象为某人脸模型，转换成trt模型，当batch size为1时结果完全正确，但是batch size大于1时结果不正确。 具体的现象是，如果跑多张不同的图，只有第一张图有结果，后面的图都没有结果。 如果跑的图中有相同的，那么和第一张相同的图都会有结果，其余的图没有结果。
1layer {2 name: &amp;#34;POD_proposal&amp;#34;3 type: &amp;#34;RPRoIFused&amp;#34;4 bottom: &amp;#34;Reshape_105&amp;#34;5 bottom: &amp;#34;Conv_100&amp;#34;6 bottom: &amp;#34;Add_95&amp;#34;7 bottom: &amp;#34;im_info&amp;#34;8 top: &amp;#34;rois&amp;#34;9 top: &amp;#34;tmp_pooling&amp;#34;10 rpn_proposal_param{11 feat_stride: 1612 anchor_ratios: 113 anchor_scales: 114 anchor_scales: 215 anchor_scales: 416 anchor_scales: 817 anchor_scales: 1618 anchor_scales: 3219 test_desc_param {20 rpn_pre_nms_top_n: 200021 rpn_post_nms_top_n: 5022 rpn_min_size: 1623 rpn_nms_thresh: 0.724 }25 }2627 roi_pooling_param{28 pooled_h: 729 pooled_w: 730 spatial_scale: 0.06253132 }33}3435特别地，proposal layer中 rpn_post_nms_top_n的参数值如果使用默认的300,那么结果都是对的。把这个值改小（只要小于300)，结果就像上面所述。
debug 经过 首先根据rpn_post_nms_top_n的值一修改，结果就是错的来看，怀疑是哪里参数写死了。 然而很快就排除了这个问题。因为faster rcnn的模型已经在另一个产品中经过长期验证了。 而且proposal layer是tensorrt自己实现的，有bug早就有人发现了。</description></item><item><title>FPN:Feature Pyramid Networks 学习笔记</title><link>https://111qqz.com/2019/12/feature-pyramid-networks/</link><pubDate>Sun, 08 Dec 2019 17:30:50 +0800</pubDate><guid>https://111qqz.com/2019/12/feature-pyramid-networks/</guid><description>
检测不同尺度的物体一直是计算机视觉领域中比较有挑战性的事情．我们先从之前的工作出发，并对比FPN比起之前的工作有哪些改进．
之前的工作 Featurized image pyramid 思路是对于同一张图，生成不同的scale，然后每个scale的image单独去做检测． 这个方法是手工设计feautre时代的常用办法． 这个办法是比较显然的，也的确可以解决检测不同尺度物体的问题． 缺点非常明显...inference的速度几乎和scale的个数线性相关． 以及由于显存的开销，没办法做end-to-end 的training.
Single feature map 再之后，手工设计的feature逐渐被由CNN生成的feature取代了． 这种办法更加鲁棒，对image的一些变化不敏感． 但是如果只有一个scale 的图片去过这个feature map,只有最终的feature map去做predict..准确率不太行．．因此还是要与Featurized image pyramid一起．只是优化了得到feature 的部分．
Pyramidal feature hierarchy 就..CNN本身就有显然的层次结构啊．．． 为什么不直接拿来用，而是提前scale image呢．．．
也就是选若干个feature map直接去做predict.. 这个办法美滋滋，既有多个feature map保证了一定的准确率，同时也没有增加很多inference的cost.
SSD应该是率先使用这种方法的． 但是这种办法仍然有不足之处，就是低层的高分辨率的feature map的semantics 太弱了．． 这就导致说对小物体的检测效果不太理想．．．
那么该怎么办呢．．．这时候就该介绍FPN啦
Feature Pyramid Networks 后面再更．
FPN 用于 faster rcnn FPN同时作用于RPN阶段和fast-rcnn detector
以下的代码实现出自　fpn.pytorch
FPN 作用于RPN 感觉直接上代码比较容易理解
先看　整个网络的forward函数
1 2 3 def forward(self, im_data, im_info, gt_boxes, num_boxes): 4 batch_size = im_data.</description></item><item><title>SSD: Single Shot MultiBox Detector 学习笔记</title><link>https://111qqz.com/2019/12/single-short-detector/</link><pubDate>Sun, 08 Dec 2019 15:00:15 +0800</pubDate><guid>https://111qqz.com/2019/12/single-short-detector/</guid><description>
概述 SSD是一种单阶段目标检测算法．所谓单阶段，是指只使用了一个deep neural network,而不是像faster-rcnn这种两阶段网络． 为什么有了faster-rcnn还要使用SSD? 最主要是慢... 两阶段网络虽然准确率高，但是在嵌入式等算力不足的设备上做inference速度非常感人，很难达到real time的要求． （实际业务上也是这样，公有云上的检测模型几乎都是faster-rcnn,而到了一些盒子之类的硬件设备，检测模型就全是SSD等single stage 模型了)
之前一直没有写SSD是因为相比faster rcnn的细节，SSD的问题似乎并不是很多．直到最近转模型的时候被FASF模型的一个细节卡了蛮久，因此决定还是记录一下．
基本概念 这部分描述SSD中涉及到的一些想法．
prior box prior box的概念其实与faster-rcnn中anchor的概念是一样的，没有本质区别． 与faster-rcnn中的anchor不同的是，SSD会在多个feature map中的每个cell 都生成若干个prior_box.
对于一个特定的feature map,尺寸为m*n,假设有k个prior box,c种类别． 那么feature map的每个location会生成 k*(c+4) 个结果，其中c代表每一类的confidence. ４代表相对prior_box中心点的offset. 整个feature_map会生成　 kmn(c+4) 个结果．
prior_box的参数选择,以及一些训练有关的细节可以参考原论文,这里不再赘述. 这里主要想强调一下和priox box有关的inference 细节. 主要是decode box的部分.
由于模型输出的bbox其实是相对每个prior_box的offset,不是真正的bbox,因此需要由网络输出的box_pred和prior box得到真正的bbox 坐标.这部分通常称为decode box,其实已经算是后处理部分了.
pytorch中decode box的代码如下:
1 variance1, variance2 = variance 2 cx = box_prior[:, :, 3 0] + box_pred[:, :, 0] * variance1 * box_prior[:, :, 2] 4 cy = box_prior[:, :, 5 1] + box_pred[:, :, 1] * variance1 * box_prior[:, :, 3] 6 w = box_prior[:, :, 2] + torch.</description></item><item><title>Kubernetes(k8s)在深度学习模型转换方面的探索</title><link>https://111qqz.com/2019/11/K8s-for-Model-Conversion/</link><pubDate>Fri, 22 Nov 2019 11:14:19 +0800</pubDate><guid>https://111qqz.com/2019/11/K8s-for-Model-Conversion/</guid><description>
年中的时候接了离职的同事模型转换的锅，在不断地更新迭代的过程中，发现了一些痛点。 发现k8s能够解决一部分痛点，因此来分享一下在这方面的探索。
什么是模型转换 简单来说，深度学习模型的流程分为training和inference两部分。训练时用的一般是pytorch等框架，这些框架训练出的model是没办法直接部署在各个硬件平台上做inference的。因此需要将使用训练框架得到的模型，转换为能够部署到各个硬件平台上的模型。这个过程就是模型转换。
模型转换的一般流程为，先将pytorch等训练框架训练得到的模型转换为caffe model（是的，caffe才是业界中间表示的事实标准，而不是号称支持所有框架中间表示的onnx)，再将caffe model 转换到各种硬件上（包括但不限于nvidia系列显卡，华为的海思系列设备等）
（事实上这个转换流程是有硬伤存在的，这个硬伤在于pytorch的模型权重和模型结构是分离的。调研过业界一些其他模型转换的解决方案，包括百度的X2Paddle,其做法是不把pytorch模型作为模型转换的起点，而是从caffe model/onnx model 开始转换。还调研过微软的MMdnn,里面似乎也没有提到这个问题。不知道pytorch 1.0版本新增的torchscript是不是一条出路...)
模型转换的痛点 最初模型转换的痛点是依赖比较多，从pytorch,onnx,caffe再到tensorrt,cuda等．　尤其编译caffe,又是出了名的坑多．　解决办法是用了docker,将所有环境封装好．这样其他人可以直接拉镜像下来进行模型转换．
然而，在用了docker之后，还是有其他痛点，主要是以下三个:
权限申请的繁琐．
caffe模型转换到不同硬件上（主要是nvidia显卡），需要在对应的机器上进行转换．通常是客户确定使用某型号的显卡，然后我们采购对应显卡的机器．接下来需要一系列权限申请，包括服务器的权限，docker命令的权限，以及docker镜像仓库的权限．　除此之外，用户还需要将模型文件从其他位置放置到新服务器上，这件事也非常麻烦．
docker 镜像的版本控制.
docker的image虽然可以打tag，但是这个tag几乎对版本控制没有帮助．由于模型转换工具更新频繁（包括但是不限于，添加新的op/layer,caffe对新显卡的编译支持，caffe2tensorrt工具的更新），这么弱的版本管理就是灾难．如果我们对于每次升级docker image,都使用一个新的tag来标记的话，docker似乎没有一个机制来通知这个新的tag的存在．用户还是可以在这个旧的镜像上用到天荒地老．如果复用之前的tag的话,同样，用户还是可以不更新．
对docker commit 的滥用．
本来docker image的大小只有7G+,但是由于用户非常钟爱docker cp命令以及docker commit 命令．曾经有用户把整个数据集都docker cp进去之后docker commit ..　然后现在镜像大小已经有35G了．．
上述几个痛点，恰好k8s可以比较好的解决．
k8s用于模型转换. 官方的说法是,k8s是一个生产级别的容器编排系统，用于容器化应用的自动部署、扩缩和管理
然而似乎我对k8s的使用是非常非主流的(其实对docker的使用也很非主流...)
不过我觉得没有规定一项技术只能用来做什么,只要解决的问题多于引入的问题,就可以尝试.
k8s的引入使得用户不再能直接接触到docker image. 这直接解决了第一和第三两个痛点. 以及可以使用 imagePullPolicy 来进行强制更新,也基本解决了版本控制的问题. 这样就不用天天push 其他用户&amp;quot; 你先docker pull一下&amp;quot;...
此外,k8s的label机制也很有用. 我们可以通过给node打上不同显卡型号的label.然后维护一个显卡的列表.这样用户不再需要知道哪台服务器上有哪个显卡,只需要添写显卡型号,k8s就可以分配一台对应的机器供用户进行模型转换.
不过实际上k8s用户模型转换也有一些缺点:
k8s对显卡的调度是对于pod(pod就是一组container)独占的.也就是说,在一台服务器上,通过k8s只能开启与显卡个数一样多的pod.目前来看不会有什么大的影响,不过这对于对性能要求不敏感的应用,确实算个缺点.
pod名称的全局唯一性.由于开启任务需要显式提供一个名称,如果多个用户使用了相同的名称的话,结果显然不是我们期望的. 目前的解决方案是制定一个命名规范,要求用户遵守.但是这种规范并没有任何检查的机制,因此只能算一个tricky的办法,算不上真正的解决方案.</description></item><item><title>faster rcnn 模型 tensorrt4与tensorrt5 结果不一致 踩坑记录</title><link>https://111qqz.com/2019/11/debug-Frcnn-Model-When-Upgrading-From-Tensorrt4-to-Tensorrt5/</link><pubDate>Thu, 07 Nov 2019 23:40:39 +0800</pubDate><guid>https://111qqz.com/2019/11/debug-Frcnn-Model-When-Upgrading-From-Tensorrt4-to-Tensorrt5/</guid><description>
最近有同事report给我们,用同一个模型转换工具,转同一个faster rcnn 模型, 同样的sdk代码,在有些显卡上结果正常,但是再比较新的显卡上(比如Titan V)上 结果完全不正确.
听说之后我的内心其实是 **喵喵喵喵喵?**的
先在模型转换工具中infer了一下,发现...结果竟然真的不一样!
于是又开始了debug faster rcnn 的旅程(奇怪..我为什么要说又)
一份典型的faster rcnn 的
prototxt
按照经验,我们先对照了ROIS,来判断RPN 是否存在问题
惊讶地发现,竟然是没有问题的...
那看一下模型的输出 cls_score 和 bbox_pred好了 发现cls_score 完全对不上,bbox_pred 倒是没问题... 这和之前遇到的情况不太一样啊... 从proposal layer 到 最后... 全都是些很常见的layer... 哪里会有问题呢...
最终发现有问题的layer 是softmax!
cls_score 的维度应该为 N*K*C*H*W
softmax应该按照C这一维度来做,因此softmax_param中axis应该为2.
查阅&amp;quot;tensorrt support matrix&amp;quot; ，发现 tensorrt5中，softmax会按照用户指定的维度进行。
而对于tensorrt4.0, softmax layer 与设定的softmax_param无关，只会在channel 的维度上来做softmax.
所以，比较好的解决办法是，干脆不写softmax_param这个参数 对于trt4，会直接按照channel 维度来做 对于trt5，会在第N-3的axis上进行。对于SSD的 N*C*H*W或者 faster rcnn 的N*K*C*H*W, N-3都是channel所在的维度。
问题解决！</description></item><item><title>Anchor Box Algorithm</title><link>https://111qqz.com/2019/07/Anchor-Box-Algorithm/</link><pubDate>Mon, 01 Jul 2019 19:53:24 +0800</pubDate><guid>https://111qqz.com/2019/07/Anchor-Box-Algorithm/</guid><description>
动机 将一张图分成多个grid cell进行检测之后,每个cell只能检测到一个object. 如果这个grid cell中不止有一个物体要怎么办呢? 因此提出了anchor box algorithm来解决这个问题.
什么是anchor anchor其实就是一组预设的参考框,每个框有不同的长宽比和大小. 提供参考框可以将问题转换为&amp;quot;这个固定参考框中有没有认识的目标，目标框偏离参考框多远&amp;quot;. 这样如果一个grid cell中有多个物体,那么就可以形状最姐姐的anchor box来负责检测该物体. anchor的其他用途 实际上当grid cell很多的时候,一个grid cell中有多个object的情况是很少的.但是anchor box仍然是十分重要的.因为我们可以通过预设一些特殊的anchor box,来帮助检测到一些极端形状的物体(比如很长的物体或者很宽的物体)
输出结果 有了anchor之后,每个grid cell的输出可能不再唯一. 于是从之前的每个grid cell对应一个输出结果,变成了每个二元组(grid_cell,anchor box)对应一个结果. 结果的格式呢,一图胜千言: 局限性 由于anchor box是要预设的,那么如果我只预设了两个anchor box,但是一个grid cell中有三个object,怎么办呢? 很遗憾,是没有办法的. 另外一个case是,如果两个object都与其中一个anchor的形状最为接近,也是没有办法检测出两个物体的.
anchor box的生成 最初anchor box是通过人手工设定的,之后的YOLO的paper中提出了使用k-means的算法来生成anchor box.</description></item><item><title>目标检测领域的滑动窗口算法</title><link>https://111qqz.com/2019/06/sliding-windows/</link><pubDate>Sun, 30 Jun 2019 16:55:40 +0800</pubDate><guid>https://111qqz.com/2019/06/sliding-windows/</guid><description>
对象检测（Object Detection）的目的是”识别对象并给出其在图中的确切位置”，其内容可解构为三部分：
识别某个对象（Classification）； 给出对象在图中的位置（Localization）； 识别图中所有的目标及其位置（Detection）。 本文将介绍滑动窗口这一方法.
滑动窗口 滑动窗口是这些方法中最暴力的一个.简单来说,就是暴力枚举侯选框的尺寸和位置,每次crop得到一张小图,将每个小图送进后面的分类器进行分类. 早年后面通常会接一个计算量比较小的分类器,比如SVM,随着算力的提升,现在常常后面会接CNN. 值得一提的是,原始的滑动窗口方法是将每个小图,分别放入后面的分类器.但是实际上,小图和小图之间,是有overlap的,也就是说做了很多重复的计算. 因此一个显然的改进是使用CNN来实现滑动窗口算法, 这种方法的优点是比较无脑,实现和理解起来都很简单.缺点是计算量还是比较大.
参考链接 Sliding Windows for Object Detection with Python and OpenCV
深度学习基础 - 对象检测（滑窗、CNN、YOLO）</description></item><item><title>caffe2 添加自定义operater</title><link>https://111qqz.com/2018/04/add-custom-operation-in-caffe2/</link><pubDate>Fri, 13 Apr 2018 03:08:19 +0000</pubDate><guid>https://111qqz.com/2018/04/add-custom-operation-in-caffe2/</guid><description>
记录一些一个没有之前没有接触过caffe/caffe2的人为了添加自定义的op 到caffe2需要做的工作.
首先参考caffe2 tutorial,随便跑个op来试试,不妨以比较简单的 Accumulate_op 为例子.
这个op的作用就是计算Y=X+gamma*Y, 其中X为输入,Y为输出,gamma是参数.
跑起来这个运算所需要的代码如下:
from caffe2.python import workspace, model_helper import numpy as np # Create the input data data = np.arange(6).reshape(2,3).astype(np.float32) print (&amp;quot;data=&amp;quot;,data) # Create labels for the data as integers [0, 9]. workspace.FeedBlob(&amp;quot;data&amp;quot;, data) # Create model using a model helper m = model_helper.ModelHelper(name=&amp;quot;my first net&amp;quot;) output = m.net.Accumulate([&amp;quot;data&amp;quot;], &amp;quot;output&amp;quot;) print(m.net.Proto()) workspace.RunNetOnce(m.param_init_net) workspace.CreateNet(m.net) workspace.RunNet(m.name,2) # run 2 times print(&amp;quot;output=&amp;quot;,workspace.FetchBlob('output')) c之后我们仿照caffe2/operators/accumultate_op.h和affe2/operators/accumultate_op.cc,仿写一个我们自己的运算atest_op.h和atest_op.cc
实现的功能为Y=5X+gammaY
 #include &amp;quot;caffe2/operators/atest_op.</description></item><item><title>非极大值抑制（Non-Maximum Suppression，NMS）</title><link>https://111qqz.com/2018/03/non-maximum-suppression/</link><pubDate>Fri, 16 Mar 2018 02:56:14 +0000</pubDate><guid>https://111qqz.com/2018/03/non-maximum-suppression/</guid><description>
NMS是为了在诸多CV任务如边缘检测，目标检测等，找到局部最大值
其主要思想是先设定一个阈值，然后计算检测框的IOU(所谓IOU，也就是intersection-over-union，指的是相交面积除以相并面积，是来衡量overlap程度的指数）。如果IOU大于阈值，说明overlap过大，我们要通过某种算法来将其剔除。
比如下图，在经典的人脸识别任务中，出现了多个检测框，每个检测框有一个置信度confidence，我们通过某个算法，保留一个最好的。
顺便说一下算法的实现步骤把，其实不太重要。就是贪心。
其基本操作流程如下： * 首先，计算每一个 bounding box 的面积： * (x1, y1) ⇒ 左上点的坐标，(x2, y2) ⇒ 右下点的坐标； * (x2-x1+1)x(y2-y1+1) * 根据 scores 进行排序（一般从小到大），将 score 最大的bounding box置于队列，接下来计算其余 bounding box 与当前 score 最大的 bounding box 的 IoU，抑制（忽略也即去除）IoU大于设定阈值的 bounding box； * 重复以上过程，直至候选 bounding boxes 为空； 最后上一段python代码吧...也很简单，直接转载了别人的...
def nms(dets, thresh): x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) # 每个boundingbox的面积 order = scores.</description></item><item><title>reid 相关任务记录</title><link>https://111qqz.com/2018/02/reid-task-notes/</link><pubDate>Sat, 24 Feb 2018 04:34:02 +0000</pubDate><guid>https://111qqz.com/2018/02/reid-task-notes/</guid><description>
被师兄（同事？）普及了一番实验规范orz...
我还是太年轻了
所谓的一个fc的版本是右边的．一个放着不动，另一个在sequence_len（１０）的维度上做ave,然后再expand成原来的维度．如下图．
任务命名规则：
如D1V2_a_1,D1表示使用第一个数据集，V2表示是第二个大版本，ａ表示在V２大版本上的微调，最后的数字表示这是第几次运行该任务（跑三次以减少波动的影响）
logdir的地址为:/mnt/lustre/renkuanze/Data_t1/reid/log/｛$jobname｝
* D1:使用ilivids 数据集 * D1V1表示最初始的　baseline model * D1V2表示改为使用一个fc * D1V2_a是一个在一个FC上，不添加光流的修改版本 * D1V2_b是在一个FC上的baseline版本（也就是有光流） * D1V2_c是在一个FC上，有光流，batchsize从３２改为６４，gpu数目从4改为8的版本 * D1V3表示将softmax改为sigmod * D1V3_b表示将softmax改为sigmod的baseline版本 * D2:使用prid2011数据集 * D2V1表示初始的baseline model * D2V2表示改为使用一个fc * D2V2_b是在一个FC上的baseline版本 *</description></item><item><title>分类评价指标之Cumulative Match Characteristi (CMC)曲线</title><link>https://111qqz.com/2018/02/cumulative-Match-characteristi/</link><pubDate>Fri, 23 Feb 2018 08:20:55 +0000</pubDate><guid>https://111qqz.com/2018/02/cumulative-Match-characteristi/</guid><description>
CMC曲线全称是Cumulative Match Characteristic (CMC) curve，也就是累积匹配曲线，同ROC曲线Receiver Operating Characteristic (ROC) curve一样，是模式识别系统，如人脸，指纹，虹膜等的重要评价指标，尤其是在生物特征识别系统中，一般同ROC曲线（ 多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法）一起给出，能够综合评价出算法的好坏。
转一篇通俗易懂的解释：
Shortly speaking, imagine that you have 5 classes. For simplicity, imagine you have one test per class. Each test produces a score when compared to each class. Let's start from test1 which belongs to class1:
As your similarity measure, here, is Euclidian distance, the more distance a test has compared to a class, the less similarity is obtained.</description></item><item><title>光流法初探</title><link>https://111qqz.com/2018/02/Optical-flow-notes/</link><pubDate>Thu, 22 Feb 2018 09:03:48 +0000</pubDate><guid>https://111qqz.com/2018/02/Optical-flow-notes/</guid><description>
算是CV领域的传统算法了
只写两句话就够了。
**它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。** 研究光流场的目的就是为了从图片序列中近似得到不能直接得到的运动场。运动场，其实就是物体在三维真实世界中的运动；光流场，是运动场在二维图像平面上（人的眼睛或者摄像头）的投影。
参考资料：
光流Optical Flow介绍与OpenCV实现</description></item><item><title>end-to-end 神经网络</title><link>https://111qqz.com/2018/02/end-to-end-neural-network/</link><pubDate>Thu, 22 Feb 2018 02:53:01 +0000</pubDate><guid>https://111qqz.com/2018/02/end-to-end-neural-network/</guid><description>
所谓end-to-end 神经网络，更多是一种思想。
这种思想的核心是，比如对于图像处理，输入原始图像数据，输出的是直接有用的结果（有用取决于具体的任务，比如自动驾驶)
也就是尽可能少得减少人为干预，使训练是end (原始数据) to end (对应用问题直接有用的结果)
端到端指的是输入是原始数据，输出是最后的结果，原来输入端不是直接的原始数据，而是在原始数据中提取的特征，这一点在图像问题上尤为突出，因为图像像素数太多，数据维度高，会产生维度灾难，所以原来一个思路是手工提取图像的一些关键特征，这实际就是就一个降维的过程。 那么问题来了，特征怎么提？ 特征提取的好坏异常关键，甚至比学习算法还重要，举个例子，对一系列人的数据分类，分类结果是性别，如果你提取的特征是头发的颜色，无论分类算法如何，分类效果都不会好，如果你提取的特征是头发的长短，这个特征就会好很多，但是还是会有错误，如果你提取了一个超强特征，比如染色体的数据，那你的分类基本就不会错了。 这就意味着，特征需要足够的经验去设计，这在数据量越来越大的情况下也越来越困难。 于是就出现了端到端网络，特征可以自己去学习，所以特征提取这一步也就融入到算法当中，不需要人来干预了。
简单得说，符合end-to-end 的神经网络，特征应该是网络自己学习，而不是人为提取。
参考资料：
什么是 end-to-end 神经网络？</description></item><item><title>Pose-driven Deep Convolutional Model for Person Re-identification 阅读笔记</title><link>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification/</link><pubDate>Thu, 22 Feb 2018 02:25:00 +0000</pubDate><guid>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification/</guid><description>
1709.08325
Reid问题指的是判断一个probe person 是否在被不同的camera捕获的gallery person 中出现。
通常是如下情景：给出一个特定camera下某个特定人的probe image 或者 video sequence，从其他camera处询问这个人的图像，地点，时间戳。
ReID问题至今没有很好得解决，主要原因是，不同camera下，人的姿势（pose),观察的视角(viewpoint) 变化太大了。
传统方法主要在两个大方向上努力:
1. **用一些在图像上抽取的不变量来作为人的特征feture** 2. **去学习一个不同的距离度量方式，以至于同一个人在不同camera之间的距离尽可能小。** 但是由于在实际中，行人的pose和 摄像机的viewpoint不可控，因此与之相关的feture可能不够健壮。
学习新的不同的距离度量方式需要每对camera分别计算距离，然而这是O(n^2)的时间复杂度，凉凉。
近些年Deep learning发展迅猛，并且在很多CV任务上表现良好。所以自然有人想把Deep learning 方法应用到Reid任务上。
目前Deep learning的做法一般分为两部分：
* 使用softmax loss 结合person ID labels得到一个global representation * 首先用预定义好的body 刚体模型去得到local representation,然后将global 和local representation 融合。 目前用deep learning的方法效果已经不错了，比传统方法要好。但是目前的deep learning方法没有考虑到人的姿势(pose)的变化。
虽然目前也有些deep learning的办法在处理Reid问题时使用pose estimation algorithms 来预测行人的pose,
但是这种办法是手动完成而不是一个end-to-end（什么是end-to-end 神经网络） 的过程
所以考虑pose的潜力还没有被完全发掘。
这篇paper主要做了以下工作：
* 提出了一种新的深层结构，将身体部分转化成相应的特征表示，以此来克服pose变化带来的问题 * 提出了一个用来自动学习各部分权值的sub-network 这两部分工作都是end-to-end的</description></item><item><title>Deep Mutual Learning（相互学习） 阅读笔记</title><link>https://111qqz.com/2018/02/deep-mutual-learning-notes/</link><pubDate>Sun, 18 Feb 2018 10:46:35 +0000</pubDate><guid>https://111qqz.com/2018/02/deep-mutual-learning-notes/</guid><description>
原始论文
DNN在很多问题上效果很不错，但是由于深度和宽度过大，导致需要的执行时间和内存过大。我们需要讨论一些能快速执行并且对内存的需要不大的模型。
已经有很多方法来做这件事，比较重要的是Model distillation（模型蒸馏）
基于蒸馏的模型压缩的有效性是基于一个发现：小的网络有和大的网络一样的表达能力，只不过是更难以训练找到合适的参数。
也就是说，难点在于优化(以找到合适的参数）而不在于网络的尺寸。
蒸馏的模型的做法是把一个有效的(deep or/and wide) network 作为teacher network,让一个size 较小的 student network 模仿teacher network.
这样做的好处是，size小的多的student network训练如何模仿teacher network通常要比直接训练得到目标函数容易，而效果上与teacher network相当，甚至超过teacher network.
在这篇paper中，作者提出了一个和蒸馏模型相关但是不同的模型: Deep Mutual Learning (相互学习，以下缩写为DML)
蒸馏模型开始于一个强大的教师网络，并通过教师网络**单向(one way)**教授学生网络知识。
相反，DML中,开始于一群没有经受过训练的学生网络。
具体来说，每个学生训练有两种损失:
* **常规的监督学习损失和对齐的模仿损失** * **每个学生的在班级中落后于其他学生的概率。** 结果表明，DML的方式：比每个学生网络单独学习要好，也比传统的传统的蒸馏模型要好。
此外，我们发现，对几个tearcher network 使用 DML，要往往有更好的结果。
另外一些发现：
* 效果随着班级中学生网络的数量的增加而增加 * 它适用于各种网络体系结构和异构群组 关于DML为什么有效的直觉讨论：
* 常规的监督学习损失保证每个学生单独学习时，整体上结果是越来越好的。 * 由于初始条件不同，对下一个最可能的class的预测概率不同，这是额外信息的来源。</description></item><item><title>Similarity learning 和Metric learning</title><link>https://111qqz.com/2018/02/similarity-learning-metric-learning/</link><pubDate>Sun, 18 Feb 2018 08:14:10 +0000</pubDate><guid>https://111qqz.com/2018/02/similarity-learning-metric-learning/</guid><description>
Similarity_learning 相似性学习（Similarity learning ）有监督机器学习，它与回归和分类密切相关，但目标是从实例中学习一个相似函数，以衡量两个对象的相似程度或相关程度。
Similarity learning通常有四种setups:
* regression similarity learning 在这种方式中，给出的数据是 ![(x_{i}^{1},x_{i}^{2})](https://wikimedia.org/api/rest_v1/media/math/render/svg/cfa249357a1b4a7baf332041d67e480d6bb1f8fb)  和他们的相似度 . 目标是学习到一个函数 ，对于 给出yi的近似值。 * Classification similarity learning 给出数据 和他们是否相似 。目标是训练出一个分类器，能够完成对一组 是否相似的二分类判断。 * Ranking similarity learning 给出有序三元组 ，其中 is known to be more similar to than to 。目标是训练出一个函数，使得对于新的三元组 ，满足 。容易看出，这种方式采取了比回归更弱的监督形式，因为不需要提供精确的相似性度量，只需要提供相似性的相对顺序。因此，这种ranking-based的相似性学习更容易应用于实际的大规模应用 * Locality sensitive hashing (LSH) 局部敏感哈希和普通哈希的不同就是，相似的项有更大的概率被放到同一个桶中。
顺便一提，这里有一个叫triplet loss 的概念，
如上图所示，triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本和不同类的样本,这两个样本对应的称为Positive (记为x_p)和Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。
我们发现，triplet loss 其实就是在ranking similarity learning 问题中，学习similarity function时的loss
Metric learning 相似性学习与距离度量学习密切相关。度量学习的目标是在对象上学习一个距离函数。度量或距离函数必须遵循四个公理:非负性、不可分辨的恒等式、对称性和次可加性/三角形不等式。在实际应用中，度量学习算法忽略了不可分辨物体的身份条件，学习了一个伪度量。</description></item><item><title>persion reid 论文列表</title><link>https://111qqz.com/2018/02/persion-reid-paper-list/</link><pubDate>Sat, 17 Feb 2018 07:17:49 +0000</pubDate><guid>https://111qqz.com/2018/02/persion-reid-paper-list/</guid><description>
Key:
(1). Pose-driven, body part alignment, combine whole feature and body part feature, focus on alignment of part model,
(2). Combine image label and human attributes classes, do classification with attributes and identity learning
(3). Based on triplet loss, improve metric learning for an end to end learning
(4). Post-process, re-ranking
AlignedReID: Surpassing Human-Level Performance in Person Re-Identification
Hydraplus-net: Attentive deep features for pedestrian analysis.
Darkrank: Accelerating deep metric learning via cross sample similarities transfer.</description></item><item><title>多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法</title><link>https://111qqz.com/2018/02/mean-average-precision-for-multi-label-classification-task/</link><pubDate>Fri, 09 Feb 2018 03:53:09 +0000</pubDate><guid>https://111qqz.com/2018/02/mean-average-precision-for-multi-label-classification-task/</guid><description>
参考资料:
多标签图像分类任务的评价方法-mAP
wiki_Sensitivity and specificity
False Positives和False Negative等含义
mean average precision（MAP）在计算机视觉中是如何计算和应用的？
首先需要了解True(False) Positives (Negatives)的含义
True Positive （真正, TP）被模型预测为正的正样本；可以称作判断为真的正确率
True Negative（真负 , TN）被模型预测为负的负样本 ；可以称作判断为假的正确率
False Positive （假正, FP）被模型预测为正的负样本；可以称作误报率
False Negative（假负 , FN）被模型预测为负的正样本；可以称作漏报率
在图像中，尤其是分类问题中应用AP，是一种评价ranking方式好不好的指标：
举例来说，我有一个两类分类问题，分别5个样本，如果这个分类器性能达到完美的话，ranking结果应该是+1，+1，+1，+1，+1，-1，-1，-1，-1，-1.
但是分类器预测的label，和实际的score肯定不会这么完美。按照从大到小来打分，我们可以计算两个指标：precision和recall。比如分类器认为打分由高到低选择了前四个，实际上这里面只有两个是正样本。此时的recall就是2（你能包住的正样本数）/5（总共的正样本数）=0.4，precision是2（你选对了的）/4（总共选的）=0.5.
图像分类中，这个打分score可以由SVM得到：s=w^Tx+b就是每一个样本的分数。
从上面的例子可以看出，其实precision，recall都是选多少个样本k的函数，很容易想到，如果我总共有1000个样本，那么我就可以像这样计算1000对P-R，并且把他们画出来，这就是PR曲线：
这里有一个趋势，recall越高，precision越低。这是很合理的，因为假如说我把1000个全拿进来，那肯定正样本都包住了，recall=1，但是此时precision就很小了，因为我全部认为他们是正样本。recall=1时的precision的数值，等于正样本所占的比例。
所以AP，average precision，就是这个曲线下的面积，这里average，等于是对recall取平均。而mean average precision的mean，是对所有类别取平均（每一个类当做一次二分类任务）。现在的图像分类论文基本都是用mAP作为标准。
使用AP会比accuracy要合理。对于accuracy，如果有9个负样本和一个正样本，那么即使分类器什么都不做全部判定为负样本accuracy也有90%。但是对于AP，recall=1那个点precision会掉到0.1.曲线下面积就会反映出来。
precision 和 recall 的计算： &amp;amp;lt;img src=&amp;quot;https://pic2.zhimg.com/50/v2-761706f5b1fe36873ba1bb20c7d1d447_hd.jpg&amp;quot; data-rawwidth=&amp;quot;301&amp;quot; data-rawheight=&amp;quot;541&amp;quot; class=&amp;quot;content_image&amp;quot; width=&amp;quot;301&amp;quot;&amp;amp;gt; 图中上部分，左边一整个矩形中（false negative和true positive）的数表示ground truth之中为1的（即为正确的）数据，右边一整个矩形中的数表示ground truth之中为0的数据。
精度precision的计算是用 检测正确的数据个数/总的检测个数。
召回率recall的计算是用 检测正确的数据个数/ground truth之中所有正数据个数。
AP：average precision 假设我们有数据： &amp;amp;lt;img src=&amp;quot;https://pic1.</description></item><item><title>Non-local Neural Networks 阅读笔记</title><link>https://111qqz.com/2018/02/non-local-neural-networks-notes/</link><pubDate>Mon, 05 Feb 2018 02:24:34 +0000</pubDate><guid>https://111qqz.com/2018/02/non-local-neural-networks-notes/</guid><description>
先粗略读了2遍orz.可能不够严谨，先写一些high-level的理解。
对于序列或者图片数据，如果想获得一个long-range的依赖，通常的做法是循环神经网络（对于序列）或者深层的卷积神经网络（对于图片数据）
但是循环操作（当前的处理依赖于前面有限的若干个）和卷积操作都是一种局部操作。
但是这种局部操作是有一些局限的，比如不好优化，计算代价比较大等。
这篇paper提出了non-local 这个操作。
non-local操作是计算机视觉中广泛使用的一种降噪算法，即non-local mean的一般化。
non-local operation被认为是一个可以被广泛使用的操作，几乎可以和当前神经网络的其他部件结合。
含有non-local opetation的一个基本操作单元我们称之为一个 non-local block
含有non-local block 的神经网络我们可以称之为Non-local Neural Networks
non-local operation是非常有效的，及时神经网络只有很少的几层（比如5）
non-local operation和《Attention is all you need》 中提出的self-attention是相似的
全连接操作可以看做non-local operation的一个特例。
Non-local Neural Networks 原始论文</description></item><item><title>non-local means algorithm 学习笔记</title><link>https://111qqz.com/2018/01/non-local-means-algorithm-notes/</link><pubDate>Thu, 25 Jan 2018 02:53:52 +0000</pubDate><guid>https://111qqz.com/2018/01/non-local-means-algorithm-notes/</guid><description>
终于忙完学校的事情可以干正事了orz
这里会记录一些第一遍看paper的过程中遇到的一些影响理解的概念，不过大多不会深究，只算做粗浅的理解。
1、高斯金字塔：
高斯金字塔是最基本的图像塔。首先将原图像作为最底层图像G0（高斯金字塔的第0层），利用高斯核（5*5）对其进行卷积，然后对卷积后的图像进行下采样（去除偶数行和列）得到上一层图像G1，将此图像作为输入，重复卷积和下采样操作得到更上一层图像，反复迭代多次，形成一个金字塔形的图像数据结构，即高斯金字塔。
2、拉普拉斯金字塔
在高斯金字塔的运算过程中，图像经过卷积和下采样操作会丢失部分高频细节信息。为描述这些高频信息，人们定义了拉普拉斯金字塔(Laplacian Pyramid， LP)。用高斯金字塔的每一层图像减去其高一层图像上采样并高斯卷积之后的预测图像，得到一系列的差值图像即为 LP 分解图像。
将Gl内插方法得到放大图像_Gl，使_Gl的尺寸与*Gl-1的尺寸相同，即放大算子Expand。
参考资料：
Gaussian and Laplacian Pyramids
拉普拉斯金字塔融合</description></item></channel></rss>