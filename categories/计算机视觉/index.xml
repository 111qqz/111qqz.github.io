<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>计算机视觉 on 111qqz的小窝</title><link>https://111qqz.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link><description>Recent content in 计算机视觉 on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sat, 02 May 2020 18:50:06 +0800</lastBuildDate><atom:link href="https://111qqz.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/index.xml" rel="self" type="application/rss+xml"/><item><title>Focal Loss for Dense Object Detection(RetinaNet) 学习笔记</title><link>https://111qqz.com/2020/05/retinanet-notes/</link><pubDate>Sat, 02 May 2020 18:50:06 +0800</pubDate><guid>https://111qqz.com/2020/05/retinanet-notes/</guid><description>
先写个简略版的笔记..看之后的情况要不要读得更精细一点..
背景 two stage的检测比one stage的检测效果好,原因是啥?
作者认为是正负样本不平衡导致的. two stage的方法在proposal 的时候干掉了大部分负样本,所以效果好.
因为作者提出了一种新的loss,称为Focal Loss 是对交叉熵loss的改进,作用是提高没有正确分类的样本的权重,降低正确分类的样本的权重.
然后设计了个retinaNet 来验证效果. 主要是用了Focal Loss 作为损失函数,以及backbone比起之前的one stage的检测用上了FPN.
Focal Loss 一图胜千言
Focal loss是在交叉熵loss的基础上增加了一个指数衰减. 对正确分类的样本影响很小,对错误分类的样本影响很大.
从图上我们可以看出,CE对正确分类的样本仍然有不小的loss,这样的样本数很多的话,就会被训练的时候模型被带歪... 因此需要减少这些正确分类的样本对loss的影响.
RetinaNet 单阶段检测,主要在于使用Focal Loss训练,以及backbone用上了FPN</description></item><item><title>记一次faster-rcnn debug记录</title><link>https://111qqz.com/2019/12/debug-faster-rcnn-once-again/</link><pubDate>Fri, 13 Dec 2019 16:32:14 +0800</pubDate><guid>https://111qqz.com/2019/12/debug-faster-rcnn-once-again/</guid><description>
问题描述 一年debug 三次faster rcnn,每次都有新感觉（不
接到一个bug report,现象为某人脸模型，转换成trt模型，当batch size为1时结果完全正确，但是batch size大于1时结果不正确。 具体的现象是，如果跑多张不同的图，只有第一张图有结果，后面的图都没有结果。 如果跑的图中有相同的，那么和第一张相同的图都会有结果，其余的图没有结果。
1layer {2 name: &amp;#34;POD_proposal&amp;#34;3 type: &amp;#34;RPRoIFused&amp;#34;4 bottom: &amp;#34;Reshape_105&amp;#34;5 bottom: &amp;#34;Conv_100&amp;#34;6 bottom: &amp;#34;Add_95&amp;#34;7 bottom: &amp;#34;im_info&amp;#34;8 top: &amp;#34;rois&amp;#34;9 top: &amp;#34;tmp_pooling&amp;#34;10 rpn_proposal_param{11 feat_stride: 1612 anchor_ratios: 113 anchor_scales: 114 anchor_scales: 215 anchor_scales: 416 anchor_scales: 817 anchor_scales: 1618 anchor_scales: 3219 test_desc_param {20 rpn_pre_nms_top_n: 200021 rpn_post_nms_top_n: 5022 rpn_min_size: 1623 rpn_nms_thresh: 0.724 }25 }2627 roi_pooling_param{28 pooled_h: 729 pooled_w: 730 spatial_scale: 0.06253132 }33}3435特别地，proposal layer中 rpn_post_nms_top_n的参数值如果使用默认的300,那么结果都是对的。把这个值改小（只要小于300)，结果就像上面所述。
debug 经过 首先根据rpn_post_nms_top_n的值一修改，结果就是错的来看，怀疑是哪里参数写死了。 然而很快就排除了这个问题。因为faster rcnn的模型已经在另一个产品中经过长期验证了。 而且proposal layer是tensorrt自己实现的，有bug早就有人发现了。</description></item><item><title>FPN:Feature Pyramid Networks 学习笔记</title><link>https://111qqz.com/2019/12/feature-pyramid-networks/</link><pubDate>Sun, 08 Dec 2019 17:30:50 +0800</pubDate><guid>https://111qqz.com/2019/12/feature-pyramid-networks/</guid><description>
检测不同尺度的物体一直是计算机视觉领域中比较有挑战性的事情．我们先从之前的工作出发，并对比FPN比起之前的工作有哪些改进．
之前的工作 Featurized image pyramid 思路是对于同一张图，生成不同的scale，然后每个scale的image单独去做检测． 这个方法是手工设计feautre时代的常用办法． 这个办法是比较显然的，也的确可以解决检测不同尺度物体的问题． 缺点非常明显...inference的速度几乎和scale的个数线性相关． 以及由于显存的开销，没办法做end-to-end 的training.
Single feature map 再之后，手工设计的feature逐渐被由CNN生成的feature取代了． 这种办法更加鲁棒，对image的一些变化不敏感． 但是如果只有一个scale 的图片去过这个feature map,只有最终的feature map去做predict..准确率不太行．．因此还是要与Featurized image pyramid一起．只是优化了得到feature 的部分．
Pyramidal feature hierarchy 就..CNN本身就有显然的层次结构啊．．． 为什么不直接拿来用，而是提前scale image呢．．．
也就是选若干个feature map直接去做predict.. 这个办法美滋滋，既有多个feature map保证了一定的准确率，同时也没有增加很多inference的cost.
SSD应该是率先使用这种方法的． 但是这种办法仍然有不足之处，就是低层的高分辨率的feature map的semantics 太弱了．． 这就导致说对小物体的检测效果不太理想．．．
那么该怎么办呢．．．这时候就该介绍FPN啦
Feature Pyramid Networks 后面再更．
FPN 用于 faster rcnn FPN同时作用于RPN阶段和fast-rcnn detector
以下的代码实现出自　fpn.pytorch
FPN 作用于RPN 感觉直接上代码比较容易理解
先看　整个网络的forward函数
1 2 3 def forward(self, im_data, im_info, gt_boxes, num_boxes): 4 batch_size = im_data.</description></item><item><title>SSD: Single Shot MultiBox Detector 学习笔记</title><link>https://111qqz.com/2019/12/single-short-detector/</link><pubDate>Sun, 08 Dec 2019 15:00:15 +0800</pubDate><guid>https://111qqz.com/2019/12/single-short-detector/</guid><description>
概述 SSD是一种单阶段目标检测算法．所谓单阶段，是指只使用了一个deep neural network,而不是像faster-rcnn这种两阶段网络． 为什么有了faster-rcnn还要使用SSD? 最主要是慢... 两阶段网络虽然准确率高，但是在嵌入式等算力不足的设备上做inference速度非常感人，很难达到real time的要求． （实际业务上也是这样，公有云上的检测模型几乎都是faster-rcnn,而到了一些盒子之类的硬件设备，检测模型就全是SSD等single stage 模型了)
之前一直没有写SSD是因为相比faster rcnn的细节，SSD的问题似乎并不是很多．直到最近转模型的时候被FASF模型的一个细节卡了蛮久，因此决定还是记录一下．
基本概念 这部分描述SSD中涉及到的一些想法．
prior box prior box的概念其实与faster-rcnn中anchor的概念是一样的，没有本质区别． 与faster-rcnn中的anchor不同的是，SSD会在多个feature map中的每个cell 都生成若干个prior_box.
对于一个特定的feature map,尺寸为m*n,假设有k个prior box,c种类别． 那么feature map的每个location会生成 k*(c+4) 个结果，其中c代表每一类的confidence. ４代表相对prior_box中心点的offset. 整个feature_map会生成　 kmn(c+4) 个结果．
prior_box的参数选择,以及一些训练有关的细节可以参考原论文,这里不再赘述. 这里主要想强调一下和priox box有关的inference 细节. 主要是decode box的部分.
由于模型输出的bbox其实是相对每个prior_box的offset,不是真正的bbox,因此需要由网络输出的box_pred和prior box得到真正的bbox 坐标.这部分通常称为decode box,其实已经算是后处理部分了.
pytorch中decode box的代码如下:
1 variance1, variance2 = variance 2 cx = box_prior[:, :, 3 0] + box_pred[:, :, 0] * variance1 * box_prior[:, :, 2] 4 cy = box_prior[:, :, 5 1] + box_pred[:, :, 1] * variance1 * box_prior[:, :, 3] 6 w = box_prior[:, :, 2] + torch.</description></item><item><title>Kubernetes(k8s)在深度学习模型转换方面的探索</title><link>https://111qqz.com/2019/11/K8s-for-Model-Conversion/</link><pubDate>Fri, 22 Nov 2019 11:14:19 +0800</pubDate><guid>https://111qqz.com/2019/11/K8s-for-Model-Conversion/</guid><description>
年中的时候接了离职的同事模型转换的锅，在不断地更新迭代的过程中，发现了一些痛点。 发现k8s能够解决一部分痛点，因此来分享一下在这方面的探索。
什么是模型转换 简单来说，深度学习模型的流程分为training和inference两部分。训练时用的一般是pytorch等框架，这些框架训练出的model是没办法直接部署在各个硬件平台上做inference的。因此需要将使用训练框架得到的模型，转换为能够部署到各个硬件平台上的模型。这个过程就是模型转换。
模型转换的一般流程为，先将pytorch等训练框架训练得到的模型转换为caffe model（是的，caffe才是业界中间表示的事实标准，而不是号称支持所有框架中间表示的onnx)，再将caffe model 转换到各种硬件上（包括但不限于nvidia系列显卡，华为的海思系列设备等）
（事实上这个转换流程是有硬伤存在的，这个硬伤在于pytorch的模型权重和模型结构是分离的。调研过业界一些其他模型转换的解决方案，包括百度的X2Paddle,其做法是不把pytorch模型作为模型转换的起点，而是从caffe model/onnx model 开始转换。还调研过微软的MMdnn,里面似乎也没有提到这个问题。不知道pytorch 1.0版本新增的torchscript是不是一条出路...)
模型转换的痛点 最初模型转换的痛点是依赖比较多，从pytorch,onnx,caffe再到tensorrt,cuda等．　尤其编译caffe,又是出了名的坑多．　解决办法是用了docker,将所有环境封装好．这样其他人可以直接拉镜像下来进行模型转换．
然而，在用了docker之后，还是有其他痛点，主要是以下三个:
权限申请的繁琐．
caffe模型转换到不同硬件上（主要是nvidia显卡），需要在对应的机器上进行转换．通常是客户确定使用某型号的显卡，然后我们采购对应显卡的机器．接下来需要一系列权限申请，包括服务器的权限，docker命令的权限，以及docker镜像仓库的权限．　除此之外，用户还需要将模型文件从其他位置放置到新服务器上，这件事也非常麻烦．
docker 镜像的版本控制.
docker的image虽然可以打tag，但是这个tag几乎对版本控制没有帮助．由于模型转换工具更新频繁（包括但是不限于，添加新的op/layer,caffe对新显卡的编译支持，caffe2tensorrt工具的更新），这么弱的版本管理就是灾难．如果我们对于每次升级docker image,都使用一个新的tag来标记的话，docker似乎没有一个机制来通知这个新的tag的存在．用户还是可以在这个旧的镜像上用到天荒地老．如果复用之前的tag的话,同样，用户还是可以不更新．
对docker commit 的滥用．
本来docker image的大小只有7G+,但是由于用户非常钟爱docker cp命令以及docker commit 命令．曾经有用户把整个数据集都docker cp进去之后docker commit ..　然后现在镜像大小已经有35G了．．
上述几个痛点，恰好k8s可以比较好的解决．
k8s用于模型转换. 官方的说法是,k8s是一个生产级别的容器编排系统，用于容器化应用的自动部署、扩缩和管理
然而似乎我对k8s的使用是非常非主流的(其实对docker的使用也很非主流...)
不过我觉得没有规定一项技术只能用来做什么,只要解决的问题多于引入的问题,就可以尝试.
k8s的引入使得用户不再能直接接触到docker image. 这直接解决了第一和第三两个痛点. 以及可以使用 imagePullPolicy 来进行强制更新,也基本解决了版本控制的问题. 这样就不用天天push 其他用户&amp;quot; 你先docker pull一下&amp;quot;...
此外,k8s的label机制也很有用. 我们可以通过给node打上不同显卡型号的label.然后维护一个显卡的列表.这样用户不再需要知道哪台服务器上有哪个显卡,只需要添写显卡型号,k8s就可以分配一台对应的机器供用户进行模型转换.
不过实际上k8s用户模型转换也有一些缺点:
k8s对显卡的调度是对于pod(pod就是一组container)独占的.也就是说,在一台服务器上,通过k8s只能开启与显卡个数一样多的pod.目前来看不会有什么大的影响,不过这对于对性能要求不敏感的应用,确实算个缺点.
pod名称的全局唯一性.由于开启任务需要显式提供一个名称,如果多个用户使用了相同的名称的话,结果显然不是我们期望的. 目前的解决方案是制定一个命名规范,要求用户遵守.但是这种规范并没有任何检查的机制,因此只能算一个tricky的办法,算不上真正的解决方案.</description></item><item><title>faster rcnn 模型 tensorrt4与tensorrt5 结果不一致 踩坑记录</title><link>https://111qqz.com/2019/11/debug-Frcnn-Model-When-Upgrading-From-Tensorrt4-to-Tensorrt5/</link><pubDate>Thu, 07 Nov 2019 23:40:39 +0800</pubDate><guid>https://111qqz.com/2019/11/debug-Frcnn-Model-When-Upgrading-From-Tensorrt4-to-Tensorrt5/</guid><description>
最近有同事report给我们,用同一个模型转换工具,转同一个faster rcnn 模型, 同样的sdk代码,在有些显卡上结果正常,但是再比较新的显卡上(比如Titan V)上 结果完全不正确.
听说之后我的内心其实是 **喵喵喵喵喵?**的
先在模型转换工具中infer了一下,发现...结果竟然真的不一样!
于是又开始了debug faster rcnn 的旅程(奇怪..我为什么要说又)
一份典型的faster rcnn 的
prototxt
按照经验,我们先对照了ROIS,来判断RPN 是否存在问题
惊讶地发现,竟然是没有问题的...
那看一下模型的输出 cls_score 和 bbox_pred好了 发现cls_score 完全对不上,bbox_pred 倒是没问题... 这和之前遇到的情况不太一样啊... 从proposal layer 到 最后... 全都是些很常见的layer... 哪里会有问题呢...
最终发现有问题的layer 是softmax!
cls_score 的维度应该为 N*K*C*H*W
softmax应该按照C这一维度来做,因此softmax_param中axis应该为2.
查阅&amp;quot;tensorrt support matrix&amp;quot; ，发现 tensorrt5中，softmax会按照用户指定的维度进行。
而对于tensorrt4.0, softmax layer 与设定的softmax_param无关，只会在channel 的维度上来做softmax.
所以，比较好的解决办法是，干脆不写softmax_param这个参数 对于trt4，会直接按照channel 维度来做 对于trt5，会在第N-3的axis上进行。对于SSD的 N*C*H*W或者 faster rcnn 的N*K*C*H*W, N-3都是channel所在的维度。
问题解决！</description></item><item><title>Anchor Box Algorithm</title><link>https://111qqz.com/2019/07/Anchor-Box-Algorithm/</link><pubDate>Mon, 01 Jul 2019 19:53:24 +0800</pubDate><guid>https://111qqz.com/2019/07/Anchor-Box-Algorithm/</guid><description>
动机 将一张图分成多个grid cell进行检测之后,每个cell只能检测到一个object. 如果这个grid cell中不止有一个物体要怎么办呢? 因此提出了anchor box algorithm来解决这个问题.
什么是anchor anchor其实就是一组预设的参考框,每个框有不同的长宽比和大小. 提供参考框可以将问题转换为&amp;quot;这个固定参考框中有没有认识的目标，目标框偏离参考框多远&amp;quot;. 这样如果一个grid cell中有多个物体,那么就可以形状最姐姐的anchor box来负责检测该物体. anchor的其他用途 实际上当grid cell很多的时候,一个grid cell中有多个object的情况是很少的.但是anchor box仍然是十分重要的.因为我们可以通过预设一些特殊的anchor box,来帮助检测到一些极端形状的物体(比如很长的物体或者很宽的物体)
输出结果 有了anchor之后,每个grid cell的输出可能不再唯一. 于是从之前的每个grid cell对应一个输出结果,变成了每个二元组(grid_cell,anchor box)对应一个结果. 结果的格式呢,一图胜千言: 局限性 由于anchor box是要预设的,那么如果我只预设了两个anchor box,但是一个grid cell中有三个object,怎么办呢? 很遗憾,是没有办法的. 另外一个case是,如果两个object都与其中一个anchor的形状最为接近,也是没有办法检测出两个物体的.
anchor box的生成 最初anchor box是通过人手工设定的,之后的YOLO的paper中提出了使用k-means的算法来生成anchor box.</description></item><item><title>目标检测领域的滑动窗口算法</title><link>https://111qqz.com/2019/06/sliding-windows/</link><pubDate>Sun, 30 Jun 2019 16:55:40 +0800</pubDate><guid>https://111qqz.com/2019/06/sliding-windows/</guid><description>
对象检测（Object Detection）的目的是”识别对象并给出其在图中的确切位置”，其内容可解构为三部分：
识别某个对象（Classification）； 给出对象在图中的位置（Localization）； 识别图中所有的目标及其位置（Detection）。 本文将介绍滑动窗口这一方法.
滑动窗口 滑动窗口是这些方法中最暴力的一个.简单来说,就是暴力枚举侯选框的尺寸和位置,每次crop得到一张小图,将每个小图送进后面的分类器进行分类. 早年后面通常会接一个计算量比较小的分类器,比如SVM,随着算力的提升,现在常常后面会接CNN. 值得一提的是,原始的滑动窗口方法是将每个小图,分别放入后面的分类器.但是实际上,小图和小图之间,是有overlap的,也就是说做了很多重复的计算. 因此一个显然的改进是使用CNN来实现滑动窗口算法, 这种方法的优点是比较无脑,实现和理解起来都很简单.缺点是计算量还是比较大.
参考链接 Sliding Windows for Object Detection with Python and OpenCV
深度学习基础 - 对象检测（滑窗、CNN、YOLO）</description></item><item><title>caffe2 添加自定义operater</title><link>https://111qqz.com/2018/04/add-custom-operation-in-caffe2/</link><pubDate>Fri, 13 Apr 2018 03:08:19 +0000</pubDate><guid>https://111qqz.com/2018/04/add-custom-operation-in-caffe2/</guid><description>
记录一些一个没有之前没有接触过caffe/caffe2的人为了添加自定义的op 到caffe2需要做的工作.
首先参考caffe2 tutorial,随便跑个op来试试,不妨以比较简单的 Accumulate_op 为例子.
这个op的作用就是计算Y=X+gamma*Y, 其中X为输入,Y为输出,gamma是参数.
跑起来这个运算所需要的代码如下:
from caffe2.python import workspace, model_helper import numpy as np # Create the input data data = np.arange(6).reshape(2,3).astype(np.float32) print (&amp;quot;data=&amp;quot;,data) # Create labels for the data as integers [0, 9]. workspace.FeedBlob(&amp;quot;data&amp;quot;, data) # Create model using a model helper m = model_helper.ModelHelper(name=&amp;quot;my first net&amp;quot;) output = m.net.Accumulate([&amp;quot;data&amp;quot;], &amp;quot;output&amp;quot;) print(m.net.Proto()) workspace.RunNetOnce(m.param_init_net) workspace.CreateNet(m.net) workspace.RunNet(m.name,2) # run 2 times print(&amp;quot;output=&amp;quot;,workspace.FetchBlob('output')) c之后我们仿照caffe2/operators/accumultate_op.h和affe2/operators/accumultate_op.cc,仿写一个我们自己的运算atest_op.h和atest_op.cc
实现的功能为Y=5X+gammaY
 #include &amp;quot;caffe2/operators/atest_op.</description></item><item><title>非极大值抑制（Non-Maximum Suppression，NMS）</title><link>https://111qqz.com/2018/03/non-maximum-suppression/</link><pubDate>Fri, 16 Mar 2018 02:56:14 +0000</pubDate><guid>https://111qqz.com/2018/03/non-maximum-suppression/</guid><description>
NMS是为了在诸多CV任务如边缘检测，目标检测等，找到局部最大值
其主要思想是先设定一个阈值，然后计算检测框的IOU(所谓IOU，也就是intersection-over-union，指的是相交面积除以相并面积，是来衡量overlap程度的指数）。如果IOU大于阈值，说明overlap过大，我们要通过某种算法来将其剔除。
比如下图，在经典的人脸识别任务中，出现了多个检测框，每个检测框有一个置信度confidence，我们通过某个算法，保留一个最好的。
顺便说一下算法的实现步骤把，其实不太重要。就是贪心。
其基本操作流程如下： * 首先，计算每一个 bounding box 的面积： * (x1, y1) ⇒ 左上点的坐标，(x2, y2) ⇒ 右下点的坐标； * (x2-x1+1)x(y2-y1+1) * 根据 scores 进行排序（一般从小到大），将 score 最大的bounding box置于队列，接下来计算其余 bounding box 与当前 score 最大的 bounding box 的 IoU，抑制（忽略也即去除）IoU大于设定阈值的 bounding box； * 重复以上过程，直至候选 bounding boxes 为空； 最后上一段python代码吧...也很简单，直接转载了别人的...
def nms(dets, thresh): x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) # 每个boundingbox的面积 order = scores.</description></item><item><title>reid 相关任务记录</title><link>https://111qqz.com/2018/02/reid-task-notes/</link><pubDate>Sat, 24 Feb 2018 04:34:02 +0000</pubDate><guid>https://111qqz.com/2018/02/reid-task-notes/</guid><description>
被师兄（同事？）普及了一番实验规范orz...
我还是太年轻了
所谓的一个fc的版本是右边的．一个放着不动，另一个在sequence_len（１０）的维度上做ave,然后再expand成原来的维度．如下图．
任务命名规则：
如D1V2_a_1,D1表示使用第一个数据集，V2表示是第二个大版本，ａ表示在V２大版本上的微调，最后的数字表示这是第几次运行该任务（跑三次以减少波动的影响）
logdir的地址为:/mnt/lustre/renkuanze/Data_t1/reid/log/｛$jobname｝
* D1:使用ilivids 数据集 * D1V1表示最初始的　baseline model * D1V2表示改为使用一个fc * D1V2_a是一个在一个FC上，不添加光流的修改版本 * D1V2_b是在一个FC上的baseline版本（也就是有光流） * D1V2_c是在一个FC上，有光流，batchsize从３２改为６４，gpu数目从4改为8的版本 * D1V3表示将softmax改为sigmod * D1V3_b表示将softmax改为sigmod的baseline版本 * D2:使用prid2011数据集 * D2V1表示初始的baseline model * D2V2表示改为使用一个fc * D2V2_b是在一个FC上的baseline版本 *</description></item><item><title>分类评价指标之Cumulative Match Characteristi (CMC)曲线</title><link>https://111qqz.com/2018/02/cumulative-Match-characteristi/</link><pubDate>Fri, 23 Feb 2018 08:20:55 +0000</pubDate><guid>https://111qqz.com/2018/02/cumulative-Match-characteristi/</guid><description>
CMC曲线全称是Cumulative Match Characteristic (CMC) curve，也就是累积匹配曲线，同ROC曲线Receiver Operating Characteristic (ROC) curve一样，是模式识别系统，如人脸，指纹，虹膜等的重要评价指标，尤其是在生物特征识别系统中，一般同ROC曲线（ 多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法）一起给出，能够综合评价出算法的好坏。
转一篇通俗易懂的解释：
Shortly speaking, imagine that you have 5 classes. For simplicity, imagine you have one test per class. Each test produces a score when compared to each class. Let's start from test1 which belongs to class1:
As your similarity measure, here, is Euclidian distance, the more distance a test has compared to a class, the less similarity is obtained.</description></item><item><title>光流法初探</title><link>https://111qqz.com/2018/02/Optical-flow-notes/</link><pubDate>Thu, 22 Feb 2018 09:03:48 +0000</pubDate><guid>https://111qqz.com/2018/02/Optical-flow-notes/</guid><description>
算是CV领域的传统算法了
只写两句话就够了。
**它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。** 研究光流场的目的就是为了从图片序列中近似得到不能直接得到的运动场。运动场，其实就是物体在三维真实世界中的运动；光流场，是运动场在二维图像平面上（人的眼睛或者摄像头）的投影。
参考资料：
光流Optical Flow介绍与OpenCV实现</description></item><item><title>end-to-end 神经网络</title><link>https://111qqz.com/2018/02/end-to-end-neural-network/</link><pubDate>Thu, 22 Feb 2018 02:53:01 +0000</pubDate><guid>https://111qqz.com/2018/02/end-to-end-neural-network/</guid><description>
所谓end-to-end 神经网络，更多是一种思想。
这种思想的核心是，比如对于图像处理，输入原始图像数据，输出的是直接有用的结果（有用取决于具体的任务，比如自动驾驶)
也就是尽可能少得减少人为干预，使训练是end (原始数据) to end (对应用问题直接有用的结果)
端到端指的是输入是原始数据，输出是最后的结果，原来输入端不是直接的原始数据，而是在原始数据中提取的特征，这一点在图像问题上尤为突出，因为图像像素数太多，数据维度高，会产生维度灾难，所以原来一个思路是手工提取图像的一些关键特征，这实际就是就一个降维的过程。 那么问题来了，特征怎么提？ 特征提取的好坏异常关键，甚至比学习算法还重要，举个例子，对一系列人的数据分类，分类结果是性别，如果你提取的特征是头发的颜色，无论分类算法如何，分类效果都不会好，如果你提取的特征是头发的长短，这个特征就会好很多，但是还是会有错误，如果你提取了一个超强特征，比如染色体的数据，那你的分类基本就不会错了。 这就意味着，特征需要足够的经验去设计，这在数据量越来越大的情况下也越来越困难。 于是就出现了端到端网络，特征可以自己去学习，所以特征提取这一步也就融入到算法当中，不需要人来干预了。
简单得说，符合end-to-end 的神经网络，特征应该是网络自己学习，而不是人为提取。
参考资料：
什么是 end-to-end 神经网络？</description></item><item><title>Pose-driven Deep Convolutional Model for Person Re-identification 阅读笔记</title><link>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification/</link><pubDate>Thu, 22 Feb 2018 02:25:00 +0000</pubDate><guid>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification/</guid><description>
1709.08325
Reid问题指的是判断一个probe person 是否在被不同的camera捕获的gallery person 中出现。
通常是如下情景：给出一个特定camera下某个特定人的probe image 或者 video sequence，从其他camera处询问这个人的图像，地点，时间戳。
ReID问题至今没有很好得解决，主要原因是，不同camera下，人的姿势（pose),观察的视角(viewpoint) 变化太大了。
传统方法主要在两个大方向上努力:
1. **用一些在图像上抽取的不变量来作为人的特征feture** 2. **去学习一个不同的距离度量方式，以至于同一个人在不同camera之间的距离尽可能小。** 但是由于在实际中，行人的pose和 摄像机的viewpoint不可控，因此与之相关的feture可能不够健壮。
学习新的不同的距离度量方式需要每对camera分别计算距离，然而这是O(n^2)的时间复杂度，凉凉。
近些年Deep learning发展迅猛，并且在很多CV任务上表现良好。所以自然有人想把Deep learning 方法应用到Reid任务上。
目前Deep learning的做法一般分为两部分：
* 使用softmax loss 结合person ID labels得到一个global representation * 首先用预定义好的body 刚体模型去得到local representation,然后将global 和local representation 融合。 目前用deep learning的方法效果已经不错了，比传统方法要好。但是目前的deep learning方法没有考虑到人的姿势(pose)的变化。
虽然目前也有些deep learning的办法在处理Reid问题时使用pose estimation algorithms 来预测行人的pose,
但是这种办法是手动完成而不是一个end-to-end（什么是end-to-end 神经网络） 的过程
所以考虑pose的潜力还没有被完全发掘。
这篇paper主要做了以下工作：
* 提出了一种新的深层结构，将身体部分转化成相应的特征表示，以此来克服pose变化带来的问题 * 提出了一个用来自动学习各部分权值的sub-network 这两部分工作都是end-to-end的</description></item><item><title>Similarity learning 和Metric learning</title><link>https://111qqz.com/2018/02/similarity-learning-metric-learning/</link><pubDate>Sun, 18 Feb 2018 08:14:10 +0000</pubDate><guid>https://111qqz.com/2018/02/similarity-learning-metric-learning/</guid><description>
Similarity_learning 相似性学习（Similarity learning ）有监督机器学习，它与回归和分类密切相关，但目标是从实例中学习一个相似函数，以衡量两个对象的相似程度或相关程度。
Similarity learning通常有四种setups:
* regression similarity learning 在这种方式中，给出的数据是 ![(x_{i}^{1},x_{i}^{2})](https://wikimedia.org/api/rest_v1/media/math/render/svg/cfa249357a1b4a7baf332041d67e480d6bb1f8fb)  和他们的相似度 . 目标是学习到一个函数 ，对于 给出yi的近似值。 * Classification similarity learning 给出数据 和他们是否相似 。目标是训练出一个分类器，能够完成对一组 是否相似的二分类判断。 * Ranking similarity learning 给出有序三元组 ，其中 is known to be more similar to than to 。目标是训练出一个函数，使得对于新的三元组 ，满足 。容易看出，这种方式采取了比回归更弱的监督形式，因为不需要提供精确的相似性度量，只需要提供相似性的相对顺序。因此，这种ranking-based的相似性学习更容易应用于实际的大规模应用 * Locality sensitive hashing (LSH) 局部敏感哈希和普通哈希的不同就是，相似的项有更大的概率被放到同一个桶中。
顺便一提，这里有一个叫triplet loss 的概念，
如上图所示，triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本和不同类的样本,这两个样本对应的称为Positive (记为x_p)和Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。
我们发现，triplet loss 其实就是在ranking similarity learning 问题中，学习similarity function时的loss
Metric learning 相似性学习与距离度量学习密切相关。度量学习的目标是在对象上学习一个距离函数。度量或距离函数必须遵循四个公理:非负性、不可分辨的恒等式、对称性和次可加性/三角形不等式。在实际应用中，度量学习算法忽略了不可分辨物体的身份条件，学习了一个伪度量。</description></item><item><title>persion reid 论文列表</title><link>https://111qqz.com/2018/02/persion-reid-paper-list/</link><pubDate>Sat, 17 Feb 2018 07:17:49 +0000</pubDate><guid>https://111qqz.com/2018/02/persion-reid-paper-list/</guid><description>
Key:
(1). Pose-driven, body part alignment, combine whole feature and body part feature, focus on alignment of part model,
(2). Combine image label and human attributes classes, do classification with attributes and identity learning
(3). Based on triplet loss, improve metric learning for an end to end learning
(4). Post-process, re-ranking
AlignedReID: Surpassing Human-Level Performance in Person Re-Identification
Hydraplus-net: Attentive deep features for pedestrian analysis.
Darkrank: Accelerating deep metric learning via cross sample similarities transfer.</description></item></channel></rss>