<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>优化 on 111qqz的小窝</title><link>https://111qqz.com/categories/%E4%BC%98%E5%8C%96/</link><description>Recent content in 优化 on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Sat, 21 Aug 2021 17:43:02 +0800</lastBuildDate><atom:link href="https://111qqz.com/categories/%E4%BC%98%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>ska::flat_hash_map 源码分析</title><link>https://111qqz.com/2021/08/ska_flat_hash_map_notes/</link><pubDate>Sat, 21 Aug 2021 17:43:02 +0800</pubDate><guid>https://111qqz.com/2021/08/ska_flat_hash_map_notes/</guid><description>
背景 最近在调研各种hashmap.. 发现ska::flat hash map性能优秀。。于是来看看代码。。 发现最大的特点是,ska::flat_hash_map使用了带probe count上限的robin hood hashing
相关概念 Distance_from_desired 对于采用了open addressing的hash实现，当插入发生冲突时，会以一定方式(如线性探测、平方探测等)来探测下一个可以插入的slot. 因而实际插入的slot位置与理想的slot位置通常不相同，这段距离定义为distance_from_desired 在没有冲突的理想情况下，所有distance_from_desired的值应该都为0 distance_from_desired的一种更常见的说法叫做probe sequence lengths(PSL)
robin hood hashing robin hood hashing的核心思想是&amp;quot;劫富济贫&amp;quot; distance_from_desired小的slot被认为更&amp;quot;富有&amp;quot;，distance_from_desired大的slot被认为更&amp;quot;贫穷&amp;quot; 具体来说，当去插入一个新的元素时，如果当前位置的元素的distance_from_desired要比待插入元素的distance_from_desired要小，那么就将待插入元素放入当前位置，将当前位置的元素取出，寻找一个新的位置。
这样做使得所有元素的distance_from_desired的分布更为平均，variance更小。 这样的分布对cache更友好（几乎全部元素distance_from_desired都小于一个cache line的长度，因此在find的时候只需要fetch一次cache line），从而拥有更好的性能。
一般的robin hashing 在find时，一般用一个全局的最大distance_from_desired作为没有找到该元素终止条件。 一种常见的改进是,不维护全局最大distance_from_desired,而是在看到当前位置元素的distance_from_desired比要插入的元素的distance_from_desired小时终止。
1 2 iterator find(const FindKey&amp;amp; key) { 3 size_t index = 4 hash_policy.index_for_hash(hash_object(key), num_slots_minus_one); 5 EntryPointer it = entries + ptrdiff_t(index); 6 for (int8_t distance = 0; it-&amp;gt;distance_from_desired &amp;gt;= distance; 7 ++distance, ++it) { 8 if (compares_equal(key, it-&amp;gt;value)) return {it}; 9 } 10 return end(); 11 } 12 带上限的robin hashing 一般的robin hashing在insert时，会不断进行寻找(包括了可能的swap过程)，直到找到一个空的slot为止。该过程在hash table较满时可能接近线性的时间复杂度。 ska::flat_hash_map对这一点的改进是，限制了insert时尝试的上限次数，作者给出的经验值为log(N),其中N为slots的个数。 这样保证每个slot的最大distance_from_desired不会超过log(N)</description></item><item><title>一次avx2在gcc上core dump的排查经历</title><link>https://111qqz.com/2021/07/core-dump-on-gcc-4-with-avx2/</link><pubDate>Thu, 22 Jul 2021 20:01:50 +0800</pubDate><guid>https://111qqz.com/2021/07/core-dump-on-gcc-4-with-avx2/</guid><description>
背景 起因是同事在实现int4的功能，结果流水线有一条死活过不了(gcc版本为4.8.5),一直core dump 经过初步排查，找出了如下最小可以复现的代码:
1 2#include &amp;lt;immintrin.h&amp;gt;3 4class Test{ 5 public: 6 Test(){ 7 tmp = _mm256_set_epi32(0,0,0,0,0,0,0,0); 8 } 9 private: 10 __m256i tmp; 11}; 12int main(){ 13 auto *tmp = new Test(); 14 return 0; 15} gcc版本为4.8.5 其中编译选项为
1g++ -std=c++11 -mavx2 a.cpp 2 现象为会core在 tmp = _mm256_set_epi32(0,0,0,0,0,0,0,0);
但是同样的代码，同样的编译选项，在gcc7.3上就不会发生core的问题。
初步排查 查看汇编代码,gcc4.8.5生成的如下:
1 2main: 3 push rbp 4 mov rbp, rsp 5 mov edi, 32 6 call operator new(unsigned long) 7 vpxor xmm0, xmm0, xmm0 8 vmovdqa YMMWORD PTR [rax], ymm0 9 mov eax, 0 10 pop rbp 11 ret 12 链接在这里</description></item><item><title>tensorRT 模型兼容性说明</title><link>https://111qqz.com/2020/03/tensorrt-model-compatibility-notes/</link><pubDate>Tue, 24 Mar 2020 12:26:01 +0800</pubDate><guid>https://111qqz.com/2020/03/tensorrt-model-compatibility-notes/</guid><description>
名词说明 CUDA. 一般来说指的是CUDA SDK. 目前经常使用的是CUDA 8.0和CUDA 10.1两个版本. 8.0和10.1都是SDK的版本号. CUDNN. The NVIDIA CUDA® Deep Neural Network library (cuDNN). 是一个可以为神经网络提供GPU加速的库 compute capability. 是GPU的固有参数,可以理解为GPU的版本.越新的显卡该数值往往越高. tensorRT.NVIDIA TensorRT™ is an SDK for high-performance deep learning inference. 是一个深度学习推理库,旨在提供高性能的推理速度. plan file,也称为 engine plan. 是生成的tensorRT 模型文件. 兼容性说明 Engine plan 的兼容性依赖于GPU的compute capability 和 TensorRT 版本, 不依赖于CUDA和CUDNN版本.
简单来说,在使用同样TensorRT版本的前提下,在具有相同compute capability 的GPU上的模型是可以通用的.
但是cuda版本是依赖于GPU的compute capability的. 也就是比较新的GPU(对应较高的compute capability)无法使用低版本的cuda.
CUDA SDK 8.0 support for compute capability 2.0 – 6.x CUDA SDK 9.0 – 9.</description></item><item><title>【施工中】 halide学习笔记</title><link>https://111qqz.com/2019/02/halide-notes/</link><pubDate>Mon, 18 Feb 2019 06:00:51 +0000</pubDate><guid>https://111qqz.com/2019/02/halide-notes/</guid><description>
**Halide is a programming language designed to make it easier to write high-performance image and array processing code on modern machines. ** halide有两个特性比较吸引人。一个是对于各种平台架构的支持。
* CPU architectures: X86, ARM, MIPS, Hexagon, PowerPC * Operating systems: Linux, Windows, macOS, Android, iOS, Qualcomm QuRT * GPU Compute APIs: CUDA, OpenCL, OpenGL, OpenGL Compute Shaders, Apple Metal, Microsoft Direct X 12 另一个是把计算什么和怎么计算(何时计算)分离开来。
Halide的Schedule可以由程序员来指定一些策略，指定硬件的buffer大小，缓冲线的相关设置，这样可以根据不同的计算硬件的特性来实现高效率的计算单元的调度，而图像算法的计算实现却不需要修改。</description></item><item><title>优化学习笔记(1):Loop unrolling</title><link>https://111qqz.com/2019/01/loop-unrolling/</link><pubDate>Wed, 23 Jan 2019 11:51:46 +0000</pubDate><guid>https://111qqz.com/2019/01/loop-unrolling/</guid><description>
迫于生计，最近要学习halide
先去学习/复习一下常见的编译优化技巧。
loop unrolling，也就是循环展开，顾名思义，就是把循环展开来写。
normal loop: int x; for (x = 0; x &amp;lt; 100; x++) { delete(x); } after loop unrolling: int x; for (x = 0; x &amp;lt; 100; x += 5 ) { delete(x); delete(x + 1); delete(x + 2); delete(x + 3); delete(x + 4); } 循环展开是一种优化，可以手动实现也可以编译器自动实现。
为什么要将循环展开？ * 循环每次都需要判断终止条件，展开后可以消除这部分开销。 * 减少[分支预测](https://en.wikipedia.org/wiki/Branch_predictor)开销。循环里的分支是指“跳出循环”还是“进行下一次迭代” * [vectorization](https://en.wikipedia.org/wiki/Automatic_vectorization) for (int y = 0; y &amp;lt; 4; y++) { for (int x_outer = 0; x_outer &amp;lt; 2; x_outer++) { // The loop over x_inner has gone away, and has been // replaced by a vectorized version of the // expression.</description></item><item><title>CUDA C Best Practices Guide 阅读笔记（二） Heterogeneous Computing</title><link>https://111qqz.com/2018/02/cuda-c-best-practices-guide-heterogeneous-computing/</link><pubDate>Tue, 13 Feb 2018 06:38:38 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-c-best-practices-guide-heterogeneous-computing/</guid><description>
CUDA 编程涉及到在不同的平台上同时运行代码:包含CPU的host 和包含GPU的device.
所以了解host和device的对性能优化是非常重要的。
2.1. Differences between Host and Device Threading resources host 上能同时运行的线程数目比较少（比如24个）
device上能同时运行的线程数目比较多（数量级通常为1E3，1E4等）
Threads 操作系统必须交换CPU执行通道上和下的线程以提供多线程功能。因此，上下文切换(当交换两个线程时)既慢又昂贵。
相比之下，GPU上的线程非常轻量级。在典型的系统中，成千上万的线程排队等待工作(每个线程有32个线程)。如果GPU必须等待 one warp of threads，它只需开始在另一个线程上执行工作。
简而言之，CPU内核被设计为每次最小化一个或两个线程的等待时间，而GPU被设计为处理大量并发的轻量级线程以最大化吞吐量。
RAM host和device 各自具有各自不同的附接物理存储器。host和device内存由PCI Express ( PCIe )总线分隔，因此host内存中的项目必须偶尔通过总线传送到device内存，反之亦然
2.2. What Runs on a CUDA-Enabled Device? 下面谈谈应该把应用的哪些部分放在device 上运行
* 大数据集上的算术运算 * 为了获得最佳性能，设备上运行的相邻线程的内存访问应该具有一定的一致性。**某些内存访问模式使硬件能够将多个数据项的读或写组合并到一个操作中**。当在CUDA上的计算中使用时，无法布局以实现合并的数据，或者没有足够的局部性来有效地使用L1或纹理缓存的数据，将倾向于看到较小的加速比。 * host和device之间的数据交换尽可能少 * **换到device上执行的数据一定会被做足够多的运算**，不然数据从Host传送到device的代价 可能与该运算在device上并行计算的优势向抵消，甚至得不偿失。 * **数据应尽可能长时间保存在设备上。**因为传输应该最小化，所以在同一数据上运行多个内核的程序应该倾向于在内核调用之间将数据保留在设备上，而不是将中间结果传输到主机，然后再将它们发送回设备进行后续计算。就是说，如果有一段连续的操作要处理某些数据，就算其中的部分操作在host上运行要比在device上快（比如不是算数运算而是逻辑处理），那么考虑到数据传输的巨大代价，将所有数据都放在device上处理可能会更好。这种处理原则即使相对较慢的内核也可能是有利的，如果它避免了一个或多个PCIe传输。</description></item><item><title>CUDA C Best Practices Guide 阅读笔记（1） 并行计算方法论(APOD)</title><link>https://111qqz.com/2018/02/cuda-c-best-practices-parallel-computing-methodology/</link><pubDate>Mon, 12 Feb 2018 04:58:31 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-c-best-practices-parallel-computing-methodology/</guid><description>
APOD指的是Assess, Parallelize, Optimize, Deploy
如图所示，APOD过程是一个循环的过程，每次只进行一部分，从A到P到O到D,然后再进行下一轮的APOD
Assess 对于一个项目，第一步要做的是接触(Assess)项目，得到项目代码中每部分的执行时间。有了这部分内容，开发者就可以找到并行优化的瓶颈所在，并开始尝试用GPU加速。
根据Amdahl's and Gustafson's laws，开发者可以确定并行优化的性能上界。
Parallelize 找到瓶颈所在并确定了优化的目标和期望，开发者就可以优化代码了。调用一些如cuBLAS, cuFFT, or Thrust 的 GPU-optimized library 可能会很有效。
另一方面，有些应用需要开发者重构代码，以此让可以被并行优化得部分被暴露出来。
Optimize 确定了需要被并行优化的部分之后，就要考虑具体得实现方式了。
具体得实现方式通常不过只有一种，所以充分理解应用的需求是很有必要的。
要记得，APOD是一个反复迭代的过程（找到可以优化的点，实现并测试优化，验证优化效果，然后再重复）
因为对于开发者来说，没有必要最初就找到解决所有性能瓶颈的策略。
优化可以在不同的level上进行，配合性能分析工具是很有帮助的。
Deploy 大的原则是当优化完一处之后，立刻将这一部分部署到生产环境，而不是再去寻找其他可以优化的地方。
这样做有很多重要的原因，比如这会使得用户尽早从这个优化中收益（尽管提速只是部分的，但是仍然有价值）
此外，每次有改动就重新部署，也使得变化是平稳而不是激进的，这有助于减少风险。
Recommendations and Best Practice 优化根据对性能影响程度的不同有不同的优先级。
在先要处理低优先级的优化之前，一定要确保其他所有的高优先级优化都做完了。
这种方法将倾向于为所投入的时间提供最佳结果，并且将避免过早优化的陷阱。
需要说明的一点是，教程中的代码为了方便简洁没有关于任何 check error的部分。
在实际上这是不可取的（这不同于编写C++代码!）</description></item><item><title>cuda c++ 基础算法库 thrust 学习笔记</title><link>https://111qqz.com/2018/02/cuda-thrust-notes/</link><pubDate>Sat, 10 Feb 2018 08:43:54 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-thrust-notes/</guid><description>
可以了解成并行版的STL(?
过了一遍nvidia的官方网文档
发现如果熟悉STL的话,thrust没什么太多好说的,看起来很简单...
不过还是开一篇记录一下,一段时间内估计要和cuda c++ 打交道,就当记录使用过程中遇到的问题吧.</description></item><item><title>cuda error checking 学习笔记</title><link>https://111qqz.com/2018/02/cuda-error-checking-notes/</link><pubDate>Fri, 09 Feb 2018 06:55:00 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-error-checking-notes/</guid><description>
由于发现cuda c++ 的 debug方式和c++ 差别很大,因此打算再开一篇,专门记录一些和error checking 以及debug有关的内容.
Error checks in CUDA code can help catch CUDA errors at their source. There are 2 sources of errors in CUDA source code:
1. Errors from CUDA **API** calls. For example, a call to `cudaMalloc()` might fail. 2. Errors from CUDA **kernel** calls. For example, there might be invalid memory access inside a kernel. All CUDA API calls return a cudaError value, so these calls are easy to check:</description></item><item><title>cuda 学习笔记</title><link>https://111qqz.com/2018/02/cuda-notes/</link><pubDate>Thu, 01 Feb 2018 07:20:04 +0000</pubDate><guid>https://111qqz.com/2018/02/cuda-notes/</guid><description>
uodate:有毒吧。kernel中出问题原来是不会报错的。。。。
请教了组里的hust学长orz..、
学到了cuda-memcheck命令和cudaGetLastError来查看问题。。可以参考What is the canonical way to check for errors using the CUDA runtime API?
先放一波资料。
* &amp;lt;del&amp;gt;[An Even Easier Introduction to CUDA](https://devblogs.nvidia.com/even-easier-introduction-cuda/)&amp;lt;/del&amp;gt; * &amp;lt;del&amp;gt;[CUDA C/C++ Basics](https://drive.google.com/open?id=1kHYyM4yiJoyjkWjp7FJp0vae_TcvskjK)&amp;lt;/del&amp;gt; * &amp;lt;del&amp;gt;[nvidia-thrust 官方文档](http://docs.nvidia.com/cuda/thrust/index.html)&amp;lt;/del&amp;gt; * [how-access-global-memory-efficiently-cuda-c-kernels](https://devblogs.nvidia.com/how-access-global-memory-efficiently-cuda-c-kernels/) * [efficient-matrix-transpose-cuda-cc](https://devblogs.nvidia.com/efficient-matrix-transpose-cuda-cc/) * [很强大的warp内shuffle](https://devblogs.nvidia.com/faster-parallel-reductions-kepler/) * [cuda-GDB官方文档](http://docs.nvidia.com/cuda/cuda-gdb/index.html) * [cuda-c-best-practices-guide](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/) cuda 提出的目的是能够让程序员透明地使用GPU来高效地进行并行运算。
kernel和c语言中的函数相似，函数名字前通常用global来标识。
下面考虑一个2个大小的1M的数组相加的例子。
总的思路是通过并行，来观察到计算速度的加快。
如果不考虑并行，2个数组相加的代码，如下：
#include &amp;lt;iostream&amp;gt; #include &amp;lt;math.h&amp;gt; // function to add the elements of two arrays void add(int n, float *x, float *y) { for (int i = 0; i &amp;lt; n; i++) y[i] = x[i] + y[i]; } int main(void) { int N = 1&amp;lt;&amp;lt;20; // 1M elements float *x = new float[N]; float *y = new float[N]; // initialize x and y arrays on the host for (int i = 0; i &amp;lt; N; i++) { x[i] = 1.</description></item></channel></rss>