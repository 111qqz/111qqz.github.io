<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>computer vision on 111qqz的小窝</title>
    <link>https://111qqz.com/categories/computer-vision/</link>
    <description>Recent content in computer vision on 111qqz的小窝</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Mar 2018 02:56:14 +0000</lastBuildDate>
    
	<atom:link href="https://111qqz.com/categories/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>非极大值抑制（Non-Maximum Suppression，NMS）</title>
      <link>https://111qqz.com/2018/03/%e9%9d%9e%e6%9e%81%e5%a4%a7%e5%80%bc%e6%8a%91%e5%88%b6%ef%bc%88non-maximum-suppression%ef%bc%8cnms%ef%bc%89/</link>
      <pubDate>Fri, 16 Mar 2018 02:56:14 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/03/%e9%9d%9e%e6%9e%81%e5%a4%a7%e5%80%bc%e6%8a%91%e5%88%b6%ef%bc%88non-maximum-suppression%ef%bc%8cnms%ef%bc%89/</guid>
      <description>NMS是为了在诸多CV任务如边缘检测，目标检测等，找到局部最大值 其主要思想是先设定一个阈值，然后计算检测框的IOU(所谓IOU，也就是int</description>
    </item>
    
    <item>
      <title>非极大值抑制（Non-Maximum Suppression，NMS）</title>
      <link>https://111qqz.com/2018/03/%e9%9d%9e%e6%9e%81%e5%a4%a7%e5%80%bc%e6%8a%91%e5%88%b6%ef%bc%88non-maximum-suppression%ef%bc%8cnms%ef%bc%89/</link>
      <pubDate>Fri, 16 Mar 2018 02:56:14 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/03/%e9%9d%9e%e6%9e%81%e5%a4%a7%e5%80%bc%e6%8a%91%e5%88%b6%ef%bc%88non-maximum-suppression%ef%bc%8cnms%ef%bc%89/</guid>
      <description>NMS是为了在诸多CV任务如边缘检测，目标检测等，找到局部最大值 其主要思想是先设定一个阈值，然后计算检测框的IOU(所谓IOU，也就是int</description>
    </item>
    
    <item>
      <title>reid 相关任务记录</title>
      <link>https://111qqz.com/2018/02/reid-%e7%9b%b8%e5%85%b3%e4%bb%bb%e5%8a%a1%e8%ae%b0%e5%bd%95/</link>
      <pubDate>Sat, 24 Feb 2018 04:34:02 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/reid-%e7%9b%b8%e5%85%b3%e4%bb%bb%e5%8a%a1%e8%ae%b0%e5%bd%95/</guid>
      <description>被师兄（同事？）普及了一番实验规范orz&amp;hellip; 我还是太年轻了 所谓的一个fc的版本是右边的．一个放着不动，另一个在sequence_</description>
    </item>
    
    <item>
      <title>reid 相关任务记录</title>
      <link>https://111qqz.com/2018/02/reid-%e7%9b%b8%e5%85%b3%e4%bb%bb%e5%8a%a1%e8%ae%b0%e5%bd%95/</link>
      <pubDate>Sat, 24 Feb 2018 04:34:02 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/reid-%e7%9b%b8%e5%85%b3%e4%bb%bb%e5%8a%a1%e8%ae%b0%e5%bd%95/</guid>
      <description>被师兄（同事？）普及了一番实验规范orz&amp;hellip; 我还是太年轻了 所谓的一个fc的版本是右边的．一个放着不动，另一个在sequence_</description>
    </item>
    
    <item>
      <title>分类评价指标之Cumulative Match Characteristi (CMC)曲线</title>
      <link>https://111qqz.com/2018/02/%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87%e4%b9%8bcumulative-match-characteristi-cmc%e6%9b%b2%e7%ba%bf/</link>
      <pubDate>Fri, 23 Feb 2018 08:20:55 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87%e4%b9%8bcumulative-match-characteristi-cmc%e6%9b%b2%e7%ba%bf/</guid>
      <description>CMC曲线全称是Cumulative Match Characteristic (CMC) curve，也就是累积匹配曲线，同ROC曲线Receiver Operating Characteristic (ROC) curve一样，是模式识别系统，</description>
    </item>
    
    <item>
      <title>分类评价指标之Cumulative Match Characteristi (CMC)曲线</title>
      <link>https://111qqz.com/2018/02/%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87%e4%b9%8bcumulative-match-characteristi-cmc%e6%9b%b2%e7%ba%bf/</link>
      <pubDate>Fri, 23 Feb 2018 08:20:55 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87%e4%b9%8bcumulative-match-characteristi-cmc%e6%9b%b2%e7%ba%bf/</guid>
      <description>CMC曲线全称是Cumulative Match Characteristic (CMC) curve，也就是累积匹配曲线，同ROC曲线Receiver Operating Characteristic (ROC) curve一样，是模式识别系统，</description>
    </item>
    
    <item>
      <title>光流法初探</title>
      <link>https://111qqz.com/2018/02/%e5%85%89%e6%b5%81%e6%b3%95%e5%88%9d%e6%8e%a2/</link>
      <pubDate>Thu, 22 Feb 2018 09:03:48 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%85%89%e6%b5%81%e6%b3%95%e5%88%9d%e6%8e%a2/</guid>
      <description>算是CV领域的传统算法了 只写两句话就够了。 **它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及</description>
    </item>
    
    <item>
      <title>光流法初探</title>
      <link>https://111qqz.com/2018/02/%e5%85%89%e6%b5%81%e6%b3%95%e5%88%9d%e6%8e%a2/</link>
      <pubDate>Thu, 22 Feb 2018 09:03:48 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%85%89%e6%b5%81%e6%b3%95%e5%88%9d%e6%8e%a2/</guid>
      <description>算是CV领域的传统算法了 只写两句话就够了。 **它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及</description>
    </item>
    
    <item>
      <title>Reid iLIDS-VID 数据集 切分</title>
      <link>https://111qqz.com/2018/02/reid-ilids-vid-%e6%95%b0%e6%8d%ae%e9%9b%86-%e5%88%87%e5%88%86/</link>
      <pubDate>Thu, 22 Feb 2018 08:12:58 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/reid-ilids-vid-%e6%95%b0%e6%8d%ae%e9%9b%86-%e5%88%87%e5%88%86/</guid>
      <description>ilids-vid数据集 150个人做trainning,150个人做 在training set里面，每个人有来自两个不同camera的两段vi</description>
    </item>
    
    <item>
      <title>Reid iLIDS-VID 数据集 切分</title>
      <link>https://111qqz.com/2018/02/reid-ilids-vid-%e6%95%b0%e6%8d%ae%e9%9b%86-%e5%88%87%e5%88%86/</link>
      <pubDate>Thu, 22 Feb 2018 08:12:58 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/reid-ilids-vid-%e6%95%b0%e6%8d%ae%e9%9b%86-%e5%88%87%e5%88%86/</guid>
      <description>ilids-vid数据集 150个人做trainning,150个人做 在training set里面，每个人有来自两个不同camera的两段vi</description>
    </item>
    
    <item>
      <title>end-to-end 神经网络</title>
      <link>https://111qqz.com/2018/02/end-to-end-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c/</link>
      <pubDate>Thu, 22 Feb 2018 02:53:01 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/end-to-end-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c/</guid>
      <description>所谓end-to-end 神经网络，更多是一种思想。 这种思想的核心是，比如对于图像处理，输入原始图像数据，输出的是直接有用的结果（有用取决于具</description>
    </item>
    
    <item>
      <title>end-to-end 神经网络</title>
      <link>https://111qqz.com/2018/02/end-to-end-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c/</link>
      <pubDate>Thu, 22 Feb 2018 02:53:01 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/end-to-end-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c/</guid>
      <description>所谓end-to-end 神经网络，更多是一种思想。 这种思想的核心是，比如对于图像处理，输入原始图像数据，输出的是直接有用的结果（有用取决于具</description>
    </item>
    
    <item>
      <title>Pose-driven Deep Convolutional Model for Person Re-identification 阅读笔记</title>
      <link>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 22 Feb 2018 02:25:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>1709.08325 Reid问题指的是判断一个probe person 是否在被不同的camera捕获的gallery person 中出现。 通常是如下情景：给出一个特定camera下某</description>
    </item>
    
    <item>
      <title>Pose-driven Deep Convolutional Model for Person Re-identification 阅读笔记</title>
      <link>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 22 Feb 2018 02:25:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>1709.08325 Reid问题指的是判断一个probe person 是否在被不同的camera捕获的gallery person 中出现。 通常是如下情景：给出一个特定camera下某</description>
    </item>
    
    <item>
      <title>Deep Mutual Learning（相互学习） 阅读笔记</title>
      <link>https://111qqz.com/2018/02/deep-mutual-learning-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sun, 18 Feb 2018 10:46:35 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/deep-mutual-learning-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>原始论文 DNN在很多问题上效果很不错，但是由于深度和宽度过大，导致需要的执行时间和内存过大。我们需要讨论一些能快速执行并且对内存的需要不大的</description>
    </item>
    
    <item>
      <title>Deep Mutual Learning（相互学习） 阅读笔记</title>
      <link>https://111qqz.com/2018/02/deep-mutual-learning-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sun, 18 Feb 2018 10:46:35 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/deep-mutual-learning-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>原始论文 DNN在很多问题上效果很不错，但是由于深度和宽度过大，导致需要的执行时间和内存过大。我们需要讨论一些能快速执行并且对内存的需要不大的</description>
    </item>
    
    <item>
      <title>persion reid 论文列表</title>
      <link>https://111qqz.com/2018/02/persion-reid-%e8%ae%ba%e6%96%87%e5%88%97%e8%a1%a8/</link>
      <pubDate>Sat, 17 Feb 2018 07:17:49 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/persion-reid-%e8%ae%ba%e6%96%87%e5%88%97%e8%a1%a8/</guid>
      <description>Key: (1). Pose-driven, body part alignment, combine whole feature and body part feature, focus on alignment of part model, (2). Combine image label and human attributes classes, do classification with attributes and identity learning (3). Based on triplet loss, improve metric learning for an end to end learning (4). Post-process, re-ranking AlignedReID: Surpassing Human-Level Performance in Person Re-Identification Hydraplus-net: Attentive deep features for pedestrian analysis. Darkrank: Accelerating deep metric learning</description>
    </item>
    
    <item>
      <title>persion reid 论文列表</title>
      <link>https://111qqz.com/2018/02/persion-reid-%e8%ae%ba%e6%96%87%e5%88%97%e8%a1%a8/</link>
      <pubDate>Sat, 17 Feb 2018 07:17:49 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/persion-reid-%e8%ae%ba%e6%96%87%e5%88%97%e8%a1%a8/</guid>
      <description>Key: (1). Pose-driven, body part alignment, combine whole feature and body part feature, focus on alignment of part model, (2). Combine image label and human attributes classes, do classification with attributes and identity learning (3). Based on triplet loss, improve metric learning for an end to end learning (4). Post-process, re-ranking AlignedReID: Surpassing Human-Level Performance in Person Re-Identification Hydraplus-net: Attentive deep features for pedestrian analysis. Darkrank: Accelerating deep metric learning</description>
    </item>
    
    <item>
      <title>cuda 学习笔记 术语篇</title>
      <link>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-%e6%9c%af%e8%af%ad%e7%af%87/</link>
      <pubDate>Fri, 09 Feb 2018 07:11:54 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-%e6%9c%af%e8%af%ad%e7%af%87/</guid>
      <description>最近在学习cuda,遇到的新词汇有点多,所以开一篇记录一下. 所记录的不一定和cuda有关,只是在学习cuda中遇到的陌生概念. * **SIMD(Single Instruction Multiple Data) 单指</description>
    </item>
    
    <item>
      <title>cuda 学习笔记 术语篇</title>
      <link>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-%e6%9c%af%e8%af%ad%e7%af%87/</link>
      <pubDate>Fri, 09 Feb 2018 07:11:54 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-%e6%9c%af%e8%af%ad%e7%af%87/</guid>
      <description>最近在学习cuda,遇到的新词汇有点多,所以开一篇记录一下. 所记录的不一定和cuda有关,只是在学习cuda中遇到的陌生概念. * **SIMD(Single Instruction Multiple Data) 单指</description>
    </item>
    
    <item>
      <title>cuda error checking 学习笔记</title>
      <link>https://111qqz.com/2018/02/cuda-error-checking-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Fri, 09 Feb 2018 06:55:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-error-checking-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>由于发现cuda c++ 的 debug方式和c++ 差别很大,因此打算再开一篇,专门记录一些和error checking 以及debug有关的内容. Error checks in CUDA code can help catch CUDA</description>
    </item>
    
    <item>
      <title>cuda error checking 学习笔记</title>
      <link>https://111qqz.com/2018/02/cuda-error-checking-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Fri, 09 Feb 2018 06:55:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-error-checking-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>由于发现cuda c++ 的 debug方式和c++ 差别很大,因此打算再开一篇,专门记录一些和error checking 以及debug有关的内容. Error checks in CUDA code can help catch CUDA</description>
    </item>
    
    <item>
      <title>多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法</title>
      <link>https://111qqz.com/2018/02/%e5%a4%9a%e6%a0%87%e7%ad%be%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e7%9a%84%e8%af%84%e4%bb%b7%e6%96%b9%e6%b3%95-map/</link>
      <pubDate>Fri, 09 Feb 2018 03:53:09 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%a4%9a%e6%a0%87%e7%ad%be%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e7%9a%84%e8%af%84%e4%bb%b7%e6%96%b9%e6%b3%95-map/</guid>
      <description>参考资料: 多标签图像分类任务的评价方法-mAP wiki_Sensitivity and specificity False Positives和False Negative等含义 mean average precision（MAP）在</description>
    </item>
    
    <item>
      <title>多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法</title>
      <link>https://111qqz.com/2018/02/%e5%a4%9a%e6%a0%87%e7%ad%be%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e7%9a%84%e8%af%84%e4%bb%b7%e6%96%b9%e6%b3%95-map/</link>
      <pubDate>Fri, 09 Feb 2018 03:53:09 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%a4%9a%e6%a0%87%e7%ad%be%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e7%9a%84%e8%af%84%e4%bb%b7%e6%96%b9%e6%b3%95-map/</guid>
      <description>参考资料: 多标签图像分类任务的评价方法-mAP wiki_Sensitivity and specificity False Positives和False Negative等含义 mean average precision（MAP）在</description>
    </item>
    
    <item>
      <title>Non-local Neural Networks 阅读笔记</title>
      <link>https://111qqz.com/2018/02/non-local-neural-networks-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Mon, 05 Feb 2018 02:24:34 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/non-local-neural-networks-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>先粗略读了2遍orz.可能不够严谨，先写一些high-level的理解。 对于序列或者图片数据，如果想获得一个long-range的依赖，通常</description>
    </item>
    
    <item>
      <title>Non-local Neural Networks 阅读笔记</title>
      <link>https://111qqz.com/2018/02/non-local-neural-networks-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Mon, 05 Feb 2018 02:24:34 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/non-local-neural-networks-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>先粗略读了2遍orz.可能不够严谨，先写一些high-level的理解。 对于序列或者图片数据，如果想获得一个long-range的依赖，通常</description>
    </item>
    
    <item>
      <title>non-local means algorithm 学习笔记</title>
      <link>https://111qqz.com/2018/01/non-local-means-algorithm-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 25 Jan 2018 02:53:52 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/01/non-local-means-algorithm-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>终于忙完学校的事情可以干正事了orz 这里会记录一些第一遍看paper的过程中遇到的一些影响理解的概念，不过大多不会深究，只算做粗浅的理解。 1</description>
    </item>
    
    <item>
      <title>non-local means algorithm 学习笔记</title>
      <link>https://111qqz.com/2018/01/non-local-means-algorithm-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 25 Jan 2018 02:53:52 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/01/non-local-means-algorithm-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>终于忙完学校的事情可以干正事了orz 这里会记录一些第一遍看paper的过程中遇到的一些影响理解的概念，不过大多不会深究，只算做粗浅的理解。 1</description>
    </item>
    
    <item>
      <title>反向传播学习笔记</title>
      <link>https://111qqz.com/2017/09/%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Tue, 05 Sep 2017 12:30:17 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/09/%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>先说下自己目前很笼统的理解： 反向传播是用来快速计算梯度的一种方法； 过程大概是把计算过程用计算图表示，这样每一个中间步骤都有一个节点，每一个l</description>
    </item>
    
    <item>
      <title>反向传播学习笔记</title>
      <link>https://111qqz.com/2017/09/%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Tue, 05 Sep 2017 12:30:17 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/09/%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>先说下自己目前很笼统的理解： 反向传播是用来快速计算梯度的一种方法； 过程大概是把计算过程用计算图表示，这样每一个中间步骤都有一个节点，每一个l</description>
    </item>
    
    <item>
      <title>Difference Between Deep Learning Training and Inference</title>
      <link>https://111qqz.com/2017/09/difference-between-deep-learning-training-and-inference/</link>
      <pubDate>Mon, 04 Sep 2017 09:36:15 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/09/difference-between-deep-learning-training-and-inference/</guid>
      <description>trainning的概念比较容易懂&amp;hellip;最近天天在说inference&amp;hellip;之前学习ML/DL 相关没有见过这个东西，有点</description>
    </item>
    
    <item>
      <title>Difference Between Deep Learning Training and Inference</title>
      <link>https://111qqz.com/2017/09/difference-between-deep-learning-training-and-inference/</link>
      <pubDate>Mon, 04 Sep 2017 09:36:15 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/09/difference-between-deep-learning-training-and-inference/</guid>
      <description>trainning的概念比较容易懂&amp;hellip;最近天天在说inference&amp;hellip;之前学习ML/DL 相关没有见过这个东西，有点</description>
    </item>
    
    <item>
      <title>tensorflow input pipline  学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-input-pipline-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 24 Aug 2017 09:12:58 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-input-pipline-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>参考资料： tf_doc_Reading data TENSORFLOW INPUT PIPELINE EXAMPLE tensorflow：理解tensorflow中的输入管道 第二个参考资料是第一个的翻译版本，翻译的水平一般，建议看原文</description>
    </item>
    
    <item>
      <title>tensorflow input pipline  学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-input-pipline-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 24 Aug 2017 09:12:58 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-input-pipline-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>参考资料： tf_doc_Reading data TENSORFLOW INPUT PIPELINE EXAMPLE tensorflow：理解tensorflow中的输入管道 第二个参考资料是第一个的翻译版本，翻译的水平一般，建议看原文</description>
    </item>
    
    <item>
      <title>tensorflow 术语总结（不断更新）</title>
      <link>https://111qqz.com/2017/08/tensorflow-%e6%9c%af%e8%af%ad%e6%80%bb%e7%bb%93%ef%bc%88%e4%b8%8d%e6%96%ad%e6%9b%b4%e6%96%b0%ef%bc%89/</link>
      <pubDate>Thu, 24 Aug 2017 07:53:12 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-%e6%9c%af%e8%af%ad%e6%80%bb%e7%bb%93%ef%bc%88%e4%b8%8d%e6%96%ad%e6%9b%b4%e6%96%b0%ef%bc%89/</guid>
      <description>epoch：一个数据集完整得过一遍叫一个epoch,比如100w个数据，batch_size=100,那么训练1W个batch才是一个epo</description>
    </item>
    
    <item>
      <title>tensorflow 术语总结（不断更新）</title>
      <link>https://111qqz.com/2017/08/tensorflow-%e6%9c%af%e8%af%ad%e6%80%bb%e7%bb%93%ef%bc%88%e4%b8%8d%e6%96%ad%e6%9b%b4%e6%96%b0%ef%bc%89/</link>
      <pubDate>Thu, 24 Aug 2017 07:53:12 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-%e6%9c%af%e8%af%ad%e6%80%bb%e7%bb%93%ef%bc%88%e4%b8%8d%e6%96%ad%e6%9b%b4%e6%96%b0%ef%bc%89/</guid>
      <description>epoch：一个数据集完整得过一遍叫一个epoch,比如100w个数据，batch_size=100,那么训练1W个batch才是一个epo</description>
    </item>
    
    <item>
      <title>tensorflow 合并模型</title>
      <link>https://111qqz.com/2017/08/tensorflow-%e5%90%88%e5%b9%b6%e6%a8%a1%e5%9e%8b/</link>
      <pubDate>Mon, 21 Aug 2017 06:56:22 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-%e5%90%88%e5%b9%b6%e6%a8%a1%e5%9e%8b/</guid>
      <description>在这里存个备份，还有些问题没有解决。 raise ValueError(&amp;ldquo;GraphDef cannot be larger than 2GB.&amp;rdquo;) 记录一些思路好了。现在是没有生成.meta文件，爆掉应该是因为所有的变量都加载到了默认图里</description>
    </item>
    
    <item>
      <title>tensorflow 合并模型</title>
      <link>https://111qqz.com/2017/08/tensorflow-%e5%90%88%e5%b9%b6%e6%a8%a1%e5%9e%8b/</link>
      <pubDate>Mon, 21 Aug 2017 06:56:22 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-%e5%90%88%e5%b9%b6%e6%a8%a1%e5%9e%8b/</guid>
      <description>在这里存个备份，还有些问题没有解决。 raise ValueError(&amp;ldquo;GraphDef cannot be larger than 2GB.&amp;rdquo;) 记录一些思路好了。现在是没有生成.meta文件，爆掉应该是因为所有的变量都加载到了默认图里</description>
    </item>
    
    <item>
      <title>tensorflow:tf.shape(a)和a.get_shape()区别</title>
      <link>https://111qqz.com/2017/08/tensorflowtf-shapea%e5%92%8ca-get_shape%e5%8c%ba%e5%88%ab/</link>
      <pubDate>Mon, 21 Aug 2017 03:25:42 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflowtf-shapea%e5%92%8ca-get_shape%e5%8c%ba%e5%88%ab/</guid>
      <description>相同点：都可以得到tensor a的尺寸 不同点：tf.shape()中a 数据的类型可以是tensor, list, array a.get_shape()中a的数据</description>
    </item>
    
    <item>
      <title>tensorflow:tf.shape(a)和a.get_shape()区别</title>
      <link>https://111qqz.com/2017/08/tensorflowtf-shapea%e5%92%8ca-get_shape%e5%8c%ba%e5%88%ab/</link>
      <pubDate>Mon, 21 Aug 2017 03:25:42 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflowtf-shapea%e5%92%8ca-get_shape%e5%8c%ba%e5%88%ab/</guid>
      <description>相同点：都可以得到tensor a的尺寸 不同点：tf.shape()中a 数据的类型可以是tensor, list, array a.get_shape()中a的数据</description>
    </item>
    
    <item>
      <title>tensorflow checkpoint 学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-checkpoint-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Mon, 21 Aug 2017 02:03:45 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-checkpoint-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>参考资料： What is the TensorFlow checkpoint meta file? TensorFlow: Restoring variables from from multiple checkpoints 合并模型的时候发现.meta一直在累加，而其他数据文件没有改变。因此来探究一下checkpoint的几</description>
    </item>
    
    <item>
      <title>tensorflow checkpoint 学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-checkpoint-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Mon, 21 Aug 2017 02:03:45 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-checkpoint-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>参考资料： What is the TensorFlow checkpoint meta file? TensorFlow: Restoring variables from from multiple checkpoints 合并模型的时候发现.meta一直在累加，而其他数据文件没有改变。因此来探究一下checkpoint的几</description>
    </item>
    
    <item>
      <title>tensorflow variable 学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-variable-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sun, 20 Aug 2017 09:36:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-variable-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>参考资料： programmers_guide/variables tf/Variable 之前感觉对tensorflow 的variable的理解不是很深刻&amp;hellip;跑个模型啥的倒不会有什么问题，但是涉及分布式，</description>
    </item>
    
    <item>
      <title>tensorflow variable 学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-variable-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sun, 20 Aug 2017 09:36:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-variable-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>参考资料： programmers_guide/variables tf/Variable 之前感觉对tensorflow 的variable的理解不是很深刻&amp;hellip;跑个模型啥的倒不会有什么问题，但是涉及分布式，</description>
    </item>
    
    <item>
      <title>tensorflow Session 学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-session-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sun, 20 Aug 2017 08:21:57 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-session-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>tensorflow-session官方文档 说下我自己的理解： session中文一般叫会话，可以理解成op执行时候需要的一层虚拟化的封装。 o</description>
    </item>
    
    <item>
      <title>tensorflow Session 学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-session-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sun, 20 Aug 2017 08:21:57 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-session-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>tensorflow-session官方文档 说下我自己的理解： session中文一般叫会话，可以理解成op执行时候需要的一层虚拟化的封装。 o</description>
    </item>
    
    <item>
      <title>使用tensorboard 生成 graph的方法</title>
      <link>https://111qqz.com/2017/08/%e4%bd%bf%e7%94%a8tensorboard-%e7%94%9f%e6%88%90-graph%e7%9a%84%e6%96%b9%e6%b3%95/</link>
      <pubDate>Fri, 18 Aug 2017 03:28:02 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/%e4%bd%bf%e7%94%a8tensorboard-%e7%94%9f%e6%88%90-graph%e7%9a%84%e6%96%b9%e6%b3%95/</guid>
      <description>tf的文档真的是&amp;hellip;其实就一句话啊orz 使用上还是多参考CS20SI比较好orz CS20SI-slides2 import tensorflow as tf a = tf.constant(2) b = tf.constant(3) x = tf.add(a, b) with tf.Session() as sess: # add this line to</description>
    </item>
    
    <item>
      <title>使用tensorboard 生成 graph的方法</title>
      <link>https://111qqz.com/2017/08/%e4%bd%bf%e7%94%a8tensorboard-%e7%94%9f%e6%88%90-graph%e7%9a%84%e6%96%b9%e6%b3%95/</link>
      <pubDate>Fri, 18 Aug 2017 03:28:02 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/%e4%bd%bf%e7%94%a8tensorboard-%e7%94%9f%e6%88%90-graph%e7%9a%84%e6%96%b9%e6%b3%95/</guid>
      <description>tf的文档真的是&amp;hellip;其实就一句话啊orz 使用上还是多参考CS20SI比较好orz CS20SI-slides2 import tensorflow as tf a = tf.constant(2) b = tf.constant(3) x = tf.add(a, b) with tf.Session() as sess: # add this line to</description>
    </item>
    
    <item>
      <title>Distributed Tensorflow : Cannot assign a device for operation save</title>
      <link>https://111qqz.com/2017/08/distributed-tensorflow-cannot-assign-a-device-for-operation-save/</link>
      <pubDate>Mon, 14 Aug 2017 01:55:15 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/distributed-tensorflow-cannot-assign-a-device-for-operation-save/</guid>
      <description>是在使用分布式tensorflow遇到的一个错误 报错如下： InvalidArgumentError (see above for traceback): Cannot assign a device for operation &amp;lsquo;save/Rest│| 2 GeForce GTX 1080 On | 0000:08:00.0 Off | N/A | oreV2_888&amp;rsquo;:</description>
    </item>
    
    <item>
      <title>Distributed Tensorflow : Cannot assign a device for operation save</title>
      <link>https://111qqz.com/2017/08/distributed-tensorflow-cannot-assign-a-device-for-operation-save/</link>
      <pubDate>Mon, 14 Aug 2017 01:55:15 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/distributed-tensorflow-cannot-assign-a-device-for-operation-save/</guid>
      <description>是在使用分布式tensorflow遇到的一个错误 报错如下： InvalidArgumentError (see above for traceback): Cannot assign a device for operation &amp;lsquo;save/Rest│| 2 GeForce GTX 1080 On | 0000:08:00.0 Off | N/A | oreV2_888&amp;rsquo;:</description>
    </item>
    
    <item>
      <title>分布式tensorflow 设备分配 学习笔记</title>
      <link>https://111qqz.com/2017/08/%e5%88%86%e5%b8%83%e5%bc%8ftensorflow-%e8%ae%be%e5%a4%87%e5%88%86%e9%85%8d-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0tf-train-replica_device_setter-and-more/</link>
      <pubDate>Fri, 11 Aug 2017 08:59:10 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/%e5%88%86%e5%b8%83%e5%bc%8ftensorflow-%e8%ae%be%e5%a4%87%e5%88%86%e9%85%8d-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0tf-train-replica_device_setter-and-more/</guid>
      <description>replica_device_setter_TF官网文档 由于看到这个函数很是激动。。。感觉这个东西也是蛮重要的。。之前完全手动分配累死辣。。</description>
    </item>
    
    <item>
      <title>分布式tensorflow 设备分配 学习笔记</title>
      <link>https://111qqz.com/2017/08/%e5%88%86%e5%b8%83%e5%bc%8ftensorflow-%e8%ae%be%e5%a4%87%e5%88%86%e9%85%8d-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0tf-train-replica_device_setter-and-more/</link>
      <pubDate>Fri, 11 Aug 2017 08:59:10 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/%e5%88%86%e5%b8%83%e5%bc%8ftensorflow-%e8%ae%be%e5%a4%87%e5%88%86%e9%85%8d-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0tf-train-replica_device_setter-and-more/</guid>
      <description>replica_device_setter_TF官网文档 由于看到这个函数很是激动。。。感觉这个东西也是蛮重要的。。之前完全手动分配累死辣。。</description>
    </item>
    
    <item>
      <title>分布式 tensorflow 学习笔记(非最终版)</title>
      <link>https://111qqz.com/2017/08/tensorflow%e5%bc%82%e6%ad%a5%e8%ae%ad%e7%bb%83%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Mon, 07 Aug 2017 12:54:23 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow%e5%bc%82%e6%ad%a5%e8%ae%ad%e7%bb%83%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>感觉资料不是很多，先收集资料好了。 tf-distributed官网文档 SO-between-graph和in-graph的区别 inception.README.md SyncReplicasOptimizer SO_How does ps work in</description>
    </item>
    
    <item>
      <title>分布式 tensorflow 学习笔记(非最终版)</title>
      <link>https://111qqz.com/2017/08/tensorflow%e5%bc%82%e6%ad%a5%e8%ae%ad%e7%bb%83%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Mon, 07 Aug 2017 12:54:23 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow%e5%bc%82%e6%ad%a5%e8%ae%ad%e7%bb%83%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>感觉资料不是很多，先收集资料好了。 tf-distributed官网文档 SO-between-graph和in-graph的区别 inception.README.md SyncReplicasOptimizer SO_How does ps work in</description>
    </item>
    
    <item>
      <title>tensorflow Supervisor 学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-supervisor-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Fri, 04 Aug 2017 09:22:44 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-supervisor-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>update:supervisor的缺点是遇到问题只会抛异常，所以现在有一个better的管理工具,MonitoredSession master,chief worker,Supervisor 这几</description>
    </item>
    
    <item>
      <title>tensorflow Supervisor 学习笔记</title>
      <link>https://111qqz.com/2017/08/tensorflow-supervisor-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Fri, 04 Aug 2017 09:22:44 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-supervisor-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>update:supervisor的缺点是遇到问题只会抛异常，所以现在有一个better的管理工具,MonitoredSession master,chief worker,Supervisor 这几</description>
    </item>
    
    <item>
      <title>TensorFlow Architecture 学习笔记（二）Adding a New Op</title>
      <link>https://111qqz.com/2017/08/tensorflow-architecture-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%88%e4%ba%8c%ef%bc%89adding-a-new-op/</link>
      <pubDate>Wed, 02 Aug 2017 03:07:37 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-architecture-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%88%e4%ba%8c%ef%bc%89adding-a-new-op/</guid>
      <description>Adding a New Op * [目录](https://www.tensorflow.org/extend/adding_an_op#top_of_page) *</description>
    </item>
    
    <item>
      <title>TensorFlow Architecture 学习笔记（二）Adding a New Op</title>
      <link>https://111qqz.com/2017/08/tensorflow-architecture-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%88%e4%ba%8c%ef%bc%89adding-a-new-op/</link>
      <pubDate>Wed, 02 Aug 2017 03:07:37 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-architecture-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%88%e4%ba%8c%ef%bc%89adding-a-new-op/</guid>
      <description>Adding a New Op * [目录](https://www.tensorflow.org/extend/adding_an_op#top_of_page) *</description>
    </item>
    
    <item>
      <title>TensorFlow Architecture 学习笔记（一）</title>
      <link>https://111qqz.com/2017/08/tensorflow-architecture-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%88%e4%b8%80%ef%bc%89/</link>
      <pubDate>Tue, 01 Aug 2017 03:01:12 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-architecture-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%88%e4%b8%80%ef%bc%89/</guid>
      <description>这篇文章不会涉及tensorflow的具体使用，而是专注于介绍tensorflow的架构，目的是让开发者能够对tensorflow现有框架进</description>
    </item>
    
    <item>
      <title>TensorFlow Architecture 学习笔记（一）</title>
      <link>https://111qqz.com/2017/08/tensorflow-architecture-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%88%e4%b8%80%ef%bc%89/</link>
      <pubDate>Tue, 01 Aug 2017 03:01:12 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/tensorflow-architecture-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%ef%bc%88%e4%b8%80%ef%bc%89/</guid>
      <description>这篇文章不会涉及tensorflow的具体使用，而是专注于介绍tensorflow的架构，目的是让开发者能够对tensorflow现有框架进</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory （LSTM） 网络 学习笔记</title>
      <link>https://111qqz.com/2017/07/lstm-notes/</link>
      <pubDate>Mon, 31 Jul 2017 10:05:01 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/lstm-notes/</guid>
      <description>参考资料： 维基百科_长短期记忆(LSTM) Understanding LSTM Networks [译] 理解 LSTM 网络 LSTM笔记 翻译的比较一般，建议看原文&amp;hellip;.比如cell还是不要</description>
    </item>
    
    <item>
      <title>Long Short-Term Memory （LSTM） 网络 学习笔记</title>
      <link>https://111qqz.com/2017/07/lstm-notes/</link>
      <pubDate>Mon, 31 Jul 2017 10:05:01 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/lstm-notes/</guid>
      <description>参考资料： 维基百科_长短期记忆(LSTM) Understanding LSTM Networks [译] 理解 LSTM 网络 LSTM笔记 翻译的比较一般，建议看原文&amp;hellip;.比如cell还是不要</description>
    </item>
    
    <item>
      <title>stanford cs 231n:常用激活函数</title>
      <link>https://111qqz.com/2017/07/stanford-cs-231n%e5%b8%b8%e7%94%a8%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0/</link>
      <pubDate>Sat, 22 Jul 2017 08:56:08 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/stanford-cs-231n%e5%b8%b8%e7%94%a8%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0/</guid>
      <description>其实我觉得这部分可以直接黑箱。。。直接无脑上Leaky ReLU或者Maxou？不过对这些激活函数的特点有个high-level的了解应该总是</description>
    </item>
    
    <item>
      <title>stanford cs 231n:常用激活函数</title>
      <link>https://111qqz.com/2017/07/stanford-cs-231n%e5%b8%b8%e7%94%a8%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0/</link>
      <pubDate>Sat, 22 Jul 2017 08:56:08 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/stanford-cs-231n%e5%b8%b8%e7%94%a8%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0/</guid>
      <description>其实我觉得这部分可以直接黑箱。。。直接无脑上Leaky ReLU或者Maxou？不过对这些激活函数的特点有个high-level的了解应该总是</description>
    </item>
    
    <item>
      <title>how to copy &amp; modify nets model on tensorflow slim</title>
      <link>https://111qqz.com/2017/07/how-to-copy-modify-nets-model-on-tensorflow-slim/</link>
      <pubDate>Wed, 19 Jul 2017 06:21:40 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/how-to-copy-modify-nets-model-on-tensorflow-slim/</guid>
      <description>想要修改tensorflow-slim 中 nets中的某个model,例如明明为kk_v2.py 观察到train_image_classifi</description>
    </item>
    
    <item>
      <title>how to copy &amp; modify nets model on tensorflow slim</title>
      <link>https://111qqz.com/2017/07/how-to-copy-modify-nets-model-on-tensorflow-slim/</link>
      <pubDate>Wed, 19 Jul 2017 06:21:40 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/how-to-copy-modify-nets-model-on-tensorflow-slim/</guid>
      <description>想要修改tensorflow-slim 中 nets中的某个model,例如明明为kk_v2.py 观察到train_image_classifi</description>
    </item>
    
    <item>
      <title>Inception-v4,Inception-ResNet 和残差连接对学习的影响</title>
      <link>https://111qqz.com/2017/07/inception-v4inception-resnet-%e5%92%8c%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e5%af%b9%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%bd%b1%e5%93%8d/</link>
      <pubDate>Tue, 18 Jul 2017 02:42:50 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/inception-v4inception-resnet-%e5%92%8c%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e5%af%b9%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%bd%b1%e5%93%8d/</guid>
      <description>原始论文 翻译链接 **——前言：**作者认为残差连接在训练深度卷积模型是很有必要的。至少在图像识别上，我们的研究似乎并不支持这一观点。 **摘要</description>
    </item>
    
    <item>
      <title>Inception-v4,Inception-ResNet 和残差连接对学习的影响</title>
      <link>https://111qqz.com/2017/07/inception-v4inception-resnet-%e5%92%8c%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e5%af%b9%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%bd%b1%e5%93%8d/</link>
      <pubDate>Tue, 18 Jul 2017 02:42:50 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/inception-v4inception-resnet-%e5%92%8c%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e5%af%b9%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%bd%b1%e5%93%8d/</guid>
      <description>原始论文 翻译链接 **——前言：**作者认为残差连接在训练深度卷积模型是很有必要的。至少在图像识别上，我们的研究似乎并不支持这一观点。 **摘要</description>
    </item>
    
    <item>
      <title>stanford CS231n notes：Linear classification</title>
      <link>https://111qqz.com/2017/07/cs231n_linear-classification/</link>
      <pubDate>Mon, 17 Jul 2017 02:02:43 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/cs231n_linear-classification/</guid>
      <description>课程链接 知乎翻译链接 之前看的原版，后来发现知乎上有翻译，正好想到之前看完没有整理总结，干脆就写一下自己的理解，顺便贴一下课程翻译（感觉翻译的</description>
    </item>
    
    <item>
      <title>stanford CS231n notes：Linear classification</title>
      <link>https://111qqz.com/2017/07/cs231n_linear-classification/</link>
      <pubDate>Mon, 17 Jul 2017 02:02:43 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/cs231n_linear-classification/</guid>
      <description>课程链接 知乎翻译链接 之前看的原版，后来发现知乎上有翻译，正好想到之前看完没有整理总结，干脆就写一下自己的理解，顺便贴一下课程翻译（感觉翻译的</description>
    </item>
    
    <item>
      <title>tensorflow slim 源码分析</title>
      <link>https://111qqz.com/2017/07/tensorflow-slim-%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90/</link>
      <pubDate>Sun, 16 Jul 2017 13:10:04 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/tensorflow-slim-%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90/</guid>
      <description>py的源码看起来还是很愉快的。。。（虽然熟练成程度完全不如cpp。。。。 datasets里是数据集相关 deployment是部署相关 nets</description>
    </item>
    
    <item>
      <title>tensorflow slim 源码分析</title>
      <link>https://111qqz.com/2017/07/tensorflow-slim-%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90/</link>
      <pubDate>Sun, 16 Jul 2017 13:10:04 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/tensorflow-slim-%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90/</guid>
      <description>py的源码看起来还是很愉快的。。。（虽然熟练成程度完全不如cpp。。。。 datasets里是数据集相关 deployment是部署相关 nets</description>
    </item>
    
    <item>
      <title>Deep Learning Tutorial - PCA and Whitening</title>
      <link>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</link>
      <pubDate>Thu, 06 Jul 2017 08:35:51 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</guid>
      <description>说下我自己的理解 PCA：主成分分析，是一种预处理手段。对于n维的数据，通过一些手段，把变化显著的k个维度保留，舍弃另外n-k个维度。对于一些</description>
    </item>
    
    <item>
      <title>Deep Learning Tutorial - PCA and Whitening</title>
      <link>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</link>
      <pubDate>Thu, 06 Jul 2017 08:35:51 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</guid>
      <description>说下我自己的理解 PCA：主成分分析，是一种预处理手段。对于n维的数据，通过一些手段，把变化显著的k个维度保留，舍弃另外n-k个维度。对于一些</description>
    </item>
    
    <item>
      <title>libgfortran.so.4 missing under archlinux</title>
      <link>https://111qqz.com/2017/06/libgfortran-so-4-missing-under-archlinux/</link>
      <pubDate>Fri, 09 Jun 2017 20:12:41 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/06/libgfortran-so-4-missing-under-archlinux/</guid>
      <description>。。。哭了哦。。终于解决了这个bug 参考资料： libgfortran broken? libgfortran=3.0 should not be install with numpy &amp;lt;= 1.9 [SOLVED] libgfortran.so.3:cannot open shared object file: No such file [Replacing gcc-libs-libs with gcc-multilib arch conflict with gcc-libs and gcc-libs-multilib on latest update 一开始以为是anaconda</description>
    </item>
    
    <item>
      <title>libgfortran.so.4 missing under archlinux</title>
      <link>https://111qqz.com/2017/06/libgfortran-so-4-missing-under-archlinux/</link>
      <pubDate>Fri, 09 Jun 2017 20:12:41 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/06/libgfortran-so-4-missing-under-archlinux/</guid>
      <description>。。。哭了哦。。终于解决了这个bug 参考资料： libgfortran broken? libgfortran=3.0 should not be install with numpy &amp;lt;= 1.9 [SOLVED] libgfortran.so.3:cannot open shared object file: No such file [Replacing gcc-libs-libs with gcc-multilib arch conflict with gcc-libs and gcc-libs-multilib on latest update 一开始以为是anaconda</description>
    </item>
    
  </channel>
</rss>