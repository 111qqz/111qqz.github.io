<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep-learning on 111qqz的小窝</title><link>https://111qqz.com/categories/deep-learning/</link><description>Recent content in deep-learning on 111qqz的小窝</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2012-2022 all rights reserved.</copyright><lastBuildDate>Mon, 05 Apr 2021 14:44:25 +0800</lastBuildDate><atom:link href="https://111qqz.com/categories/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>caffe 源码阅读笔记</title><link>https://111qqz.com/2020/06/caffe-notes/</link><pubDate>Tue, 30 Jun 2020 19:47:02 +0800</pubDate><guid>https://111qqz.com/2020/06/caffe-notes/</guid><description>
caffe做部署是YYDS!
blob
layer
net
激活函数
卷积
reshape
slice
loss function
reduce
eltwise
argmax</description></item><item><title>(CSE 599W)Reverse Mode Autodiff</title><link>https://111qqz.com/2021/04/reverse-mode-autodiff/</link><pubDate>Mon, 05 Apr 2021 14:44:25 +0800</pubDate><guid>https://111qqz.com/2021/04/reverse-mode-autodiff/</guid><description>
背景 怎么算微分。。通常有三种方法。
Symbolic Differentiation Numerical Differentiation Automatic Differentiation (auto diff)
auto diff中两种主流的方式分别是forward-mode和reverse-mode 由于forward-mode的方法中，计算的时间复杂度是O(n),n是输入的参数个数。而reverse-mode中，计算的时间复杂度是O(m),m是输出节点的个数。在dnn中，n往往很大，远大于m，因此这里主要介绍reverse-mode auto diff方法。
backprop和reverse mode auto diff的区别 看了reverse mode auto diff的过程，感觉和backprop是一回事呀。。。 实际上，backprop指的是训练神经网络根据loss的gradient来更新weight的过程，而auto diff是backprop使用的一个用来计算gradient的 technique.
Bakpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode auto diff is simply a technique used to compute gradients efficiently and it happens to be used by backpropagation.</description></item><item><title>【推荐系统】Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions</title><link>https://111qqz.com/2020/01/toward-the-next-gen-of-recom-sys/</link><pubDate>Sat, 23 Jan 2021 17:42:15 +0800</pubDate><guid>https://111qqz.com/2020/01/toward-the-next-gen-of-recom-sys/</guid><description>
迫于生计，从今天开始学习推荐系统相关的内容，今天先来读一篇推荐系统领域的综述 Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions
由于目前的工作其实是偏向推荐系统的serving,训练的开发，因此这些paper可能都是粗读，也不会把paper中的内容逐句翻译，而是找出我认为最为重要的一些概念加以记录。
INTRODUCTION 推荐的问题简单可以归纳成对user未看见的item进行打分的过程，这个分一般称之为rating.有了rating,推荐前top k 个最好的rating给用户即可。 推荐系统的预测内容有两种不同的类别，一种是预测绝对的rating，另外一种是预测一个user对不同item的相对喜好，称为“preference- based recommender systems”. 本文只讲前者，也就是预测具体的rating数值 根据使用的方法不同，推荐系统又分为三类: 基于内容的推荐：user会被推荐与他之前喜欢的item相似的item 基于协同过滤的推荐: user会被推荐其他和该user品味相近的user喜欢的item 将上述两种方法结合在一起使用 基于内容的方法( Content-Based Methods ) 常用在基于text内容的领域，可以用一些keyword来描述内容。具体做法是先拿到keyword的weight,然后基于这些weight来做推荐。计算keyword weight的方法中，比较有名的是 TF-IDF
TF-IDF tf–idf 是一个用来衡量一篇text中，每个keyword的重要性(或者叫weight)的方法。 从名字就可以看出，这个方法分为两部分。 tf和idf
tf是&amp;quot;term frequency&amp;quot;的缩写，表示的是某个keyword在一篇text里的频率，也就是出现的次数除以所有字词出现的次数 idf是&amp;quot;inverse document frequency&amp;quot;的缩写，衡量的是某个keyword在语料库里的普遍性。越普遍，改指越小（比如结构助词&amp;quot;的&amp;quot;，几乎在每一个text里都会出现，那么idf值就会很小)
tf和idf两部分都有一个公式来计算得到数值，tf-idf算法是将两个数值相乘起来，使得最终结果可以被这两部分影响。 tf-idf算法认为，某个keyword的tf值越高，idf越小，说明这个keyword对这篇text越重要。
直观地说,tf-idf的tf是保留高频重要词语，idf是将常见的词语去除的过程。
我们刚刚说tf-idf可以用来分析一个text(也就是item)中，每个Keyword的weight 那么实际上，tf-idf也可以用来分析对于一个user,每个keyword的weight 这样我们就得到了两个weight vector,分别表示每个keyword对一个user的重要性和每个keyword对一个item(text)的重要性
此时可以算一下两个向量的相似度，比如算个cos距离。 然后根据这个相似度进行推荐
缺点 非text-based的item不好提取feature top k keyword相同的两个item,不好区分 只会推荐之前打分过的item,使得推荐系统缺乏多样性 新用户的冷启动问题，推荐系统中没有新用户的喜好，导致无法做推荐 基于协同过滤的方法 找到和某个user taste类似的user group，然后将这个user group喜欢的item 推给这个user 根据使用的方法不同，通常分为两类:memory-based(or heuristic-based) and model-based。前者我更多的基于某个rule类做预测，后者是通过ML以及之后的DL方法来train 一个model,用这个model 做预测 由于可能需要计算任意两个user的相似度来判断taste,很多sys会先将任意两个user的相似度预处理出来。 虽然协同过滤的方法最初是用于计算user的相似度来做推荐，但是后面也有基于item的相似度来做推荐的方法，通常被写作&amp;quot;user-based CF&amp;quot;和&amp;quot;item-based CF&amp;quot; 优点 能够推荐给user他没见过，但是和他品味类似的user group喜欢的item 由于是基于ratings算相似度，所以适用于任何形态的内容（而不仅仅是text-based content，说白了就是当时其他形态的内容不容易拿到feature) 缺点 new user problem: 不知道新用户的喜好,不知道哪些才是与new user taste 相似的user new item problem: 新item没有被足够的用户打分，很难被推荐给其他user Sparsity: 协同过滤依赖于大量的user.</description></item><item><title>【施工中】torch2trt　学习笔记</title><link>https://111qqz.com/2020/09/torch2trt/</link><pubDate>Fri, 18 Sep 2020 11:02:50 +0800</pubDate><guid>https://111qqz.com/2020/09/torch2trt/</guid><description>
前言 偶然发现了 torch2trt 的模型转换方案，思路是直接将pytorch op映射到TensorRT的python api. 在pytorch进行每个op　forward的时候，tensorrt也相应往network上添加op. 这里会先涉及torch2trt的使用，后面会补充这个转换工具的代码学习
使用torch2trt torch2trt pytorch可以直接安装，但是torchvision根据 pytorch-for-jetson-version-1-6-0-now-available 中的说法，需要编译安装
1git clone https://github.com/pytorch/vision 然后切换到tag v0.7.0 执行
1sudo python3 setup.py install 可以正常安装
然后报错: ModuleNotFoundError: No module named 'termcolor'
尝试 sudo apt install python3-termcolor 后解决
接下来尝试trt samples中的　/python/network_api_pytorch_mnist 发现安装pycuda报错找不到cuda.h的头文件 参考　pycuda installation failure on jetson nano 执行了如下命令后成功:
1export LIBRARY_PATH=/usr/local/cuda/targets/aarch64-linux/lib/:$LIBRARY_PATH 2export CPATH=/usr/local/cuda/include:$CPATH 3 然后尝试使用TensorRT python api对一个engine file 做inference的时候，报错: pycuda._driver.LogicError: explicit_context_dependent failed: invalid device context - no currently active context? 在　import pycuda.</description></item><item><title>Jetson Nano踩坑记录</title><link>https://111qqz.com/2020/09/jetson-nano/</link><pubDate>Tue, 08 Sep 2020 14:41:34 +0800</pubDate><guid>https://111qqz.com/2020/09/jetson-nano/</guid><description>
写在前面 主要是需要在jetson nano做模型转换，来记录下踩的坑 目前有两条路径，一条是我们现有的转换路径，也就是pytorch-&amp;gt;onnx(-&amp;gt;caffe)-&amp;gt;trt的路径 在这条路径上踩了比较多的坑，最终暂时放弃，最直接的原因是cudnn8.0升级接口发生改动，编译caffe遇到较多问题 这里其实仍然采用了两条平行的路径，一条是直接在nano上构建环境，另外一种是基于docker(包括构建交叉编译环境用于加快编译速度)
另一条路径是基于torch2trt,是一条直接pytorch-&amp;gt;trt的路径 这里主要记录在第一条路径上踩过的坑
环境准备 先过一遍开发者手册 主要是介绍了下nano的硬件和jetpack的组件 jetpack可以理解成一个nvidia用在嵌入式设备上的SDK工具包
镜像烧录 然后需要烧录镜像，参考Write Image to the microSD Card 最初使用 Etcher 来烧录，没有启动起来，原因未知。使用终端命令烧录就没问题了。
更换国内apt源 我拿到的jeston nano是基于ubuntu 18.04 lts的版本，默认源比较慢，换成国内源。需要注意要换arm版本的源
1wget -O /etc/apt/sources.list https://repo.huaweicloud.com/repository/conf/Ubuntu-Ports-bionic.list 2apt update 基于docker的模型转换 由于在x86上模型转换的工具是基于docker的，因此最初的想法是在nano上也基于docker进行转模型，这样或许可以复用一些x86上的工作。
确认了Jetson Nano上是可以跑docker和NVIDIA Container Runtime on Jetson (Beta)的 但是最初发现arm的cuda docker image最低支持cuda11.0的版本 cuda-arm64 docker image tag 然而目前(2020年9月)最新的jetpack4.4版本只支持到cuda10.2的版本 尝试了下使用cuda-arm64的docker image　发现驱动版本不够，而nano上似乎很难升级驱动版本，因此差点就放弃这条路径了。
然而发现，在nano上要用的cuda镜像并不应该是cuda-arm,而是NVIDIA L4T Base
结论 参考https://docs.nvidia.com/jetson/archives/，发现一个可能的问题是，jetpack中cuDNN,TensorRT等库的版本不是连续的。
jetpack4.4: CUDA10.2,TensorRT 7.1.3,cuDNN 8.0.0 jetpack4.3: CUDA 10.0.326, TensorRT 6.0.1.10, cuDNN 7.</description></item><item><title>caffe 源码学习笔记(11) argmax layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part11/</link><pubDate>Wed, 06 May 2020 21:26:03 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part11/</guid><description>
背景 似乎没什么背景,继续看caffe代码
argmax的作用是返回一个blob某个维度或者batch_size之后的维度的top_k的index(或者pair(index,value))
proto 还是先看proto
12message ArgMaxParameter {3 // If true produce pairs (argmax, maxval) 4 optional bool out_max_val = 1 [default = false];5 optional uint32 top_k = 2 [default = 1];6 // The axis along which to maximise -- may be negative to index from the 7 // end (e.g., -1 for the last axis). 8 // By default ArgMaxLayer maximizes over the flattened trailing dimensions 9 // for each index of the first / num dimension.</description></item><item><title>caffe 源码学习笔记(10) eltwise layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part10/</link><pubDate>Sun, 03 May 2020 17:53:22 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part10/</guid><description>
背景 这个layer和reduce layer有一些相似,就干脆一起看了. 作用是输入至少两个blob,然后对每个blob中的元素所一些运算,最后得到一个blob.
caffe 支持的运算有&amp;quot;PROD&amp;quot;,&amp;quot;SUM&amp;quot;,&amp;quot;MAX&amp;quot;三种
顺便提一句,TensorRT支持的要多一些:
1 2enum class ElementWiseOperation : int 3{ 4 kSUM = 0, //!&amp;lt; Sum of the two elements. 5 kPROD = 1, //!&amp;lt; Product of the two elements. 6 kMAX = 2, //!&amp;lt; Maximum of the two elements. 7 kMIN = 3, //!&amp;lt; Minimum of the two elements. 8 kSUB = 4, //!&amp;lt; Substract the second element from the first. 9 kDIV = 5, //!</description></item><item><title>caffe 源码学习笔记(9) reduce layer</title><link>https://111qqz.com/2020/05/caffe-source-code-analysis-part9/</link><pubDate>Sun, 03 May 2020 15:16:53 +0800</pubDate><guid>https://111qqz.com/2020/05/caffe-source-code-analysis-part9/</guid><description>
背景 其实没什么背景,继续啃caffe代码而已2333
reduce layer其实就是做reduce操作,把一个任意shape的blob通过某种运算变成一个scalar.
caffe目前支持求和(SUM),绝对值的和(ASUM),平方和(SUMSQ),以及对得到的scalar的总数求平均的求和(MEAN).
说句题外话,TensorRT支持的操作是求和,求积,max,min和ave. 还是有一些gap的
proto 先看proto
12message ReductionParameter {3 enum ReductionOp {4 SUM = 1;5 ASUM = 2;6 SUMSQ = 3;7 MEAN = 4;8 }910 optional ReductionOp operation = 1 [default = SUM]; // reduction operation 1112 // The first axis to reduce to a scalar -- may be negative to index from the 13 // end (e.g., -1 for the last axis). 14 // (Currently, only reduction along ALL &amp;#34;tail&amp;#34; axes is supported; reduction 15 // of axis M through N, where N &amp;lt; num_axes - 1, is unsupported.</description></item><item><title>Focal Loss for Dense Object Detection(RetinaNet) 学习笔记</title><link>https://111qqz.com/2020/05/retinanet-notes/</link><pubDate>Sat, 02 May 2020 18:50:06 +0800</pubDate><guid>https://111qqz.com/2020/05/retinanet-notes/</guid><description>
先写个简略版的笔记..看之后的情况要不要读得更精细一点..
背景 two stage的检测比one stage的检测效果好,原因是啥?
作者认为是正负样本不平衡导致的. two stage的方法在proposal 的时候干掉了大部分负样本,所以效果好.
因为作者提出了一种新的loss,称为Focal Loss 是对交叉熵loss的改进,作用是提高没有正确分类的样本的权重,降低正确分类的样本的权重.
然后设计了个retinaNet 来验证效果. 主要是用了Focal Loss 作为损失函数,以及backbone比起之前的one stage的检测用上了FPN.
Focal Loss 一图胜千言
Focal loss是在交叉熵loss的基础上增加了一个指数衰减. 对正确分类的样本影响很小,对错误分类的样本影响很大.
从图上我们可以看出,CE对正确分类的样本仍然有不小的loss,这样的样本数很多的话,就会被训练的时候模型被带歪... 因此需要减少这些正确分类的样本对loss的影响.
RetinaNet 单阶段检测,主要在于使用Focal Loss训练,以及backbone用上了FPN</description></item><item><title>caffe 源码学习笔记(8) loss function</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part8/</link><pubDate>Sat, 18 Apr 2020 18:33:29 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part8/</guid><description>
背景 虽然不太care 训练的过程，但是由于容易看懂的layer都看得差不多了 所以打算看一下这些loss function.
Euclidean Loss (L2 loss) 一般用于“real-valued regression tasks” 。　比如之前的项目上用的人脸年龄模型，就是用了这个Loss
这个loss没什么额外的参数，实现也很简单。
1 2template &amp;lt;typename Dtype&amp;gt; 3void EuclideanLossLayer&amp;lt;Dtype&amp;gt;::Reshape( 4 const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom, const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) { 5 LossLayer&amp;lt;Dtype&amp;gt;::Reshape(bottom, top); 6 CHECK_EQ(bottom[0]-&amp;gt;count(1), bottom[1]-&amp;gt;count(1)) 7 &amp;lt;&amp;lt; &amp;#34;Inputs must have the same dimension.&amp;#34;; 8 diff_.ReshapeLike(*bottom[0]); 9} 10 11template &amp;lt;typename Dtype&amp;gt; 12void EuclideanLossLayer&amp;lt;Dtype&amp;gt;::Forward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom, 13 const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) { 14 int count = bottom[0]-&amp;gt;count(); 15 caffe_sub( 16 count, 17 bottom[0]-&amp;gt;cpu_data(), 18 bottom[1]-&amp;gt;cpu_data(), 19 diff_.</description></item><item><title>caffe 源码学习笔记(7) slice layer</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part7/</link><pubDate>Mon, 13 Apr 2020 21:22:54 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part7/</guid><description>
背景　 ocr组那边有个shuffle net 的网络,里面有个pytorch op叫chunk,转成的onnx对应的op是 split
作用是:
Split a tensor into a list of tensors, along the specified 'axis'. Lengths of the parts can be specified using argument 'split'. Otherwise, the tensor is split to equal sized parts.
然后发现这个op模型转换里不支持转到caffe的layer,于是想办法支持了一下. 发现是要转到caffe的slice layer.(caffe也有一个split layer,但是这个split layer是除了一个输出blob作为多个layer的输入时用的)
proto 12message SliceParameter {3 // The axis along which to slice -- may be negative to index from the end 4 // (e.g., -1 for the last axis).</description></item><item><title>caffe 源码学习笔记(6) reshape layer</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part6/</link><pubDate>Thu, 09 Apr 2020 21:03:06 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part6/</guid><description>
背景　 最近在魔改 tensorRT 的caffe parser 之前caffe模型转到trt模型时，有一个修改是需要将reshape　layer的param末尾补1,比较繁琐，于是看了下caffe的reshape layer的实现．
proto 12message ReshapeParameter {3 // Specify the output dimensions. If some of the dimensions are set to 0, 4 // the corresponding dimension from the bottom layer is used (unchanged). 5 // Exactly one dimension may be set to -1, in which case its value is 6 // inferred from the count of the bottom blob and the remaining dimensions. 7 // For example, suppose we want to reshape a 2D blob &amp;#34;input&amp;#34; with shape 2 x 8: 8 // 9 // layer { 10 // type: &amp;#34;Reshape&amp;#34; bottom: &amp;#34;input&amp;#34; top: &amp;#34;output&amp;#34; 11 // reshape_param { .</description></item><item><title>caffe 源码学习笔记(5) 卷积</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part5/</link><pubDate>Wed, 08 Apr 2020 20:29:37 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part5/</guid><description>
caffe中卷积运算的实现 暴力实现的卷积大概是这样子的
1 2for w in 1..W 3 for h in 1..H 4 for x in 1..K 5 for y in 1..K 6 for m in 1..M 7 for d in 1..D 8 output(w, h, m) += input(w+x, h+y, d) * filter(m, x, y, d) 9 end 10 end 11 end 12 end 13 end 14end 15 这种方式的效率显然很低，不意外地,caffe中并不是这样实现的．
注释里面说:
Caffe convolves by reduction to matrix multiplication. This achieves high-throughput and generality of input and filter dimensions but comes at the cost of memory for matrices.</description></item><item><title>tensorrt INT8 量化debug记录（cuda error 700）</title><link>https://111qqz.com/2020/04/cuda-error-700-when-using-tensorrt-calibration/</link><pubDate>Wed, 08 Apr 2020 14:58:16 +0800</pubDate><guid>https://111qqz.com/2020/04/cuda-error-700-when-using-tensorrt-calibration/</guid><description>
背景是要把某个caffe model,转换成tensorrt的INT8 模型。 然后遇到如下报错:
1 2E0403 08:54:35.951987 5704 engine.h:62] engine.cpp (572) - Cuda Error in commonEmitTensor: 1 (invalid argument) 3E0403 08:54:35.952157 5704 engine.h:62] Failure while trying to emit debug blob. 4engine.cpp (572) - Cuda Error in commonEmitTensor: 1 (invalid argument) 5E0403 08:54:35.952235 5704 engine.h:62] cuda/caskConvolutionLayer.cpp (355) - Cuda Error in execute: 1 (invalid argument) 6E0403 08:54:35.952291 5704 engine.h:62] cuda/caskConvolutionLayer.cpp (355) - Cuda Error in execute: 1 (invalid argument) 7W0403 08:54:35.952324 5704 calibrator.</description></item><item><title>caffe 源码学习笔记(4) 激活函数</title><link>https://111qqz.com/2020/04/caffe-source-code-analysis-part4/</link><pubDate>Tue, 07 Apr 2020 23:21:40 +0800</pubDate><guid>https://111qqz.com/2020/04/caffe-source-code-analysis-part4/</guid><description>
在看过caffe代码的三个核心部分,blob,layer,net之后，陷入了不知道以什么顺序继续看的困境。
blob,layer,net只是三个最基本的概念，关键还是在于各个layer. 但是layer这么多，要怎么看呢？ 想了一下决定把相同作用的layer放在一起分析。 今天打算先分析一下激活函数。
sigmoid 表达式为 f(t) = 1/(1+e^-t)
caffe GPU实现，非常直接
1 2template &amp;lt;typename Dtype&amp;gt; 3__global__ void SigmoidForward(const int n, const Dtype* in, Dtype* out) { 4 CUDA_KERNEL_LOOP(index, n) { 5 out[index] = 1. / (1. + exp(-in[index])); 6 } 7} 8 sigmoid激活函数的一大优点是求导非常容易，因此backward函数其实也很简单。
1 2template &amp;lt;typename Dtype&amp;gt; 3__global__ void SigmoidBackward(const int n, const Dtype* in_diff, 4 const Dtype* out_data, Dtype* out_diff) { 5 CUDA_KERNEL_LOOP(index, n) { 6 const Dtype sigmoid_x = out_data[index]; 7 out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x); 8 } 9} 10 然后proto里面也没什么内容。因为sigmoid函数没什么参数</description></item><item><title>Faster Rcnn 目标检测算法</title><link>https://111qqz.com/2020/04/faster-rcnn/</link><pubDate>Sun, 05 Apr 2020 20:38:28 +0800</pubDate><guid>https://111qqz.com/2020/04/faster-rcnn/</guid><description>
背景 2019年对了好几次faster rcnn，第一次是赛事之窗项目和北京的同事，对齐sdk和训练的实现。 第二次是被tensorRT4和tensorRT5之间默认参数不一致的问题坑了一下。 第三次是被caffe proto中roi align 的默认参数坑了。
虽然debug了这么多次，踩了一堆坑，但是一段时间不用，细节就会慢慢不记得了。因此来记录一下。
faster rcnn，是一种&amp;quot;two stage&amp;quot;的目标检测算法。
所谓&amp;quot;two stage&amp;quot;，是说在实际进行目标检测之前，先会通过某种&amp;quot;region proposals&amp;quot; algorithm，来获得一定数量的RoI(Regions of Interest),我们下一阶段要检测的obejct有极大可能被包含在这些RoI. 这种&amp;quot;Region based&amp;quot;的方法是对基于&amp;quot;sliding windows&amp;quot;方法的极大改进，因为不需要遍历每一个可能的位置以及crop大小，只需要对这些RoI进行检测，有效地减小了计算量。
下面简单说一下这一类&amp;quot;Region based&amp;quot;方法的历史脉络
rcnn RCNN的做法是通过一种传统方法&amp;quot;selective search&amp;quot;来得到若干RoI,然后把每一个RoI，后面接CNN进行后续的检测。 显然，这个方法的问题在于计算量非常大。
selective search的策略是，既然是不知道尺度是怎样的，那我们就尽可能遍历所有的尺度，但是不同于暴力穷举，我们可以先得到小尺度的区域，然后一次次合并得到大的尺寸.
fast rcnn 明眼人可以看出，rcnn计算量过大的原因之一是做了非常多的重复计算。
因此fast rcnn做的改进是，与其把每一个通过&amp;quot;selective search&amp;quot;得到的RoI在原图上crop出来送进CNN,不如先让整张图过一段CNN,然后把通过&amp;quot;selective search&amp;quot;在原图上得到的RoI先映射到这段CNN的某个conv feature map. 相当于这部分CNN只做了一次，与RoI数量无关，极大地减小了计算量。
faster rcnn 终于轮到主角登场了。 fast rcnn极大提高了检测的速度。 然后发现，速度的瓶颈已经不在后续的检测部分了，而是在于“region proposals” algorithm.
于是，faster-rcnn提出&amp;quot;Region proposal network&amp;quot;来替代&amp;quot;selective search&amp;quot;,进一步提高了检测速度。
放一张结构图，非常清楚。 Region proposal network(RPN) anchor 介绍RPN网络首先就要介绍一下anchor.
（被坑过一次，某个足球项目上，training和inference用的anchor竟然是不一致的。。）
其实anchor这个概念很简单，用大白话说就是，根据要检测的物体的形状（高矮胖瘦等），预先 设置一些不同尺寸（高矮胖瘦）的粗略的框，然后对这些框做一个二分类，判断前景还是背景，同时做bbox regression 来微调坐标，最终得到proposals.
设置anchor的思路其实就是修改了proposals的默认位置为生成的anchors的位置。对这些anchors进行微调总要比从零开始生成容易得多。
要注意的是，anchors是在进入网络前预先生成的。 实际项目中，通常设置长宽比为[1:1,2:1,1:2]三种比例，然后通过 generate_anchors.py 来生成anchors.</description></item><item><title>resnet 学习笔记</title><link>https://111qqz.com/2020/04/resnet-learning-notes/</link><pubDate>Sun, 05 Apr 2020 16:49:44 +0800</pubDate><guid>https://111qqz.com/2020/04/resnet-learning-notes/</guid><description>
背景 基于Conv的方法在某年的ImageNet比赛上又重新被人想起之后，大家发现网络堆叠得越深，似乎在cv的各个任务上表现的越好。
然而事情当然没有无脑退跌深度那么简单，人们发现，当网络深到一定程度时，结果还不如浅一些的网络结构。
可能第一反应是，网路那么深，多了那么多参数，有那么多数据吗？ overfit了吧
然而情况没有那么简单。如果只是单纯得overfit，那么应该只有test error很高才对。然而现在的情况是training error也很高。
那这是怎么回事呢？ Resnet的团队认为，是因为深层的网络在训练的时候很难收敛。
这个想法是有依据的，因为我们可以通过构造一个较深的网络结构，使得后面的layer学成一个&amp;quot;identity mapping&amp;quot;的函数。这样training error和test error应该至少和一个浅层网络的结果一样好才对。
那么问题很可能就出在，深层的网络没办法学到这样的函数。
基于这样的想法，resnet团队提出了一种新的结构，称之为&amp;quot;skip connection&amp;quot;,来验证该假设。
resnet网络结构 我们可以看到，该结构把原来网络要学的H(x)，变成了F(X)+X的形势。 因此网络只需要学习F(X),也就是在 &amp;quot;identity mapping&amp;quot;上学习一个偏移。
实验表明，这种结构对于深层的网络是非常有效的，因为这种结构将默认设置变为了&amp;quot;identity mapping&amp;quot;,整个网络变得更加容易收敛。
resnet也成了目前工业界各种网络结构的标准backbone
resnet 结构的caffe prototxt 放了resnet50的部分结构，截止到第一个resnet block
12name: &amp;#34;ResNet-50&amp;#34;3input: &amp;#34;data&amp;#34;4input_dim: 15input_dim: 36input_dim: 2247input_dim: 22489layer {10 bottom: &amp;#34;data&amp;#34;11 top: &amp;#34;conv1&amp;#34;12 name: &amp;#34;conv1&amp;#34;13 type: &amp;#34;Convolution&amp;#34;14 convolution_param {15 num_output: 6416 kernel_size: 717 pad: 318 stride: 219 }20}2122layer {23 bottom: &amp;#34;conv1&amp;#34;24 top: &amp;#34;conv1&amp;#34;25 name: &amp;#34;bn_conv1&amp;#34;26 type: &amp;#34;BatchNorm&amp;#34;27 batch_norm_param {28 use_global_stats: true29 }30}3132layer {33 bottom: &amp;#34;conv1&amp;#34;34 top: &amp;#34;conv1&amp;#34;35 name: &amp;#34;scale_conv1&amp;#34;36 type: &amp;#34;Scale&amp;#34;37 scale_param {38 bias_term: true39 }40}4142layer {43 bottom: &amp;#34;conv1&amp;#34;44 top: &amp;#34;conv1&amp;#34;45 name: &amp;#34;conv1_relu&amp;#34;46 type: &amp;#34;ReLU&amp;#34;47}4849layer {50 bottom: &amp;#34;conv1&amp;#34;51 top: &amp;#34;pool1&amp;#34;52 name: &amp;#34;pool1&amp;#34;53 type: &amp;#34;Pooling&amp;#34;54 pooling_param {55 kernel_size: 356 stride: 257 pool: MAX58 }59}6061layer {62 bottom: &amp;#34;pool1&amp;#34;63 top: &amp;#34;res2a_branch1&amp;#34;64 name: &amp;#34;res2a_branch1&amp;#34;65 type: &amp;#34;Convolution&amp;#34;66 convolution_param {67 num_output: 25668 kernel_size: 169 pad: 070 stride: 171 bias_term: false72 }73}7475layer {76 bottom: &amp;#34;res2a_branch1&amp;#34;77 top: &amp;#34;res2a_branch1&amp;#34;78 name: &amp;#34;bn2a_branch1&amp;#34;79 type: &amp;#34;BatchNorm&amp;#34;80 batch_norm_param {81 use_global_stats: true82 }83}8485layer {86 bottom: &amp;#34;res2a_branch1&amp;#34;87 top: &amp;#34;res2a_branch1&amp;#34;88 name: &amp;#34;scale2a_branch1&amp;#34;89 type: &amp;#34;Scale&amp;#34;90 scale_param {91 bias_term: true92 }93}9495layer {96 bottom: &amp;#34;pool1&amp;#34;97 top: &amp;#34;res2a_branch2a&amp;#34;98 name: &amp;#34;res2a_branch2a&amp;#34;99 type: &amp;#34;Convolution&amp;#34;100 convolution_param {101 num_output: 64102 kernel_size: 1103 pad: 0104 stride: 1105 bias_term: false106 }107}108109layer {110 bottom: &amp;#34;res2a_branch2a&amp;#34;111 top: &amp;#34;res2a_branch2a&amp;#34;112 name: &amp;#34;bn2a_branch2a&amp;#34;113 type: &amp;#34;BatchNorm&amp;#34;114 batch_norm_param {115 use_global_stats: true116 }117}118119layer {120 bottom: &amp;#34;res2a_branch2a&amp;#34;121 top: &amp;#34;res2a_branch2a&amp;#34;122 name: &amp;#34;scale2a_branch2a&amp;#34;123 type: &amp;#34;Scale&amp;#34;124 scale_param {125 bias_term: true126 }127}128129layer {130 bottom: &amp;#34;res2a_branch2a&amp;#34;131 top: &amp;#34;res2a_branch2a&amp;#34;132 name: &amp;#34;res2a_branch2a_relu&amp;#34;133 type: &amp;#34;ReLU&amp;#34;134}135136layer {137 bottom: &amp;#34;res2a_branch2a&amp;#34;138 top: &amp;#34;res2a_branch2b&amp;#34;139 name: &amp;#34;res2a_branch2b&amp;#34;140 type: &amp;#34;Convolution&amp;#34;141 convolution_param {142 num_output: 64143 kernel_size: 3144 pad: 1145 stride: 1146 bias_term: false147 }148}149150layer {151 bottom: &amp;#34;res2a_branch2b&amp;#34;152 top: &amp;#34;res2a_branch2b&amp;#34;153 name: &amp;#34;bn2a_branch2b&amp;#34;154 type: &amp;#34;BatchNorm&amp;#34;155 batch_norm_param {156 use_global_stats: true157 }158}159160layer {161 bottom: &amp;#34;res2a_branch2b&amp;#34;162 top: &amp;#34;res2a_branch2b&amp;#34;163 name: &amp;#34;scale2a_branch2b&amp;#34;164 type: &amp;#34;Scale&amp;#34;165 scale_param {166 bias_term: true167 }168}169170layer {171 bottom: &amp;#34;res2a_branch2b&amp;#34;172 top: &amp;#34;res2a_branch2b&amp;#34;173 name: &amp;#34;res2a_branch2b_relu&amp;#34;174 type: &amp;#34;ReLU&amp;#34;175}176177layer {178 bottom: &amp;#34;res2a_branch2b&amp;#34;179 top: &amp;#34;res2a_branch2c&amp;#34;180 name: &amp;#34;res2a_branch2c&amp;#34;181 type: &amp;#34;Convolution&amp;#34;182 convolution_param {183 num_output: 256184 kernel_size: 1185 pad: 0186 stride: 1187 bias_term: false188 }189}190191layer {192 bottom: &amp;#34;res2a_branch2c&amp;#34;193 top: &amp;#34;res2a_branch2c&amp;#34;194 name: &amp;#34;bn2a_branch2c&amp;#34;195 type: &amp;#34;BatchNorm&amp;#34;196 batch_norm_param {197 use_global_stats: true198 }199}200201layer {202 bottom: &amp;#34;res2a_branch2c&amp;#34;203 top: &amp;#34;res2a_branch2c&amp;#34;204 name: &amp;#34;scale2a_branch2c&amp;#34;205 type: &amp;#34;Scale&amp;#34;206 scale_param {207 bias_term: true208 }209}210211layer {212 bottom: &amp;#34;res2a_branch1&amp;#34;213 bottom: &amp;#34;res2a_branch2c&amp;#34;214 top: &amp;#34;res2a&amp;#34;215 name: &amp;#34;res2a&amp;#34;216 type: &amp;#34;Eltwise&amp;#34;217}218219layer {220 bottom: &amp;#34;res2a&amp;#34;221 top: &amp;#34;res2a&amp;#34;222 name: &amp;#34;res2a_relu&amp;#34;223 type: &amp;#34;ReLU&amp;#34;224}可视化的结构为: 重点要关注的是 res2a 这个layer，把两个分支的结果直接加在了一起。 eltwise的layer是按照元素操作的，支持乘积，相加，或者取最大值，默认是相加。 可以参考caffe的proto文件</description></item><item><title>caffe 源码学习笔记(3) Net</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part3/</link><pubDate>Sun, 12 Jan 2020 19:18:15 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part3/</guid><description>
Net 基本介绍 网络通过组成和自微分共同定义一个函数及其梯度。
网络是一些Layer组成的DAG,也就是有向无环图，在caffe中通常由prototxt定义．
比如
1name: &amp;#34;LogReg&amp;#34;2layer {3 name: &amp;#34;mnist&amp;#34;4 type: &amp;#34;Data&amp;#34;5 top: &amp;#34;data&amp;#34;6 top: &amp;#34;label&amp;#34;7 data_param {8 source: &amp;#34;input_leveldb&amp;#34;9 batch_size: 6410 }11}12layer {13 name: &amp;#34;ip&amp;#34;14 type: &amp;#34;InnerProduct&amp;#34;15 bottom: &amp;#34;data&amp;#34;16 top: &amp;#34;ip&amp;#34;17 inner_product_param {18 num_output: 219 }20}21layer {22 name: &amp;#34;loss&amp;#34;23 type: &amp;#34;SoftmaxWithLoss&amp;#34;24 bottom: &amp;#34;ip&amp;#34;25 bottom: &amp;#34;label&amp;#34;26 top: &amp;#34;loss&amp;#34;27}定义了
值得强调的是，caffe中网络结构的定义与实现是无关的,这点比pytorch之类的深度学习框架不知高到哪里去了，也是caffe作为部署端的事实标准的重要原因．
Net 实现细节 我们先看下NetParameter的proto
12 message NetParameter {3 optional string name = 1; // consider giving the network a name 4 // DEPRECATED.</description></item><item><title>caffe 源码学习笔记(2) Layer</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part2/</link><pubDate>Sat, 11 Jan 2020 17:47:05 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part2/</guid><description>
layer 整体介绍 layer是模型计算的基本单元 类似于pytorch或者其他深度学习框架的op layer中的数据流向为,输入若干个blob，称之为&amp;quot;bottom blob&amp;quot;,然后经过layer的计算，输出若干个blob,称之为&amp;quot;top blob&amp;quot;
也就是数据是从“bottom”流向“top”
layer通常会进行两种计算，forward和backward
forward是指，根据bottom blob计算得到top blob backward是指，根据top blob的结果和参数值计算得到的gradient,回传给前面的layer.
layer 实现细节 layer作为一个base class,实现了所有layer都需要的common的部分。 比如:
1 /** 2* @brief Implements common layer setup functionality. 3* 4* @param bottom the preshaped input blobs 5* @param top 6* the allocated but unshaped output blobs, to be shaped by Reshape 7* 8* Checks that the number of bottom and top blobs is correct. 9* Calls LayerSetUp to do special layer setup for individual layer types, 10* followed by Reshape to set up sizes of top blobs and internal buffers.</description></item><item><title>caffe 源码学习笔记(1) Blob</title><link>https://111qqz.com/2020/01/caffe-source-code-analysis-part1/</link><pubDate>Fri, 10 Jan 2020 11:24:23 +0800</pubDate><guid>https://111qqz.com/2020/01/caffe-source-code-analysis-part1/</guid><description>
迫于生计，开始看caffe代码。 会侧重于分析inference部分。
blob 整体介绍 blob的含义及目的 blob在逻辑上表示的就是所谓的tensor,blob是tensor在caffe中的叫法。 在框架层面上，blob的意义在于对数据进行封装，提供统一的接口。 这里的数据包含训练/inference时用的数据，也包含模型参数，导数等数据。 深度学习离不开在GPU上的计算。 blob对数据的封装使得用户不必关心和cuda有关的数据传输细节。
blob的表示 对于图像数据，blob通常为４-dim，也就是N*C*H*W 其中N表示number,也就是batch_num C表示channel H表示height W表示width
在内存中，blob是按照&amp;quot;C-contiguous fashion&amp;quot;存储的，也就是&amp;quot;row-major &amp;quot;
因此，位于(n,c,h,w)的下标在OS中的位置是　((n * C + c) * H + h) * W + w.
在代码blob.hpp中，我们也可以看到名为offset的函数是其对应的实现。
1 inline int offset(const int n, const int c = 0, const int h = 0, 2 const int w = 0) const { 3 CHECK_GE(n, 0); 4 CHECK_LE(n, num()); 5 CHECK_GE(channels(), 0); 6 CHECK_LE(c, channels()); 7 CHECK_GE(height(), 0); 8 CHECK_LE(h, height()); 9 CHECK_GE(width(), 0); 10 CHECK_LE(w, width()); 11 return ((n * channels() + c) * height() + h) * width() + w; 12 } 13 // 给一个indices,计算这是第几个值。 14 inline int offset(const vector&amp;lt;int&amp;gt;&amp;amp; indices) const { 15 CHECK_LE(indices.</description></item><item><title>记一次faster-rcnn debug记录</title><link>https://111qqz.com/2019/12/debug-faster-rcnn-once-again/</link><pubDate>Fri, 13 Dec 2019 16:32:14 +0800</pubDate><guid>https://111qqz.com/2019/12/debug-faster-rcnn-once-again/</guid><description>
问题描述 一年debug 三次faster rcnn,每次都有新感觉（不
接到一个bug report,现象为某人脸模型，转换成trt模型，当batch size为1时结果完全正确，但是batch size大于1时结果不正确。 具体的现象是，如果跑多张不同的图，只有第一张图有结果，后面的图都没有结果。 如果跑的图中有相同的，那么和第一张相同的图都会有结果，其余的图没有结果。
1layer {2 name: &amp;#34;POD_proposal&amp;#34;3 type: &amp;#34;RPRoIFused&amp;#34;4 bottom: &amp;#34;Reshape_105&amp;#34;5 bottom: &amp;#34;Conv_100&amp;#34;6 bottom: &amp;#34;Add_95&amp;#34;7 bottom: &amp;#34;im_info&amp;#34;8 top: &amp;#34;rois&amp;#34;9 top: &amp;#34;tmp_pooling&amp;#34;10 rpn_proposal_param{11 feat_stride: 1612 anchor_ratios: 113 anchor_scales: 114 anchor_scales: 215 anchor_scales: 416 anchor_scales: 817 anchor_scales: 1618 anchor_scales: 3219 test_desc_param {20 rpn_pre_nms_top_n: 200021 rpn_post_nms_top_n: 5022 rpn_min_size: 1623 rpn_nms_thresh: 0.724 }25 }2627 roi_pooling_param{28 pooled_h: 729 pooled_w: 730 spatial_scale: 0.06253132 }33}3435特别地，proposal layer中 rpn_post_nms_top_n的参数值如果使用默认的300,那么结果都是对的。把这个值改小（只要小于300)，结果就像上面所述。
debug 经过 首先根据rpn_post_nms_top_n的值一修改，结果就是错的来看，怀疑是哪里参数写死了。 然而很快就排除了这个问题。因为faster rcnn的模型已经在另一个产品中经过长期验证了。 而且proposal layer是tensorrt自己实现的，有bug早就有人发现了。</description></item><item><title>FPN:Feature Pyramid Networks 学习笔记</title><link>https://111qqz.com/2019/12/feature-pyramid-networks/</link><pubDate>Sun, 08 Dec 2019 17:30:50 +0800</pubDate><guid>https://111qqz.com/2019/12/feature-pyramid-networks/</guid><description>
检测不同尺度的物体一直是计算机视觉领域中比较有挑战性的事情．我们先从之前的工作出发，并对比FPN比起之前的工作有哪些改进．
之前的工作 Featurized image pyramid 思路是对于同一张图，生成不同的scale，然后每个scale的image单独去做检测． 这个方法是手工设计feautre时代的常用办法． 这个办法是比较显然的，也的确可以解决检测不同尺度物体的问题． 缺点非常明显...inference的速度几乎和scale的个数线性相关． 以及由于显存的开销，没办法做end-to-end 的training.
Single feature map 再之后，手工设计的feature逐渐被由CNN生成的feature取代了． 这种办法更加鲁棒，对image的一些变化不敏感． 但是如果只有一个scale 的图片去过这个feature map,只有最终的feature map去做predict..准确率不太行．．因此还是要与Featurized image pyramid一起．只是优化了得到feature 的部分．
Pyramidal feature hierarchy 就..CNN本身就有显然的层次结构啊．．． 为什么不直接拿来用，而是提前scale image呢．．．
也就是选若干个feature map直接去做predict.. 这个办法美滋滋，既有多个feature map保证了一定的准确率，同时也没有增加很多inference的cost.
SSD应该是率先使用这种方法的． 但是这种办法仍然有不足之处，就是低层的高分辨率的feature map的semantics 太弱了．． 这就导致说对小物体的检测效果不太理想．．．
那么该怎么办呢．．．这时候就该介绍FPN啦
Feature Pyramid Networks 后面再更．
FPN 用于 faster rcnn FPN同时作用于RPN阶段和fast-rcnn detector
以下的代码实现出自　fpn.pytorch
FPN 作用于RPN 感觉直接上代码比较容易理解
先看　整个网络的forward函数
1 2 3 def forward(self, im_data, im_info, gt_boxes, num_boxes): 4 batch_size = im_data.</description></item><item><title>SSD: Single Shot MultiBox Detector 学习笔记</title><link>https://111qqz.com/2019/12/single-short-detector/</link><pubDate>Sun, 08 Dec 2019 15:00:15 +0800</pubDate><guid>https://111qqz.com/2019/12/single-short-detector/</guid><description>
概述 SSD是一种单阶段目标检测算法．所谓单阶段，是指只使用了一个deep neural network,而不是像faster-rcnn这种两阶段网络． 为什么有了faster-rcnn还要使用SSD? 最主要是慢... 两阶段网络虽然准确率高，但是在嵌入式等算力不足的设备上做inference速度非常感人，很难达到real time的要求． （实际业务上也是这样，公有云上的检测模型几乎都是faster-rcnn,而到了一些盒子之类的硬件设备，检测模型就全是SSD等single stage 模型了)
之前一直没有写SSD是因为相比faster rcnn的细节，SSD的问题似乎并不是很多．直到最近转模型的时候被FASF模型的一个细节卡了蛮久，因此决定还是记录一下．
基本概念 这部分描述SSD中涉及到的一些想法．
prior box prior box的概念其实与faster-rcnn中anchor的概念是一样的，没有本质区别． 与faster-rcnn中的anchor不同的是，SSD会在多个feature map中的每个cell 都生成若干个prior_box.
对于一个特定的feature map,尺寸为m*n,假设有k个prior box,c种类别． 那么feature map的每个location会生成 k*(c+4) 个结果，其中c代表每一类的confidence. ４代表相对prior_box中心点的offset. 整个feature_map会生成　 kmn(c+4) 个结果．
prior_box的参数选择,以及一些训练有关的细节可以参考原论文,这里不再赘述. 这里主要想强调一下和priox box有关的inference 细节. 主要是decode box的部分.
由于模型输出的bbox其实是相对每个prior_box的offset,不是真正的bbox,因此需要由网络输出的box_pred和prior box得到真正的bbox 坐标.这部分通常称为decode box,其实已经算是后处理部分了.
pytorch中decode box的代码如下:
1 variance1, variance2 = variance 2 cx = box_prior[:, :, 3 0] + box_pred[:, :, 0] * variance1 * box_prior[:, :, 2] 4 cy = box_prior[:, :, 5 1] + box_pred[:, :, 1] * variance1 * box_prior[:, :, 3] 6 w = box_prior[:, :, 2] + torch.</description></item><item><title>rankboost 算法学习笔记</title><link>https://111qqz.com/2019/11/rankboost-Algorithm/</link><pubDate>Tue, 26 Nov 2019 20:59:04 +0800</pubDate><guid>https://111qqz.com/2019/11/rankboost-Algorithm/</guid><description>
boosting 算法是什么． 机缘使然，接触到了 Boosting 算法．Boosting是一种通过组合弱学习器来产生强学习器的通用且有效的方法.
动机是基于如下观察:尽管委员会中每个成员只提供一些不成熟的判断,但整个委员会却产生较为准确的决策。通过组合多个弱学习器来解决学习问题。给定训练数据,弱学习算法(如决策树)可以训练产生弱学习器,这些弱学习器只需要比随机猜测的准确率好一些。用不同的训练数据训练可以得到不同的弱学习器。这些弱学习器作为委员会成员,共同决策。
ranking task是什么 此处的ranking task指的的pairwise ranking
排名(ranking)的学习问题出现在许多现代应用程序中，包括搜索引擎，信息提取平台和电影推荐系统的设计。 在这些应用程序中，返回文档或电影的顺序是系统的关键方面。 在二进制情况下，对分类进行排名的主要动机是资源的限制：对于非常大的数据集，显示或处理由分类器标记为相关的所有项目可能是不切实际的，甚至是不可能的。 搜索引擎的标准用户不愿意查询响应查询而返回的所有文档，而只查询前十名左右。 同样，信用卡公司欺诈检测部门的成员不能调查被分类为潜在欺诈的数千笔交易，而只能调查几十个最可疑的交易。
所以到底什么才是一个ranking task呢?
ranking task是一种有监督学习，通过label 信息，来对所有的数据点做出排名的预测．这里的label是一种关系信息，与数据点对一一对应的．设label函数为f,一般来说，假设有数据点x1,x2,那么有
1f(x1,x2) = 1 if x1 ranks higher than x2 2f(x1,x2) = -1 if x1 ranks lower than x2 3f(x1,x2) = 0 if there is no enough info to compare x1 and x2 需要注意的是，通常来说label并没有传递性．也就是说，从f(x1,x2) = 1和　f(x2,x3) =1 并不能推断出f(x1,x3) = 1.原因是在比较x1,x2,x3时，可能使用了不同的feature(通俗的解释就是，我喜欢A胜过B是因为A比B有钱，我喜欢B胜过C是因为B比C可爱，但是我可能因为其他原因喜欢C胜过A)
rankboost 算法 字面理解，rankboost算法就是将boosting算法用于pairwise ranking task. 该过程的伪代码为: 要注意的是算法的输入为
1S=((x1,x1&amp;#39;,y1),...(xm,xm&amp;#39;,ym)) 其中　(x1,x1')为一个pair,y1为与这个pair对应的label.</description></item><item><title>Kubernetes(k8s)在深度学习模型转换方面的探索</title><link>https://111qqz.com/2019/11/K8s-for-Model-Conversion/</link><pubDate>Fri, 22 Nov 2019 11:14:19 +0800</pubDate><guid>https://111qqz.com/2019/11/K8s-for-Model-Conversion/</guid><description>
年中的时候接了离职的同事模型转换的锅，在不断地更新迭代的过程中，发现了一些痛点。 发现k8s能够解决一部分痛点，因此来分享一下在这方面的探索。
什么是模型转换 简单来说，深度学习模型的流程分为training和inference两部分。训练时用的一般是pytorch等框架，这些框架训练出的model是没办法直接部署在各个硬件平台上做inference的。因此需要将使用训练框架得到的模型，转换为能够部署到各个硬件平台上的模型。这个过程就是模型转换。
模型转换的一般流程为，先将pytorch等训练框架训练得到的模型转换为caffe model（是的，caffe才是业界中间表示的事实标准，而不是号称支持所有框架中间表示的onnx)，再将caffe model 转换到各种硬件上（包括但不限于nvidia系列显卡，华为的海思系列设备等）
（事实上这个转换流程是有硬伤存在的，这个硬伤在于pytorch的模型权重和模型结构是分离的。调研过业界一些其他模型转换的解决方案，包括百度的X2Paddle,其做法是不把pytorch模型作为模型转换的起点，而是从caffe model/onnx model 开始转换。还调研过微软的MMdnn,里面似乎也没有提到这个问题。不知道pytorch 1.0版本新增的torchscript是不是一条出路...)
模型转换的痛点 最初模型转换的痛点是依赖比较多，从pytorch,onnx,caffe再到tensorrt,cuda等．　尤其编译caffe,又是出了名的坑多．　解决办法是用了docker,将所有环境封装好．这样其他人可以直接拉镜像下来进行模型转换．
然而，在用了docker之后，还是有其他痛点，主要是以下三个:
权限申请的繁琐．
caffe模型转换到不同硬件上（主要是nvidia显卡），需要在对应的机器上进行转换．通常是客户确定使用某型号的显卡，然后我们采购对应显卡的机器．接下来需要一系列权限申请，包括服务器的权限，docker命令的权限，以及docker镜像仓库的权限．　除此之外，用户还需要将模型文件从其他位置放置到新服务器上，这件事也非常麻烦．
docker 镜像的版本控制.
docker的image虽然可以打tag，但是这个tag几乎对版本控制没有帮助．由于模型转换工具更新频繁（包括但是不限于，添加新的op/layer,caffe对新显卡的编译支持，caffe2tensorrt工具的更新），这么弱的版本管理就是灾难．如果我们对于每次升级docker image,都使用一个新的tag来标记的话，docker似乎没有一个机制来通知这个新的tag的存在．用户还是可以在这个旧的镜像上用到天荒地老．如果复用之前的tag的话,同样，用户还是可以不更新．
对docker commit 的滥用．
本来docker image的大小只有7G+,但是由于用户非常钟爱docker cp命令以及docker commit 命令．曾经有用户把整个数据集都docker cp进去之后docker commit ..　然后现在镜像大小已经有35G了．．
上述几个痛点，恰好k8s可以比较好的解决．
k8s用于模型转换. 官方的说法是,k8s是一个生产级别的容器编排系统，用于容器化应用的自动部署、扩缩和管理
然而似乎我对k8s的使用是非常非主流的(其实对docker的使用也很非主流...)
不过我觉得没有规定一项技术只能用来做什么,只要解决的问题多于引入的问题,就可以尝试.
k8s的引入使得用户不再能直接接触到docker image. 这直接解决了第一和第三两个痛点. 以及可以使用 imagePullPolicy 来进行强制更新,也基本解决了版本控制的问题. 这样就不用天天push 其他用户&amp;quot; 你先docker pull一下&amp;quot;...
此外,k8s的label机制也很有用. 我们可以通过给node打上不同显卡型号的label.然后维护一个显卡的列表.这样用户不再需要知道哪台服务器上有哪个显卡,只需要添写显卡型号,k8s就可以分配一台对应的机器供用户进行模型转换.
不过实际上k8s用户模型转换也有一些缺点:
k8s对显卡的调度是对于pod(pod就是一组container)独占的.也就是说,在一台服务器上,通过k8s只能开启与显卡个数一样多的pod.目前来看不会有什么大的影响,不过这对于对性能要求不敏感的应用,确实算个缺点.
pod名称的全局唯一性.由于开启任务需要显式提供一个名称,如果多个用户使用了相同的名称的话,结果显然不是我们期望的. 目前的解决方案是制定一个命名规范,要求用户遵守.但是这种规范并没有任何检查的机制,因此只能算一个tricky的办法,算不上真正的解决方案.</description></item><item><title>faster rcnn 模型 tensorrt4与tensorrt5 结果不一致 踩坑记录</title><link>https://111qqz.com/2019/11/debug-Frcnn-Model-When-Upgrading-From-Tensorrt4-to-Tensorrt5/</link><pubDate>Thu, 07 Nov 2019 23:40:39 +0800</pubDate><guid>https://111qqz.com/2019/11/debug-Frcnn-Model-When-Upgrading-From-Tensorrt4-to-Tensorrt5/</guid><description>
最近有同事report给我们,用同一个模型转换工具,转同一个faster rcnn 模型, 同样的sdk代码,在有些显卡上结果正常,但是再比较新的显卡上(比如Titan V)上 结果完全不正确.
听说之后我的内心其实是 **喵喵喵喵喵?**的
先在模型转换工具中infer了一下,发现...结果竟然真的不一样!
于是又开始了debug faster rcnn 的旅程(奇怪..我为什么要说又)
一份典型的faster rcnn 的
prototxt
按照经验,我们先对照了ROIS,来判断RPN 是否存在问题
惊讶地发现,竟然是没有问题的...
那看一下模型的输出 cls_score 和 bbox_pred好了 发现cls_score 完全对不上,bbox_pred 倒是没问题... 这和之前遇到的情况不太一样啊... 从proposal layer 到 最后... 全都是些很常见的layer... 哪里会有问题呢...
最终发现有问题的layer 是softmax!
cls_score 的维度应该为 N*K*C*H*W
softmax应该按照C这一维度来做,因此softmax_param中axis应该为2.
查阅&amp;quot;tensorrt support matrix&amp;quot; ，发现 tensorrt5中，softmax会按照用户指定的维度进行。
而对于tensorrt4.0, softmax layer 与设定的softmax_param无关，只会在channel 的维度上来做softmax.
所以，比较好的解决办法是，干脆不写softmax_param这个参数 对于trt4，会直接按照channel 维度来做 对于trt5，会在第N-3的axis上进行。对于SSD的 N*C*H*W或者 faster rcnn 的N*K*C*H*W, N-3都是channel所在的维度。
问题解决！</description></item><item><title>Anchor Box Algorithm</title><link>https://111qqz.com/2019/07/Anchor-Box-Algorithm/</link><pubDate>Mon, 01 Jul 2019 19:53:24 +0800</pubDate><guid>https://111qqz.com/2019/07/Anchor-Box-Algorithm/</guid><description>
动机 将一张图分成多个grid cell进行检测之后,每个cell只能检测到一个object. 如果这个grid cell中不止有一个物体要怎么办呢? 因此提出了anchor box algorithm来解决这个问题.
什么是anchor anchor其实就是一组预设的参考框,每个框有不同的长宽比和大小. 提供参考框可以将问题转换为&amp;quot;这个固定参考框中有没有认识的目标，目标框偏离参考框多远&amp;quot;. 这样如果一个grid cell中有多个物体,那么就可以形状最姐姐的anchor box来负责检测该物体. anchor的其他用途 实际上当grid cell很多的时候,一个grid cell中有多个object的情况是很少的.但是anchor box仍然是十分重要的.因为我们可以通过预设一些特殊的anchor box,来帮助检测到一些极端形状的物体(比如很长的物体或者很宽的物体)
输出结果 有了anchor之后,每个grid cell的输出可能不再唯一. 于是从之前的每个grid cell对应一个输出结果,变成了每个二元组(grid_cell,anchor box)对应一个结果. 结果的格式呢,一图胜千言: 局限性 由于anchor box是要预设的,那么如果我只预设了两个anchor box,但是一个grid cell中有三个object,怎么办呢? 很遗憾,是没有办法的. 另外一个case是,如果两个object都与其中一个anchor的形状最为接近,也是没有办法检测出两个物体的.
anchor box的生成 最初anchor box是通过人手工设定的,之后的YOLO的paper中提出了使用k-means的算法来生成anchor box.</description></item><item><title>目标检测领域的滑动窗口算法</title><link>https://111qqz.com/2019/06/sliding-windows/</link><pubDate>Sun, 30 Jun 2019 16:55:40 +0800</pubDate><guid>https://111qqz.com/2019/06/sliding-windows/</guid><description>
对象检测（Object Detection）的目的是”识别对象并给出其在图中的确切位置”，其内容可解构为三部分：
识别某个对象（Classification）； 给出对象在图中的位置（Localization）； 识别图中所有的目标及其位置（Detection）。 本文将介绍滑动窗口这一方法.
滑动窗口 滑动窗口是这些方法中最暴力的一个.简单来说,就是暴力枚举侯选框的尺寸和位置,每次crop得到一张小图,将每个小图送进后面的分类器进行分类. 早年后面通常会接一个计算量比较小的分类器,比如SVM,随着算力的提升,现在常常后面会接CNN. 值得一提的是,原始的滑动窗口方法是将每个小图,分别放入后面的分类器.但是实际上,小图和小图之间,是有overlap的,也就是说做了很多重复的计算. 因此一个显然的改进是使用CNN来实现滑动窗口算法, 这种方法的优点是比较无脑,实现和理解起来都很简单.缺点是计算量还是比较大.
参考链接 Sliding Windows for Object Detection with Python and OpenCV
深度学习基础 - 对象检测（滑窗、CNN、YOLO）</description></item><item><title>caffe2 添加自定义operater</title><link>https://111qqz.com/2018/04/add-custom-operation-in-caffe2/</link><pubDate>Fri, 13 Apr 2018 03:08:19 +0000</pubDate><guid>https://111qqz.com/2018/04/add-custom-operation-in-caffe2/</guid><description>
记录一些一个没有之前没有接触过caffe/caffe2的人为了添加自定义的op 到caffe2需要做的工作.
首先参考caffe2 tutorial,随便跑个op来试试,不妨以比较简单的 Accumulate_op 为例子.
这个op的作用就是计算Y=X+gamma*Y, 其中X为输入,Y为输出,gamma是参数.
跑起来这个运算所需要的代码如下:
from caffe2.python import workspace, model_helper import numpy as np # Create the input data data = np.arange(6).reshape(2,3).astype(np.float32) print (&amp;quot;data=&amp;quot;,data) # Create labels for the data as integers [0, 9]. workspace.FeedBlob(&amp;quot;data&amp;quot;, data) # Create model using a model helper m = model_helper.ModelHelper(name=&amp;quot;my first net&amp;quot;) output = m.net.Accumulate([&amp;quot;data&amp;quot;], &amp;quot;output&amp;quot;) print(m.net.Proto()) workspace.RunNetOnce(m.param_init_net) workspace.CreateNet(m.net) workspace.RunNet(m.name,2) # run 2 times print(&amp;quot;output=&amp;quot;,workspace.FetchBlob('output')) c之后我们仿照caffe2/operators/accumultate_op.h和affe2/operators/accumultate_op.cc,仿写一个我们自己的运算atest_op.h和atest_op.cc
实现的功能为Y=5X+gammaY
 #include &amp;quot;caffe2/operators/atest_op.</description></item><item><title>非极大值抑制（Non-Maximum Suppression，NMS）</title><link>https://111qqz.com/2018/03/non-maximum-suppression/</link><pubDate>Fri, 16 Mar 2018 02:56:14 +0000</pubDate><guid>https://111qqz.com/2018/03/non-maximum-suppression/</guid><description>
NMS是为了在诸多CV任务如边缘检测，目标检测等，找到局部最大值
其主要思想是先设定一个阈值，然后计算检测框的IOU(所谓IOU，也就是intersection-over-union，指的是相交面积除以相并面积，是来衡量overlap程度的指数）。如果IOU大于阈值，说明overlap过大，我们要通过某种算法来将其剔除。
比如下图，在经典的人脸识别任务中，出现了多个检测框，每个检测框有一个置信度confidence，我们通过某个算法，保留一个最好的。
顺便说一下算法的实现步骤把，其实不太重要。就是贪心。
其基本操作流程如下： * 首先，计算每一个 bounding box 的面积： * (x1, y1) ⇒ 左上点的坐标，(x2, y2) ⇒ 右下点的坐标； * (x2-x1+1)x(y2-y1+1) * 根据 scores 进行排序（一般从小到大），将 score 最大的bounding box置于队列，接下来计算其余 bounding box 与当前 score 最大的 bounding box 的 IoU，抑制（忽略也即去除）IoU大于设定阈值的 bounding box； * 重复以上过程，直至候选 bounding boxes 为空； 最后上一段python代码吧...也很简单，直接转载了别人的...
def nms(dets, thresh): x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) # 每个boundingbox的面积 order = scores.</description></item><item><title>reid 相关任务记录</title><link>https://111qqz.com/2018/02/reid-task-notes/</link><pubDate>Sat, 24 Feb 2018 04:34:02 +0000</pubDate><guid>https://111qqz.com/2018/02/reid-task-notes/</guid><description>
被师兄（同事？）普及了一番实验规范orz...
我还是太年轻了
所谓的一个fc的版本是右边的．一个放着不动，另一个在sequence_len（１０）的维度上做ave,然后再expand成原来的维度．如下图．
任务命名规则：
如D1V2_a_1,D1表示使用第一个数据集，V2表示是第二个大版本，ａ表示在V２大版本上的微调，最后的数字表示这是第几次运行该任务（跑三次以减少波动的影响）
logdir的地址为:/mnt/lustre/renkuanze/Data_t1/reid/log/｛$jobname｝
* D1:使用ilivids 数据集 * D1V1表示最初始的　baseline model * D1V2表示改为使用一个fc * D1V2_a是一个在一个FC上，不添加光流的修改版本 * D1V2_b是在一个FC上的baseline版本（也就是有光流） * D1V2_c是在一个FC上，有光流，batchsize从３２改为６４，gpu数目从4改为8的版本 * D1V3表示将softmax改为sigmod * D1V3_b表示将softmax改为sigmod的baseline版本 * D2:使用prid2011数据集 * D2V1表示初始的baseline model * D2V2表示改为使用一个fc * D2V2_b是在一个FC上的baseline版本 *</description></item><item><title>分类评价指标之Cumulative Match Characteristi (CMC)曲线</title><link>https://111qqz.com/2018/02/cumulative-Match-characteristi/</link><pubDate>Fri, 23 Feb 2018 08:20:55 +0000</pubDate><guid>https://111qqz.com/2018/02/cumulative-Match-characteristi/</guid><description>
CMC曲线全称是Cumulative Match Characteristic (CMC) curve，也就是累积匹配曲线，同ROC曲线Receiver Operating Characteristic (ROC) curve一样，是模式识别系统，如人脸，指纹，虹膜等的重要评价指标，尤其是在生物特征识别系统中，一般同ROC曲线（ 多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法）一起给出，能够综合评价出算法的好坏。
转一篇通俗易懂的解释：
Shortly speaking, imagine that you have 5 classes. For simplicity, imagine you have one test per class. Each test produces a score when compared to each class. Let's start from test1 which belongs to class1:
As your similarity measure, here, is Euclidian distance, the more distance a test has compared to a class, the less similarity is obtained.</description></item><item><title>pytorch 函数笔记</title><link>https://111qqz.com/2018/02/pytorch-function-notes/</link><pubDate>Fri, 23 Feb 2018 02:55:25 +0000</pubDate><guid>https://111qqz.com/2018/02/pytorch-function-notes/</guid><description>
记录一些常用的...总去查文档也是有点麻烦
* tensor.view 的作用是reshape 比如 a = torch.range(1, 16) 得到一个tensor that has 16 elements from 1 to 16. 在a=a.view(4,4)就得到了一个44的tensor。 需要注意reshape之后元素的个数不能改变(16==44) 参数-1的作用是，我懒得算这一维度应该是多少,（由于元素个数不能改变）所以希望自动被计算。**需要注意的是，只有一个维度可以写-1。 **不过view和reshape有些区别：reshape always copies memory. view never copies memory * torch.squeeze 将输入张量形状中的1 去除并返回。 如果输入是形如(A×1×B×1×C×1×D)，那么输出形状就为： (A×B×C×D)当给定dim时，那么挤压操作只在给定维度上。例如，输入形状为: (A×1×B), squeeze(input, 0) 将会保持张量不变，只有用 squeeze(input, 1)，形状会变成 (A×B)。注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。 * torch.unsqueeze 返回一个新的张量，对输入的制定位置插入维度 1 注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。如果dim为负，则将会被转化dim+input.dim()+1 &amp;gt;&amp;gt;&amp;gt; x = torch.Tensor([1, 2, 3, 4]) &amp;gt;&amp;gt;&amp;gt; torch.unsqueeze(x, 0) 1 2 3 4 [torch.FloatTensor of size 1x4] &amp;gt;&amp;gt;&amp;gt; torch.</description></item><item><title>光流法初探</title><link>https://111qqz.com/2018/02/Optical-flow-notes/</link><pubDate>Thu, 22 Feb 2018 09:03:48 +0000</pubDate><guid>https://111qqz.com/2018/02/Optical-flow-notes/</guid><description>
算是CV领域的传统算法了
只写两句话就够了。
**它是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。** 研究光流场的目的就是为了从图片序列中近似得到不能直接得到的运动场。运动场，其实就是物体在三维真实世界中的运动；光流场，是运动场在二维图像平面上（人的眼睛或者摄像头）的投影。
参考资料：
光流Optical Flow介绍与OpenCV实现</description></item><item><title>end-to-end 神经网络</title><link>https://111qqz.com/2018/02/end-to-end-neural-network/</link><pubDate>Thu, 22 Feb 2018 02:53:01 +0000</pubDate><guid>https://111qqz.com/2018/02/end-to-end-neural-network/</guid><description>
所谓end-to-end 神经网络，更多是一种思想。
这种思想的核心是，比如对于图像处理，输入原始图像数据，输出的是直接有用的结果（有用取决于具体的任务，比如自动驾驶)
也就是尽可能少得减少人为干预，使训练是end (原始数据) to end (对应用问题直接有用的结果)
端到端指的是输入是原始数据，输出是最后的结果，原来输入端不是直接的原始数据，而是在原始数据中提取的特征，这一点在图像问题上尤为突出，因为图像像素数太多，数据维度高，会产生维度灾难，所以原来一个思路是手工提取图像的一些关键特征，这实际就是就一个降维的过程。 那么问题来了，特征怎么提？ 特征提取的好坏异常关键，甚至比学习算法还重要，举个例子，对一系列人的数据分类，分类结果是性别，如果你提取的特征是头发的颜色，无论分类算法如何，分类效果都不会好，如果你提取的特征是头发的长短，这个特征就会好很多，但是还是会有错误，如果你提取了一个超强特征，比如染色体的数据，那你的分类基本就不会错了。 这就意味着，特征需要足够的经验去设计，这在数据量越来越大的情况下也越来越困难。 于是就出现了端到端网络，特征可以自己去学习，所以特征提取这一步也就融入到算法当中，不需要人来干预了。
简单得说，符合end-to-end 的神经网络，特征应该是网络自己学习，而不是人为提取。
参考资料：
什么是 end-to-end 神经网络？</description></item><item><title>Pose-driven Deep Convolutional Model for Person Re-identification 阅读笔记</title><link>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification/</link><pubDate>Thu, 22 Feb 2018 02:25:00 +0000</pubDate><guid>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification/</guid><description>
1709.08325
Reid问题指的是判断一个probe person 是否在被不同的camera捕获的gallery person 中出现。
通常是如下情景：给出一个特定camera下某个特定人的probe image 或者 video sequence，从其他camera处询问这个人的图像，地点，时间戳。
ReID问题至今没有很好得解决，主要原因是，不同camera下，人的姿势（pose),观察的视角(viewpoint) 变化太大了。
传统方法主要在两个大方向上努力:
1. **用一些在图像上抽取的不变量来作为人的特征feture** 2. **去学习一个不同的距离度量方式，以至于同一个人在不同camera之间的距离尽可能小。** 但是由于在实际中，行人的pose和 摄像机的viewpoint不可控，因此与之相关的feture可能不够健壮。
学习新的不同的距离度量方式需要每对camera分别计算距离，然而这是O(n^2)的时间复杂度，凉凉。
近些年Deep learning发展迅猛，并且在很多CV任务上表现良好。所以自然有人想把Deep learning 方法应用到Reid任务上。
目前Deep learning的做法一般分为两部分：
* 使用softmax loss 结合person ID labels得到一个global representation * 首先用预定义好的body 刚体模型去得到local representation,然后将global 和local representation 融合。 目前用deep learning的方法效果已经不错了，比传统方法要好。但是目前的deep learning方法没有考虑到人的姿势(pose)的变化。
虽然目前也有些deep learning的办法在处理Reid问题时使用pose estimation algorithms 来预测行人的pose,
但是这种办法是手动完成而不是一个end-to-end（什么是end-to-end 神经网络） 的过程
所以考虑pose的潜力还没有被完全发掘。
这篇paper主要做了以下工作：
* 提出了一种新的深层结构，将身体部分转化成相应的特征表示，以此来克服pose变化带来的问题 * 提出了一个用来自动学习各部分权值的sub-network 这两部分工作都是end-to-end的</description></item><item><title>Deep Mutual Learning（相互学习） 阅读笔记</title><link>https://111qqz.com/2018/02/deep-mutual-learning-notes/</link><pubDate>Sun, 18 Feb 2018 10:46:35 +0000</pubDate><guid>https://111qqz.com/2018/02/deep-mutual-learning-notes/</guid><description>
原始论文
DNN在很多问题上效果很不错，但是由于深度和宽度过大，导致需要的执行时间和内存过大。我们需要讨论一些能快速执行并且对内存的需要不大的模型。
已经有很多方法来做这件事，比较重要的是Model distillation（模型蒸馏）
基于蒸馏的模型压缩的有效性是基于一个发现：小的网络有和大的网络一样的表达能力，只不过是更难以训练找到合适的参数。
也就是说，难点在于优化(以找到合适的参数）而不在于网络的尺寸。
蒸馏的模型的做法是把一个有效的(deep or/and wide) network 作为teacher network,让一个size 较小的 student network 模仿teacher network.
这样做的好处是，size小的多的student network训练如何模仿teacher network通常要比直接训练得到目标函数容易，而效果上与teacher network相当，甚至超过teacher network.
在这篇paper中，作者提出了一个和蒸馏模型相关但是不同的模型: Deep Mutual Learning (相互学习，以下缩写为DML)
蒸馏模型开始于一个强大的教师网络，并通过教师网络**单向(one way)**教授学生网络知识。
相反，DML中,开始于一群没有经受过训练的学生网络。
具体来说，每个学生训练有两种损失:
* **常规的监督学习损失和对齐的模仿损失** * **每个学生的在班级中落后于其他学生的概率。** 结果表明，DML的方式：比每个学生网络单独学习要好，也比传统的传统的蒸馏模型要好。
此外，我们发现，对几个tearcher network 使用 DML，要往往有更好的结果。
另外一些发现：
* 效果随着班级中学生网络的数量的增加而增加 * 它适用于各种网络体系结构和异构群组 关于DML为什么有效的直觉讨论：
* 常规的监督学习损失保证每个学生单独学习时，整体上结果是越来越好的。 * 由于初始条件不同，对下一个最可能的class的预测概率不同，这是额外信息的来源。</description></item><item><title>Similarity learning 和Metric learning</title><link>https://111qqz.com/2018/02/similarity-learning-metric-learning/</link><pubDate>Sun, 18 Feb 2018 08:14:10 +0000</pubDate><guid>https://111qqz.com/2018/02/similarity-learning-metric-learning/</guid><description>
Similarity_learning 相似性学习（Similarity learning ）有监督机器学习，它与回归和分类密切相关，但目标是从实例中学习一个相似函数，以衡量两个对象的相似程度或相关程度。
Similarity learning通常有四种setups:
* regression similarity learning 在这种方式中，给出的数据是 ![(x_{i}^{1},x_{i}^{2})](https://wikimedia.org/api/rest_v1/media/math/render/svg/cfa249357a1b4a7baf332041d67e480d6bb1f8fb)  和他们的相似度 . 目标是学习到一个函数 ，对于 给出yi的近似值。 * Classification similarity learning 给出数据 和他们是否相似 。目标是训练出一个分类器，能够完成对一组 是否相似的二分类判断。 * Ranking similarity learning 给出有序三元组 ，其中 is known to be more similar to than to 。目标是训练出一个函数，使得对于新的三元组 ，满足 。容易看出，这种方式采取了比回归更弱的监督形式，因为不需要提供精确的相似性度量，只需要提供相似性的相对顺序。因此，这种ranking-based的相似性学习更容易应用于实际的大规模应用 * Locality sensitive hashing (LSH) 局部敏感哈希和普通哈希的不同就是，相似的项有更大的概率被放到同一个桶中。
顺便一提，这里有一个叫triplet loss 的概念，
如上图所示，triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本和不同类的样本,这两个样本对应的称为Positive (记为x_p)和Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。
我们发现，triplet loss 其实就是在ranking similarity learning 问题中，学习similarity function时的loss
Metric learning 相似性学习与距离度量学习密切相关。度量学习的目标是在对象上学习一个距离函数。度量或距离函数必须遵循四个公理:非负性、不可分辨的恒等式、对称性和次可加性/三角形不等式。在实际应用中，度量学习算法忽略了不可分辨物体的身份条件，学习了一个伪度量。</description></item><item><title>persion reid 论文列表</title><link>https://111qqz.com/2018/02/persion-reid-paper-list/</link><pubDate>Sat, 17 Feb 2018 07:17:49 +0000</pubDate><guid>https://111qqz.com/2018/02/persion-reid-paper-list/</guid><description>
Key:
(1). Pose-driven, body part alignment, combine whole feature and body part feature, focus on alignment of part model,
(2). Combine image label and human attributes classes, do classification with attributes and identity learning
(3). Based on triplet loss, improve metric learning for an end to end learning
(4). Post-process, re-ranking
AlignedReID: Surpassing Human-Level Performance in Person Re-Identification
Hydraplus-net: Attentive deep features for pedestrian analysis.
Darkrank: Accelerating deep metric learning via cross sample similarities transfer.</description></item><item><title>推荐系统之 LFM (Latent Factor Model) 隐因子模型 学习笔记</title><link>https://111qqz.com/2018/02/Latent-Factor-Model-notes/</link><pubDate>Fri, 09 Feb 2018 13:02:06 +0000</pubDate><guid>https://111qqz.com/2018/02/Latent-Factor-Model-notes/</guid><description>
起因是被assgin了一个新的任务.....要死.
参考资料:
推荐系统学习笔记之三 LFM (Latent Factor Model) 隐因子模型 + SVD (singular value decomposition) 奇异值分解
基于矩阵分解的隐因子模型
实时推荐系统的三种方式
先说下我的理解...
隐因子模型(LFM)是一种推荐算法,&amp;quot;隐&amp;quot;可以理解成用户喜欢某个item的间接原因.
该算法的核心思想是转化成一个矩阵分解问题..
然后用传统机器学习算法去优化分解得到的矩阵...
主要的优势如下： * 比较容易编程实现，随机梯度下降方法依次迭代即可训练出模型。 * 预测的精度比较高，预测准确率要高于基于领域的协同过滤以及基于内容CBR等方法。 * 比较低的时间和空间复杂度，高维矩阵映射为两个低维矩阵节省了存储空间，训练过程比较费时，但是可以离线完成；评分预测一般在线计算，直接使用离线训练得到的参数，可以实时推荐。 * 非常好的扩展性，如由SVD拓展而来的SVD++和 TIME SVD++。 矩阵分解的不足主要有：
* 训练模型较为费时。 * 推荐结果不具有很好的可解释性，无法用现实概念给分解出来的用户和物品矩阵的每个维度命名，只能理解为潜在语义空间。 我们用user1,2,3表示用户，item 1,2,3表示物品，Rij表示用户i对于物品j的评分，也就是喜好度。那么我们需要得到一个关于用户-物品的二维矩阵，如下面的R。
常见的系统中，R是一个非常稀疏的矩阵，因为我们不可能得到所有用户对于所有物品的评分。于是利用稀疏的R，填充得到一个满矩阵R’就是我们的目的。
下面我们就来看看LFM是如何解决上面的问题的？对于一个给定的用户行为数据集（数据集包含的是所有的user, 所有的item，以及每个user有过行为的item列表），使用LFM对其建模后，我们可以得到如下图所示的模型：（假设数据集中有3个user, 4个item, LFM建模的分类数为4）
 R矩阵是user-item矩阵，矩阵值Rij表示的是user i 对item j的兴趣度，这正是我们要求的值。对于一个user来说，当计算出他对所有item的兴趣度后，就可以进行排序并作出推荐。LFM算法从数据集中抽取出若干主题，作为user和item之间连接的桥梁，将R矩阵表示为P矩阵和Q矩阵相乘。其中P矩阵是user-class矩阵，矩阵值Pij表示的是user i对class j的兴趣度；Q矩阵式class-item矩阵，矩阵值Qij表示的是item j在class i中的权重，权重越高越能作为该类的代表。所以LFM根据如下公式来计算用户U对物品I的兴趣度
我们发现使用LFM后，
1. 我们不需要关心分类的角度，结果都是基于用户行为统计自动聚类的，全凭数据自己说了算。 2. 不需要关心分类粒度的问题，通过设置LFM的最终分类数就可控制粒度，分类数越大，粒度约细。 3. 对于一个item，并不是明确的划分到某一类，而是计算其属于每一类的概率，是一种标准的软分类。 4.</description></item><item><title>多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法</title><link>https://111qqz.com/2018/02/mean-average-precision-for-multi-label-classification-task/</link><pubDate>Fri, 09 Feb 2018 03:53:09 +0000</pubDate><guid>https://111qqz.com/2018/02/mean-average-precision-for-multi-label-classification-task/</guid><description>
参考资料:
多标签图像分类任务的评价方法-mAP
wiki_Sensitivity and specificity
False Positives和False Negative等含义
mean average precision（MAP）在计算机视觉中是如何计算和应用的？
首先需要了解True(False) Positives (Negatives)的含义
True Positive （真正, TP）被模型预测为正的正样本；可以称作判断为真的正确率
True Negative（真负 , TN）被模型预测为负的负样本 ；可以称作判断为假的正确率
False Positive （假正, FP）被模型预测为正的负样本；可以称作误报率
False Negative（假负 , FN）被模型预测为负的正样本；可以称作漏报率
在图像中，尤其是分类问题中应用AP，是一种评价ranking方式好不好的指标：
举例来说，我有一个两类分类问题，分别5个样本，如果这个分类器性能达到完美的话，ranking结果应该是+1，+1，+1，+1，+1，-1，-1，-1，-1，-1.
但是分类器预测的label，和实际的score肯定不会这么完美。按照从大到小来打分，我们可以计算两个指标：precision和recall。比如分类器认为打分由高到低选择了前四个，实际上这里面只有两个是正样本。此时的recall就是2（你能包住的正样本数）/5（总共的正样本数）=0.4，precision是2（你选对了的）/4（总共选的）=0.5.
图像分类中，这个打分score可以由SVM得到：s=w^Tx+b就是每一个样本的分数。
从上面的例子可以看出，其实precision，recall都是选多少个样本k的函数，很容易想到，如果我总共有1000个样本，那么我就可以像这样计算1000对P-R，并且把他们画出来，这就是PR曲线：
这里有一个趋势，recall越高，precision越低。这是很合理的，因为假如说我把1000个全拿进来，那肯定正样本都包住了，recall=1，但是此时precision就很小了，因为我全部认为他们是正样本。recall=1时的precision的数值，等于正样本所占的比例。
所以AP，average precision，就是这个曲线下的面积，这里average，等于是对recall取平均。而mean average precision的mean，是对所有类别取平均（每一个类当做一次二分类任务）。现在的图像分类论文基本都是用mAP作为标准。
使用AP会比accuracy要合理。对于accuracy，如果有9个负样本和一个正样本，那么即使分类器什么都不做全部判定为负样本accuracy也有90%。但是对于AP，recall=1那个点precision会掉到0.1.曲线下面积就会反映出来。
precision 和 recall 的计算： &amp;amp;lt;img src=&amp;quot;https://pic2.zhimg.com/50/v2-761706f5b1fe36873ba1bb20c7d1d447_hd.jpg&amp;quot; data-rawwidth=&amp;quot;301&amp;quot; data-rawheight=&amp;quot;541&amp;quot; class=&amp;quot;content_image&amp;quot; width=&amp;quot;301&amp;quot;&amp;amp;gt; 图中上部分，左边一整个矩形中（false negative和true positive）的数表示ground truth之中为1的（即为正确的）数据，右边一整个矩形中的数表示ground truth之中为0的数据。
精度precision的计算是用 检测正确的数据个数/总的检测个数。
召回率recall的计算是用 检测正确的数据个数/ground truth之中所有正数据个数。
AP：average precision 假设我们有数据： &amp;amp;lt;img src=&amp;quot;https://pic1.</description></item><item><title>Non-local Neural Networks 阅读笔记</title><link>https://111qqz.com/2018/02/non-local-neural-networks-notes/</link><pubDate>Mon, 05 Feb 2018 02:24:34 +0000</pubDate><guid>https://111qqz.com/2018/02/non-local-neural-networks-notes/</guid><description>
先粗略读了2遍orz.可能不够严谨，先写一些high-level的理解。
对于序列或者图片数据，如果想获得一个long-range的依赖，通常的做法是循环神经网络（对于序列）或者深层的卷积神经网络（对于图片数据）
但是循环操作（当前的处理依赖于前面有限的若干个）和卷积操作都是一种局部操作。
但是这种局部操作是有一些局限的，比如不好优化，计算代价比较大等。
这篇paper提出了non-local 这个操作。
non-local操作是计算机视觉中广泛使用的一种降噪算法，即non-local mean的一般化。
non-local operation被认为是一个可以被广泛使用的操作，几乎可以和当前神经网络的其他部件结合。
含有non-local opetation的一个基本操作单元我们称之为一个 non-local block
含有non-local block 的神经网络我们可以称之为Non-local Neural Networks
non-local operation是非常有效的，及时神经网络只有很少的几层（比如5）
non-local operation和《Attention is all you need》 中提出的self-attention是相似的
全连接操作可以看做non-local operation的一个特例。
Non-local Neural Networks 原始论文</description></item><item><title>non-local means algorithm 学习笔记</title><link>https://111qqz.com/2018/01/non-local-means-algorithm-notes/</link><pubDate>Thu, 25 Jan 2018 02:53:52 +0000</pubDate><guid>https://111qqz.com/2018/01/non-local-means-algorithm-notes/</guid><description>
终于忙完学校的事情可以干正事了orz
这里会记录一些第一遍看paper的过程中遇到的一些影响理解的概念，不过大多不会深究，只算做粗浅的理解。
1、高斯金字塔：
高斯金字塔是最基本的图像塔。首先将原图像作为最底层图像G0（高斯金字塔的第0层），利用高斯核（5*5）对其进行卷积，然后对卷积后的图像进行下采样（去除偶数行和列）得到上一层图像G1，将此图像作为输入，重复卷积和下采样操作得到更上一层图像，反复迭代多次，形成一个金字塔形的图像数据结构，即高斯金字塔。
2、拉普拉斯金字塔
在高斯金字塔的运算过程中，图像经过卷积和下采样操作会丢失部分高频细节信息。为描述这些高频信息，人们定义了拉普拉斯金字塔(Laplacian Pyramid， LP)。用高斯金字塔的每一层图像减去其高一层图像上采样并高斯卷积之后的预测图像，得到一系列的差值图像即为 LP 分解图像。
将Gl内插方法得到放大图像_Gl，使_Gl的尺寸与*Gl-1的尺寸相同，即放大算子Expand。
参考资料：
Gaussian and Laplacian Pyramids
拉普拉斯金字塔融合</description></item><item><title>PCA + kmeans</title><link>https://111qqz.com/2017/11/pca-kmeans/</link><pubDate>Sun, 26 Nov 2017 11:05:50 +0000</pubDate><guid>https://111qqz.com/2017/11/pca-kmeans/</guid><description>
先记录一下PCA实战需要用到的安装包(arch下,python2环境) python2-scikit-learn python2-numpy
python2-pandas
python2-matplotlib
python2-seaborn
pandas.DataFrame
pandas 数据结构介绍
几个和科学计算数据分析有关的重要的python库:Numpy、Matplotlib ,pandas
(之前数字图像处理课程都接触过了orz)
其中matplotlib 主要用于图像绘制
sklearn 是用于机器学习的python 模块
Seaborn也是用于图像绘制
str.fomat() 是 python2语法
format中的变量会按照str中{} 出现的顺序替换
import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.datasets import fetch_mldata from sklearn.decomposition import PCA import seaborn as sns mnist = fetch_mldata(&amp;quot;MNIST original&amp;quot;) X = mnist.data / 255.0 y = mnist.target #print X.shape, y.shape feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ] df = pd.</description></item><item><title>反向传播学习笔记</title><link>https://111qqz.com/2017/09/back-propagation-notes/</link><pubDate>Tue, 05 Sep 2017 12:30:17 +0000</pubDate><guid>https://111qqz.com/2017/09/back-propagation-notes/</guid><description>
先说下自己目前很笼统的理解：
反向传播是用来快速计算梯度的一种方法；
过程大概是把计算过程用计算图表示，这样每一个中间步骤都有一个节点，每一个local gradient都会比较容易计算；
思想涉及 chain rule + 计算图 + 记忆化
因为计算不同自变量的偏导数会存在很多共同路径，这部分就只计算了一次，因此可以加快计算速度。
所以核心的东西大概是两点：
* 用计算图表示计算，局部gradient 替代繁琐的微积分计算 * 共同部分只计算一次，类似一个记忆化。</description></item><item><title>tensorflow input pipline 学习笔记</title><link>https://111qqz.com/2017/08/tensorflow-input-pipline-notes/</link><pubDate>Thu, 24 Aug 2017 09:12:58 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-input-pipline-notes/</guid><description>
参考资料：
tf_doc_Reading data
TENSORFLOW INPUT PIPELINE EXAMPLE
tensorflow：理解tensorflow中的输入管道
第二个参考资料是第一个的翻译版本，翻译的水平一般，建议看原文，不是很长。
下面是我挑了文章中重点的部分+自己的理解。
TL;DR; 一个适用于不是很大的数据集的pipline input 的例子。
Load Data in Tensorflow input pipline 可以理解称一种load data的方式。 一般有两种方式load data,一种是比较传统的，使用feed 的方式。如果数据集比较大，这种方式就不适用了，因为这种方式需要将数据全部导入到memory中。因此tf提供了pipline input的读入数据的方式。
input pipline 会处理 csv file,解码文件格式，重构数据结构，打乱数据顺序，做数据扩充或者其他预处理，然后使用线程(threads)将数据导进batch.
Load the Label Data 确保使用正确的dataset,csv文件路径。
然后处理 得到train和test 的label
由于我们只是读数据而没有真的打算训练，所以没有使用one-hot的编码方式，而是直接将（本来也是由数字字符组成的）字符串，转化成int.
def encode_label(label): return int(label) def read_label_file(file): f = open(file, &amp;quot;r&amp;quot;) filepaths = [] labels = [] for line in f: filepath, label = line.split(&amp;quot;,&amp;quot;) filepaths.append(filepath) labels.append(encode_label(label)) return filepaths, labels # reading labels and file path train_filepaths, train_labels = read_label_file(dataset_path + train_labels_file) test_filepaths, test_labels = read_label_file(dataset_path + test_labels_file) Do Some Optional Processing on Our String Lists # transform relative path into full path train_filepaths = [ dataset_path + fp for fp in train_filepaths] test_filepaths = [ dataset_path + fp for fp in test_filepaths] # for this example we will create or own test partition all_filepaths = train_filepaths + test_filepaths all_labels = train_labels + test_labels # we limit the number of files to 20 to make the output more clear!</description></item><item><title>tensorflow 合并模型</title><link>https://111qqz.com/2017/08/tensorflow-model-merging/</link><pubDate>Mon, 21 Aug 2017 06:56:22 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-model-merging/</guid><description>
在这里存个备份，还有些问题没有解决。
raise ValueError(&amp;quot;GraphDef cannot be larger than 2GB.&amp;quot;)
记录一些思路好了。现在是没有生成.meta文件，爆掉应该是因为所有的变量都加载到了默认图里。
也就是说我处理完checkpoint 0 之后开始处理checkpoint1,但是checkpoint0的那些变量还是存在的...所以越来越多？
目前有两个想法，第一个想法是是受TensorFlow极简教程：创建、保存和恢复机器学习模型 中启发，用多个saver，每个saver指定要搞的图（但是这样好像要每个checkpoint都是不同的saver才有意义？）
第二个想法是，每次save完变量之后，将图恢复成默认状态（可以把图中所有变量清空。。
想法二大失败：
会遇到if self.stack[-1] is not default: │ IndexError: list index out of range 的问题。。
根据 reset_default_graph awkwardly breaks graph nesting
中提到了。。。reset_default_graph本身就不舍被设计成放在graph中清空变量用的。。。然后tf的代码也写得很不友好。。。没有 指明这个错误的原因。。。
For historical context, `tf.reset_default_graph()` was never designed to be used with `with g.as_default():` context managers. I think the proper fix here is to make `tf.reset_default_graph()` fail with an informative error message when used inside a `with g.</description></item><item><title>tensorflow checkpoint 学习笔记</title><link>https://111qqz.com/2017/08/tensorflow-checkpoint-notes/</link><pubDate>Mon, 21 Aug 2017 02:03:45 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-checkpoint-notes/</guid><description>
参考资料：
What is the TensorFlow checkpoint meta file?
TensorFlow: Restoring variables from from multiple checkpoints
合并模型的时候发现.meta一直在累加，而其他数据文件没有改变。因此来探究一下checkpoint的几个文件的含义。
This file contains a serialized [`MetaGraphDef` protocol buffer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto). The `MetaGraphDef` is designed as a serialization format that includes all of the information required to restore a training or inference process (including the [`GraphDef`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto) that describes the dataflow, and additional annotations that describe the variables, input pipelines, and other relevant information). For example, the `MetaGraphDef` is used by [TensorFlow Serving](https://tensorflow.</description></item><item><title>tensorflow variable 学习笔记</title><link>https://111qqz.com/2017/08/tensorflow-variable-notes/</link><pubDate>Sun, 20 Aug 2017 09:36:00 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-variable-notes/</guid><description>
参考资料：
programmers_guide/variables
tf/Variable
之前感觉对tensorflow 的variable的理解不是很深刻...跑个模型啥的倒不会有什么问题，但是涉及分布式，模型并行之类的，感觉有些地方还是要理解得仔细一点比较好。
OVERVIEW variable的目的是将状态可持久化。
Unlike tf.Tensor objects, a tf.Variable exists outside the context of a singlesession.run call.
通俗地说就是，variable可以用来存储一个可持久化的tensor
一些op允许读取或者修改tensor的值，这些修改是跨session可见的，也就是说，对于用variable可持久化过的tensor,多个worker（多卡之间）之间可以看到相同的值。
Creating a Variable 创建变量可以通过调用tf.get_variable function的方法实现。这个函数要求指明变量名称，这个名称会被作为标识该变量的key。
tf.get_variable也允许变量复用，意思是用之前创建过的有相同名字的变量，创建当前的变量。
my_int_variable = tf.get_variable(&amp;quot;my_int_variable&amp;quot;, [1, 2, 3], dtype=tf.int32, initializer=tf.zeros_initializer) #分别是变量名，shape，变量类型，初始化方法。 #默认类型为tf.float32，默认初始化方法是随机数。 #tf提供很多其他的初始化方法，以及也可以用一个tensor作为初始化。 #注意用tensor作为初始化时，shape就不用提供了，因为会用tensor的shape作为variable的shape Variable collections 由于在不同的部分创建了的变量可能有是够想一起访问，所以我们需要一个简单的能访问一个集合的变量的方法。tensorflow提供了collecetions,可以理解成python list,
是一个存储tensor，variable或者其他实例的容器。
默认情况下，tf.variable被收集到如下两个collcetions:
* tf.GraphKeys.GLOBAL_VARIABLES：放置可以被多个设备共享的variable(从名字中的GLOBAL也可以看出来...) * tf.GraphKeys.TRAINABLE_VARIABLES:用来放置用来计算梯度的variable 如果不想训练某个variable,那么将它加入名字叫tf.GraphKeys.LOCAL_VARIABLES 的默认collection...
下面是一个将名字叫my_local的variable加入tf.GraphKeys.LOCAL_VARIABLES的例子
my_local = tf.get_variable(&amp;quot;my_local&amp;quot;, shape=(), collections=[tf.GraphKeys.LOCAL_VARIABLES]) 一个等价写法是，将trainable属性设置为False
my_non_trainable = tf.get_variable(&amp;quot;my_non_trainable&amp;quot;, shape=(), trainable=False) 当然自定义collcetion也是可以的，名字可以是任何字符串。</description></item><item><title>tensorflow Session 学习笔记</title><link>https://111qqz.com/2017/08/tensorflow-session-notes/</link><pubDate>Sun, 20 Aug 2017 08:21:57 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-session-notes/</guid><description>
tensorflow-session官方文档
说下我自己的理解：
session中文一般叫会话，可以理解成op执行时候需要的一层虚拟化的封装。
op必须在session中才能执行。
tensor也是在tensor中才可以存在（tf.variable和tensor几乎是一回事，只是tf.variable的会话不要求session，也可以理解成tf.variable在session中就成了tensor.
需要注意的是session一般会占据资源，所以在使用完记得释放，或者写成with的形式（看到with总想叫成开域语句...感觉暴露年龄orz
下面这两种形式是等价的：
# Using the `close()` method. sess = tf.Session() sess.run(...) sess.close() # Using the context manager. with tf.Session() as sess: sess.run(...) session本身有一些配置，我们使用configproto：
# Launch the graph in a session that allows soft device placement and # logs the placement decisions. sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) allow_soft_placement的作用是自动选择可用的设备（如果指定的设备不可用（？）），防止指定的设备不可用而挂掉的情况。
log_device_placement :To find out which devices your operations and tensors are assigned to.</description></item><item><title>Distributed Tensorflow : Cannot assign a device for operation save</title><link>https://111qqz.com/2017/08/distributed-tensorflow-cannot-assign-a-device-for-operation-save/</link><pubDate>Mon, 14 Aug 2017 01:55:15 +0000</pubDate><guid>https://111qqz.com/2017/08/distributed-tensorflow-cannot-assign-a-device-for-operation-save/</guid><description>
是在使用分布式tensorflow遇到的一个错误
报错如下：
InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/Rest│| 2 GeForce GTX 1080 On | 0000:08:00.0 Off | N/A | oreV2_888': Operation was explicitly assigned to /job:worker/task:0/device:CPU:0 but available │| 24% 39C P8 12W / 180W | 0MiB / 8114MiB | 0% Default | devices are [ /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/gpu:0 ]. Make sure the device specification refers to a valid device.
其中看到Distributed Tensorflow : Cannot assign a device for operation.</description></item><item><title>分布式 tensorflow 学习笔记(非最终版)</title><link>https://111qqz.com/2017/08/tensorflow-notes/</link><pubDate>Mon, 07 Aug 2017 12:54:23 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-notes/</guid><description>
感觉资料不是很多，先收集资料好了。
tf-distributed官网文档
SO-between-graph和in-graph的区别
inception.README.md
SyncReplicasOptimizer
SO_How does ps work in distribute Tensorflow?
update:在多个nodes（机）上跑。。。tf默认是异步更新的。。。同步的话。。大概需要syncreplicasoptimizer?
来直观感受下，不同task的之间并不同步
创建一个cluster 每一个task唯一对应一个server,server中有一个master，还有若干个worker
cluster是task的集合
cluster是在处理分布式问题时抽象出的概念，类似缩点。
cluster也可以划分成一个或者多个job，每个job包含一个或者多个task.
所以task,job,cluster的关系，从集合的角度考虑：
task的集合中的元素，job是task的（不相交？）子集，cluster是task的全集。
（似乎要求所有job的交为空，所有job的补为全集，也就是似乎不能越过job直接到taks(?)）
如下图：
_A TensorFlow "cluster" is a set of "tasks" that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow "server", which contains a "master" that can be used to create sessions, and a "worker" that executes operations in the graph.</description></item><item><title>tensorflow Supervisor 学习笔记</title><link>https://111qqz.com/2017/08/tensorflow-supervisor-notes/</link><pubDate>Fri, 04 Aug 2017 09:22:44 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-supervisor-notes/</guid><description>
update:supervisor的缺点是遇到问题只会抛异常，所以现在有一个better的管理工具,MonitoredSession
master,chief worker,Supervisor 这几个概念有点搞不清（我最菜.jpg 因此来学习一下。
概述 原生的tensorflow 是各种东西都需要自己手动，如果是小规模的训练问题倒是不大，但是如果是训练的数据量比较大，可能需要训练几天或者几个月。。。
那原生的tensorflow的健壮性可能就比较堪忧。。。
万一断电了之类。。。
这时候我们就可以使用supervisor
其主要提供下面三个功能，以增强训练的健壮性：
* Handles shutdowns and crashes cleanly. * Can be resumed after a shutdown or a crash. * Can be monitored through TensorBoard. supervisor可以看做一个工具，或者说是对原生tensorflow的一层封装，目的主要是通过定期save的方法增强训练健壮性，
就算程序挂掉了也可以从上一次save的checkpoint恢复，而不是从头再来（虽然这些也可以手动实现（？）
同时也可以简化代码量
除了supervisor,还有tf.learn库，里面提供对原生tensorflow更高层的封装，也提供更丰富的功能。
实例 来举个具体的例子好了：
在不使用supervisor的时候，我们的训练代码如下：
variables ... ops ... summary_op ... merge_all_summarie saver init_op with tf.Session() as sess: writer = tf.tf.train.SummaryWriter() sess.run(init) saver.restore() for ...: train merged_summary = sess.run(merge_all_summarie) writer.add_summary(merged_summary,i) saver.save 如果使用supervisor，代码如下：</description></item><item><title>k-means clustering 学习笔记</title><link>https://111qqz.com/2017/08/k-means-clustering-notes/</link><pubDate>Thu, 03 Aug 2017 13:09:17 +0000</pubDate><guid>https://111qqz.com/2017/08/k-means-clustering-notes/</guid><description>
其实这算法巨简单。。。。让我想到了均分纸牌（noip200?
还是大致说一下：
对于有 features 但是 **没有 **labels 的数据，没办法用监督学习，但是可以使用非监督学习的聚类算法。
所谓聚类，简单理解，就是把相似的分成一组。。。
k-means就是一个常见的聚类算法。。。
k代表可以把数据分成k组。
举一个平面上二维点的例子，算法步骤如下：
1. 随机k个点当做k个点作为k组的中心。 2. 根据现在的k个中心，将数据集中的点，按照【距离哪个中心最近就属于哪个中心】的原则，分组。 3. 在每一个组内求点的二维平均数，作为新的中心。**如果存在一个组的数据中心改变，那么返回2，否则结束**。![](http://stanford.edu/~cpiech/cs221/img/kmeansViz.png) 可以很容易推广到高维度，就只是求平均数和算距离的时候有区别。
一般化的流程：
然后该算法是Expectation Maximization的一个特例
该算法和KNN算法没有半毛钱关系。。。
参考资料：
k means 维基百科
CS221-kmeans</description></item><item><title>TensorFlow Architecture 学习笔记（二）Adding a New Op</title><link>https://111qqz.com/2017/08/tensorflow-architecture-notes-2/</link><pubDate>Wed, 02 Aug 2017 03:07:37 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-architecture-notes-2/</guid><description>
Adding a New Op * [目录](https://www.tensorflow.org/extend/adding_an_op#top_of_page) * [定义运算的接口](https://www.tensorflow.org/extend/adding_an_op#define_the_ops_interface) * [实现运算的核心部分(kernels)](https://www.tensorflow.org/extend/adding_an_op#implement_the_kernel_for_the_op) * [多线程cpu kernels](https://www.tensorflow.org/extend/adding_an_op#multi-threaded_cpu_kernels) * [GPU kernels](https://www.tensorflow.org/extend/adding_an_op#gpu_kernels) * [构建运算库](https://www.tensorflow.org/extend/adding_an_op#build_the_op_library) * [用系统编译器编译你的运算（TensorFlow binary installation）](https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation) * [使用bazel编译你的运算(TensorFlow source installation)](https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_bazel_tensorflow_source_installation) * [在 Python 中使用你的运算](https://www.tensorflow.org/extend/adding_an_op#use_the_op_in_python) * [验证你添加的运算可以工作](https://www.tensorflow.org/extend/adding_an_op#verify_that_the_op_works) * [在你的运算中添加高级特性](https://www.tensorflow.org/extend/adding_an_op#building_advanced_features_into_your_op) * [条件检查和验证](https://www.tensorflow.org/extend/adding_an_op#conditional_checks_and_validation) * [Op registration](https://www.tensorflow.org/extend/adding_an_op#op_registration) * [GPU Support](https://www.tensorflow.org/extend/adding_an_op#gpu_support) * [用python 实现梯度](https://www.tensorflow.org/extend/adding_an_op#implement_the_gradient_in_python) * [Shape functions in C++](https://www.tensorflow.org/extend/adding_an_op#shape_functions_in_c) 对于要添加原生tensorflow中没有定义的运算的需求，首先建议在python层面，能不能将需要的op用其他原生的op拼凑起来。
如果不能这样做，或者这样做逻辑很复杂，或者这样做效率比较低的时候，我们才考虑在cpp层面添加一个新的op
去实现你自定义的运算需要如下步骤：
1. 在C++文件中注册该运算。包含参数个数，类型，返回值类型，运算名称等。大概就是C++头文件中函数的定义吧.同时在此处也要定义shape function 2. 用C++实现该运算，运算的实现被称之为kernel，该实现与第一步中的注册对应，相同功能的op对于不同的输入输出类型，或者是架构（GPU,CPU)可能 有不同的实现. 3. 创建一个python包装器（**可选**），该包装器使得可以在python层面用API对该运算进行调用。在运算注册阶段会生成默认的包装器 4. 编写计算该运算梯度 的函数**（可选）** 5. 测试你新添加的运算。为了方便一般在python层面进行测试，不过你想在C++层面测试也没什么问题。 Define the op's interface 直接看代码好了.</description></item><item><title>TensorFlow Architecture 学习笔记（一）</title><link>https://111qqz.com/2017/08/tensorflow-architecture-notes-1/</link><pubDate>Tue, 01 Aug 2017 03:01:12 +0000</pubDate><guid>https://111qqz.com/2017/08/tensorflow-architecture-notes-1/</guid><description>
这篇文章不会涉及tensorflow的具体使用，而是专注于介绍tensorflow的架构，目的是让开发者能够对tensorflow现有框架进行自定义的扩展。
tensorflow被设计用来处理大规模分布式训练，但是也足够灵活去处理新的machine learning模型或是系统层面的优化。
Overview tensorflow的结构图如下：
从下往上大致上抽象程度越来越高。
其中C API那一层将tensorflow底层的runtime core 封装成不同语言（python,cpp,etc)的用户层代码，并提供相应的接口
这篇文章主要侧重如下layer:
* **Client**: * 将计算定义为数据流图. * 使用 **[session](https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/client/session.py)(**会话)启动图 的执行 * **Distributed Master** * 通过Session.run()中参数的定义，修改图中特定的子图 * 将子图分成多个pieces,使之运行在不同的进程和设备中） * 将得到的pieces分发到 worker services上 * 由worker services 启动graph pieces * **Worker Services** (one for each task) * Schedule the execution of graph operations using kernel implementations appropriate to the available hardware (CPUs, GPUs, etc). * 使用kernel中对于特定硬件设备(cpu,gpu,etc)合适的实现去安全图中操作的执行。 * 从其他worker service 处发送或接收运算结果 * **Kernel Implementations** * 完成各个操作的计算 client,master,worker的关系如下</description></item><item><title>Long Short-Term Memory （LSTM） 网络 学习笔记</title><link>https://111qqz.com/2017/07/lstm-notes/</link><pubDate>Mon, 31 Jul 2017 10:05:01 +0000</pubDate><guid>https://111qqz.com/2017/07/lstm-notes/</guid><description>
参考资料： 维基百科_长短期记忆(LSTM)
Understanding LSTM Networks
[译] 理解 LSTM 网络
LSTM笔记
翻译的比较一般，建议看原文....比如cell还是不要翻译成【细胞】比较好吧...让人以为和生物学的【细胞】有什么关系呢orz
说下我自己的理解：
LSTM是一种特殊的RNN，所谓RNN,也就是循环神经网络，对之前的信息存在“记忆”，可以解决带有时序性的问题。
所谓时序性的问题，简单理解就是，当前的结果依赖于之前的信息。
比如“我来自内蒙古，我能讲一口流利的____” 横线处大概率填写“蒙语”，这是因为前面的信息“内蒙古”
LSTM的全称是long short term memory, LSTM默认就可以记住长期信息，从而实现信息的持久化。
LSTM的本质是一种构造法，通过特定的设计完成信息的持久化。
LSTM有如下结构：
1、cell单元 最基本的单元，从上一个时间节点到当前时间节点是线性控制的。LSTM能够通过门结构增加或者减少信息。 门结构上有sigmoid层（输出0~1）作用，信息通过乘上一个0~1的来决定能够通过多少信息。 2、forget门 forget门决定有多少历史信息能够通过，这一层通过ht−1ht−1和xtxt决定，ft=sigmoid(w(f)xt+u(i)ht−1)∈[0,1]ft=sigmoid(w(f)xt+u(i)ht−1)∈[0,1]作用到Ct−1Ct−1上。 3、input门和新的cell input门是一个sigmoid层决定需要更新多少信息，新的cell是一个tanh层决定要添加多少信息进入记忆单元cell。分别为it=sigmoid(wixt+u(i)ht−1)it=sigmoid(wixt+u(i)ht−1)，与ct~=tanh(w(c)xt+ucht−1)ct~=tanh(w(c)xt+ucht−1) 4、更新记忆单元 forget门作用在ct−1ct−1上，input门和新的cell结合加入组合成新的记忆单元ct=ft.∗ct−1+it.∗ct~ct=ft.∗ct−1+it.∗ct~ 5、output门 添加sigmoid 的output门决定cell单元的信息有多少输出，而cell上套一个tanh使输出在-1到1之间，ot=sigmoid(w(o)xt+u(o)ht−1)ot=sigmoid(w(o)xt+u(o)ht−1)，ht=ot.∗tanh(ct)ht=ot.∗tanh(ct)。</description></item><item><title>stanford cs 231n:常用激活函数</title><link>https://111qqz.com/2017/07/common-activation-functions/</link><pubDate>Sat, 22 Jul 2017 08:56:08 +0000</pubDate><guid>https://111qqz.com/2017/07/common-activation-functions/</guid><description>
其实我觉得这部分可以直接黑箱。。。直接无脑上Leaky ReLU或者Maxou？不过对这些激活函数的特点有个high-level的了解应该总是没坏处的，只要别太纠结细节就好了把。。 每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数： ————————————————————————————————————————
左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。
————————————————————————————————————————
**Sigmoid。**sigmoid非线性函数的数学公式是 ，函数图像如上图的左边所示。在前一节中已经提到过，它输入实数值并将其“挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和（saturated）的激活（1）。然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：
* _Sigmoid函数饱和使梯度消失_。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。 * _Sigmoid函数的输出不是零中心的_。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在![f=w^Tx+b](http://www.zhihu.com/equation?tex=fwTxb) 中每个元素都 ），那么关于 的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式 而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。
**Tanh。**tanh非线性函数图像如上图右边所示。它将实数值压缩到[-1,1]之间。和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是： 。
————————————————————————————————————————
左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当 时函数值为0。当 函数的斜率为1。右边是从 Krizhevsky等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。
————————————————————————————————————————
**ReLU。**在近些年ReLU变得非常流行。它的函数公式是 。换句话说，这个激活函数就是一个关于0的阈值（如上图左侧）。使用ReLU有以下一些优缺点：
* 优点：相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（ [Krizhevsky ](http://link.zhihu.com/?target=http//www.cs.toronto.edu/fritz/absps/imagenet.pdf)等的论文指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的。 * 优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。 * 缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。 **Leaky ReLU。**Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x&amp;lt;0时，函数值为0。而Leaky ReLU则是给出一个很小的负数梯度值，比如0.01。所以其函数公式为 其中 是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。Kaiming He等人在2015年发布的论文Delving Deep into Rectifiers中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。
**Maxout。**一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用 函数形式。一个相关的流行选择是Maxout（最近由Goodfellow等发布）神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是： 。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当 的时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。
以上就是一些常用的神经元及其激活函数。最后需要注意一点：在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。
一句话：“那么该用那种呢？”用ReLU非线性函数。注意设置好学习率，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。</description></item><item><title>how to copy &amp; modify nets model on tensorflow slim</title><link>https://111qqz.com/2017/07/how-to-copy-modify-nets-model-on-tensorflow-slim/</link><pubDate>Wed, 19 Jul 2017 06:21:40 +0000</pubDate><guid>https://111qqz.com/2017/07/how-to-copy-modify-nets-model-on-tensorflow-slim/</guid><description>
想要修改tensorflow-slim 中 nets中的某个model,例如明明为kk_v2.py
观察到train_image_classifier.py中调用模型的部分
network_fn = nets_factory.get_network_fn( FLAGS.model_name, num_classes=(dataset.num_classes - FLAGS.labels_offset), weight_decay=FLAGS.weight_decay, is_training=True) 调用了nets_factory.get_network_fn，get_network如下：
def get_network_fn(name, num_classes, weight_decay=0.0, is_training=False): &amp;quot;&amp;quot;&amp;quot;Returns a network_fn such as `logits, end_points = network_fn(images)`. Args: name: The name of the network. num_classes: The number of classes to use for classification. weight_decay: The l2 coefficient for the model weights. is_training: `True` if the model is being used for training and `False` otherwise. Returns: network_fn: A function that applies the model to a batch of images.</description></item><item><title>Inception-v4,Inception-ResNet 和残差连接对学习的影响</title><link>https://111qqz.com/2017/07/inception-resnet-notes/</link><pubDate>Tue, 18 Jul 2017 02:42:50 +0000</pubDate><guid>https://111qqz.com/2017/07/inception-resnet-notes/</guid><description>
原始论文
翻译链接
**——前言：**作者认为残差连接在训练深度卷积模型是很有必要的。至少在图像识别上，我们的研究似乎并不支持这一观点。 摘要： 近年来，深度卷积神经网络对图像识别性能的巨大提升发挥着关键作用。以Inception网络为例，其以相对较低的计算代价取得出色的表现。最近，与传统结构相结合的残差连接网络在2015ILSVRC挑战赛上取得非常优异的成绩；它的性能跟最新的Inception-v3 网络非常接近。因此也就引出了结合残差连接的Inception结构能否对性能进行提高的问题。本文给出实验证明，残差连接可以明显加速Inception网络的训练。同时实验也证明，相比没有残差连接的消耗相似的Inception网络，残差Inception网络在性能上具有微弱的优势。针对是否包含残差连接的Inception网络，本文同时提出了一些新的简化网络。这些网络的变体在ILSVRC2012分类任务上很明显的改善了单一框架的识别性能。本文进一步展示了适当的激活缩放如何使得很宽的残差Inception网络的训练更加稳定。本文通过对三个残差和一个Inception-v4进行组合，在top-5错误率上达到了 3.08%。
前言： 自从Krizhevsky等人于2012年赢得Image Net比赛，其网络“AlexNet”已在越来越多的机器视觉任务中得到成功应用，比如目标检测、分割、人体姿态估计、视频分类、目标跟踪、超分辨率等。这些都是使用深度卷积网络的成功案例。 本研究结合最近的两个想法：残差连接和最近的Inception网络结构除了直接的融合，我们也研究了Inception本身通过变得更深更宽能否能变得更加高效。为了实现这个目的，我们设计了一个新版本的Inception-v4，相比Inception-v3，它有更加统一简化的网络结构和更多的inception模块。从历史观点来看，Inception-v3继承了之前的很多方法。技术性局限主要在于使用DistBelief对分布式训练进行模型划分。 如今，将训练迁移到TensorFlow上以后，这些问题也就随之解决，这样就允许我们对结构进行简化。简化的网络结构详见第三节。 在本文中，我们将两个单一Inception变体——Inception-v3和v4与消耗相似的 InceptionResNet混合版本进行比较。这些模型的挑选主要满足以下约束条件，即和非残差模型具有相似的参数和计算复杂度。事实上我们对更深更宽的Inception-ResNet变体也进行测试，它们在ImageNet分类任务上表现性能相似。 最新的实验对组合模型的性能进行了评估。结果显示Inception-v4和Inception-ResNetv2的性能都很好，在ImageNet验证集上其性能已超过业界领先的单个框架模型，我们想看这种结合如何将业界领先水准继续推进，令人惊讶的是，我们发现单个框架性能的提升不会引起组合性能大幅的提高。尽管如此，我们仍然用四个模型组合在验证集上取得了top-5上3.1%的错误率。 在最后一部分，我们分析了分类任务失败的原因，并总结出组合模型在标注数据上的类标噪声上仍然没有达到很好的效果，同时对于预测还有很有大的提升空间。
近期工作： 卷积网络在大规模图像识别任务上的运用非常广泛。主要的模型有Network-in-network、VGGNet、GoogleLeNet(Inception-v1)。残差连接在引文5中提出，并指出附加残差网络对于图像识别尤其是目标检测具有很大的优势，并给出理论和实验验证。作者认为残差连接在训练深度卷积模型是很有必要的。至少在图像识别上，我们的研究似乎并不支持这一观点。然而，残差连接所带来的潜在优势可能需要在更深网络结构中来展现。在实验部分，我们展示了不使用残差连接时深度网络的训练并不难做到。然而，使用残差连接能够极大的提高训练速度，单单这一点就值的肯定。 Inception深度卷积网络被称为GoogleLeNet或Incention-v1。继而我们通过各种方法对 Inception结构进行优化，首先引入batch normalization(Inception-v2)。后来在第三代中增加factorization因子，即本文中提到的Inception-v3。
图1.残差连接
图2.优化版本的ResNet连接 3.结构选择 3.1纯净的Inception模块 我们对以前的Inception模型通过分布式进行训练，将每个副本被划分成一个含多个子网络的模型，以达到在内存中对整个模型进行拟合的目的。然而，Inception结构是高度可调的，这就意味着各层滤波器（filter）的数量可以有多种变化，而整个训练网络的质量不会受到影响。为了优化训练速度，我们对层大小进行调整以平衡不同子网络的计算。 相反，随着TensorFlow的引入，大部分最新的模型无需分布式的对副本进行训练。它通过反向传播（back propagation）进行内存优化，并仔细考虑梯度计算需要的tensors，以及通过结构化计算减少这类tensors的数量。从历史观点来讲，我们对网络结构的更迭已经做得非常保守，并限制实验改变独立网络的组分，同时保持其余网络的稳定性。 由于之前没有对网络进行简化，导致网络看起来更加复杂。在最新的实验中，针对 Inception-v4网络，我们决定丢掉不必要的包袱，对于inception块的每个网格大小进行统一。如图9，展示了大尺寸的Inceptionv4网络结构。图3至8是每个部分的详细结构。所有图中没有标记“V”的卷积使用same的填充原则，意即其输出网格与输入的尺寸正好匹配。使用“V”标记的卷积使用valid的填充原则，意即每个单元输入块全部包含在前几层中，同时输出激活图（output activation map）的网格尺寸也相应会减少。
图3 Inception-v4网络和Inception-ResNet-v2网络的结构。这是网络的输入部分。
图 4 Inception-v4网络35×35网格的框架。对应图9中Inception-A块。
图 5 Incep-v4网络17×17网格块的框架。对应图9中Inception-B块。
图 6 Inception-v4网络的8×8网格模块的框架。对应图9中Inception-C块。
图7 35×35到17×17减少模块的框架。这个块不同的变化（不同滤波器）在图9和15中使用，同时在网络Inception(-v4,-ResNet-v1,-ResNet-v2).k,l,m,n数量表示滤波器尺寸大小，如表1所示。
图 8 17×17到8×8网格缩减框架。减少的模块在图9中Inception-v4网络。</description></item><item><title>stanford CS231n notes：Linear classification</title><link>https://111qqz.com/2017/07/cs231n-linear-classification/</link><pubDate>Mon, 17 Jul 2017 02:02:43 +0000</pubDate><guid>https://111qqz.com/2017/07/cs231n-linear-classification/</guid><description>
课程链接 知乎翻译链接
之前看的原版，后来发现知乎上有翻译，正好想到之前看完没有整理总结，干脆就写一下自己的理解，顺便贴一下课程翻译（感觉翻译的质量好像还可以？
分类器就是一个函数,自变量是图像信息，因变量是类别信息。
比如线性分类器，SVM,softmax
不同的分类器有着不同的score function,对应着不同的cost function.
之所以选择不同的cost function的原因是，要保证cost funtion是凸函数，不然会存在很多局部极值。
分类器使得分类问题变成了一个优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。
然后，所谓overfit,就是参数太多而训练集太小，导致可以完美符合这些训练集，但是无法一般化。
解决overfit有很多方法，这里介绍了正则化（Regularization）
思路是在cost funtion中添加一项Regularization loss
,用作对参数值的惩罚，lambda为惩罚因子
这样就可以使得每个参数尽可能小，这样每个参数对于cost function的贡献就比较小，可以改善overfit的问题
原文如下 内容列表：
* 线性分类器简介 * 线性评分函数 * 阐明线性分类器 _**译者注：上篇翻译截止处**_ * 损失函数 * 多类SVM * Softmax分类器 * SVM和Softmax的比较 * 基于Web的可交互线性分类器原型 * 小结 线性分类 上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。我们还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：
* 分类器必须_记住_所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以GB计。 * 对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。 概述：我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是评分函数（score function），它是原始图像数据到类别分值的映射。另一个是损失函数（loss function），它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。
从图像到标签分值的参数化映射 该方法的第一部分就是定义一个评分函数，这个函数将图像的像素值映射为各个分类类别的得分，得分高低代表图像属于该类别的可能性高低。下面会利用一个具体例子来展示该方法。现在假设有一个包含很多图像的训练集 ，每个图像都有一个对应的分类标签 。这里 并且 。这就是说，我们有N个图像样例，每个图像的维度是D，共有K种不同的分类。</description></item><item><title>tensorflow slim 源码分析</title><link>https://111qqz.com/2017/07/tensorflow-slim-code-notes/</link><pubDate>Sun, 16 Jul 2017 13:10:04 +0000</pubDate><guid>https://111qqz.com/2017/07/tensorflow-slim-code-notes/</guid><description>
py的源码看起来还是很愉快的。。。（虽然熟练成程度完全不如cpp。。。。
datasets里是数据集相关
deployment是部署相关
nets里给了很多网络结构
preprocessing给了几种预处理的方式
这些都和slim没有太大关系，就不多废话了。
分析的部分见代码注释...
由于刚刚入门machine learning 一周...还有很多内容还没有从理论层面接触...所以源码的理解也十分有限...希望能以后有机会补充一波
1 # Copyright 2016 The TensorFlow Authors. All Rights Reserved. 2 # 3 # Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;); 4 # you may not use this file except in compliance with the License. 5 # You may obtain a copy of the License at 6 # 7 # http://www.apache.org/licenses/LICENSE-2.0 8 # 9 # Unless required by applicable law or agreed to in writing, software 10 # distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS, 11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</description></item><item><title>几种梯度下降(GD)法的比较（转载）</title><link>https://111qqz.com/2017/07/Gradient-descent-methods/</link><pubDate>Mon, 10 Jul 2017 01:49:04 +0000</pubDate><guid>https://111qqz.com/2017/07/Gradient-descent-methods/</guid><description>
参考资料
机器学习中梯度下降（Gradient Descent， GD）算法只需要计算损失函数的一阶导数，计算代价小，非常适合训练数据非常大的应用。
梯度下降法的物理意义很好理解，就是沿着当前点的梯度方向进行线搜索，找到下一个迭代点。但是，为什么有会派生出 batch、mini-batch、online这些GD算法呢？
原来，batch、mini-batch、SGD、online的区别在于训练数据的选择上：
** ** **batch** **mini-batch** **Stochastic** **Online** **训练集** 固定 固定 固定 实时更新 **单次迭代样本数** 整个训练集 训练集的子集 单个样本 根据具体算法定 **算法复杂度** 高 一般 低 低 **时效性** 低 一般（delta 模型） 一般（delta 模型） 高 **收敛性** 稳定 较稳定 不稳定 不稳定 1.</description></item><item><title>Deep Learning Tutorial - PCA and Whitening</title><link>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</link><pubDate>Thu, 06 Jul 2017 08:35:51 +0000</pubDate><guid>https://111qqz.com/2017/07/deep-learning-tutorial-pca-and-whitening/</guid><description>
说下我自己的理解
PCA：主成分分析，是一种预处理手段。对于n维的数据，通过一些手段，把变化显著的k个维度保留，舍弃另外n-k个维度。对于一些非监督学习算法，降低维度可以有效加快运算速度。而n-k个最次要方向的丢失带来的误差不会很大。
PCA的思想是将n维特征映射到k维上（k&amp;lt;n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。
whitening:是一种预处理手段，为了解决数据的冗余问题。比如如果数据是一个16_16的图像，raw data 有16_16=256维度，但是实际上这256个维度不是相互独立的，相邻的像素位置实际上有大关联！
Principal Component Analysis PCA is a method for reducing the number of dimensions in the vectors in a dataset. Essentially, you’re compressing the data by exploiting correlations between some of the dimensions.
Covariance Matrix PCA starts with computing the covariance matrix. I found this tutorial helpful for getting a basic understanding of covariance matrices (I only read a little bit of it to get the basic idea).</description></item><item><title>文本相似度判断-simhash算法学习笔记</title><link>https://111qqz.com/2017/03/simhash/</link><pubDate>Fri, 10 Mar 2017 03:33:08 +0000</pubDate><guid>https://111qqz.com/2017/03/simhash/</guid><description>
先放原始论文。。。以此表达对这个算法的敬意orz
论文链接
问题引出： 那天百度一面，frog学姐问了我如何判断两篇新闻稿的相似度的问题....我满篇口胡...也只是回答了一些诸如从图片上考虑。。或者去掉stop word之后得到特征向量然后计算余弦值之类得到传统想法。。。
今天看到了google在用的网页去重的算法（？。。。感觉好神奇。。。准备面试到现在，第一个让我感到惊异而不是套路的算法orz
对于处理**大规模文本（500字以上吧）**的时候效果很好。。。但是算法思想却又非常简单。
这才是算法的美丽之处吧。。。。leetcode上的那些纱布技巧也好意思叫算法。。。？
网页去重，其实本质还是网页相似度的计算....首先是两篇，之后还可以推广到海量数据。
算法初探： simhash算法。。。字面上也可以看出。。是一种hash算法。。。那么它和一般的hash有什么不同呢？
最大的问题在于。。。传统hash的设计目的之一是使得映射后的值的分布尽可能均匀...对于同样的key会有同样的value,但是每当key有轻微的变化的时候，value就会千差万别。
举个例子：
“你妈妈喊你回家吃饭哦，回家罗回家罗” 和 “你妈妈叫你回家吃饭啦，回家罗回家罗”。 通过simhash计算结果为：
1000010010101101111111100000101011010001001111100001001011001011
1000010010101101011111100000101011010001001111100001101010001011
通过 hashcode计算为：
1111111111111111111111111111111110001000001100110100111011011110
1010010001111111110010110011101
也就是说。。。没办法通过hash之后得到的值的差异，去分析key的相似程度。
而simhash就是通过某种方法进行hash，使得hash之后得到的value可以反应key的相似度。
流程 simhash算法分为5个步骤：分词、hash、加权、合并、降维，具体过程如下所述： * 分词 * 给定一段语句，进行分词，得到有效的特征向量，然后为每一个特征向量设置1-5等5个级别的权重（如果是给定一个文本，那么特征向量可以是文本中的词，其权重可以是这个词出现的次数）。例如给定一段语句：“CSDN博客结构之法算法之道的作者July”，分词后为：“CSDN 博客 结构 之 法 算法 之 道 的 作者 July”，然后为每个特征向量赋予权值：CSDN(4) 博客(5) 结构(3) 之(1) 法(2) 算法(3) 之(1) 道(2) 的(1) 作者(5) July(5)，其中括号里的数字代表这个单词在整条语句中的重要程度，数字越大代表越重要。 * hash * 通过hash函数计算各个特征向量的hash值，hash值为二进制数01组成的n-bit签名。比如“CSDN”的hash值Hash(CSDN)为100101，“博客”的hash值Hash(博客)为“101011”。就这样，字符串就变成了一系列数字。 * 加权 * 在hash值的基础上，给所有特征向量进行加权，即W = Hash * weight，且遇到1则hash值和权值正相乘，遇到0则hash值和权值负相乘。例如给“CSDN”的hash值“100101”加权得到：W(CSDN) = 100101 _4 = 4 -4 -4 4 -4 4，给“博客”的hash值“101011”加权得到：W(博客)=101011 _5 = 5 -5 5 -5 5 5，其余特征向量类似此般操作。 * 合并 * 将上述各个特征向量的加权结果累加，变成只有一个序列串。拿前两个特征向量举例，例如“CSDN”的“4 -4 -4 4 -4 4”和“博客”的“5 -5 5 -5 5 5”进行累加，得到“4+5 -4+-5 -4+5 4+-5 -4+5 4+5”，得到“9 -9 1 -1 1”。 * 降维 * 对于n-bit签名的累加结果，如果大于0则置1，否则置0，从而得到该语句的simhash值，最后我们便可以根据不同语句simhash的海明距离来判断它们的相似度。例如把上面计算出来的“9 -9 1 -1 1 9”降维（某位大于0记为1，小于0记为0），得到的01串为：“1 0 1 0 1 1”，从而形成它们的simhash签名。 每篇文档得到SimHash签名值后，接着计算两个签名的海明距离即可。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高。</description></item></channel></rss>