<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on 111qqz的小窝</title>
    <link>https://111qqz.com/categories/deep-learning/</link>
    <description>Recent content in deep-learning on 111qqz的小窝</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Apr 2018 03:08:19 +0000</lastBuildDate>
    
	<atom:link href="https://111qqz.com/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>caffe2 添加自定义op</title>
      <link>https://111qqz.com/2018/04/caffe2-%e6%b7%bb%e5%8a%a0%e8%87%aa%e5%ae%9a%e4%b9%89op/</link>
      <pubDate>Fri, 13 Apr 2018 03:08:19 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/04/caffe2-%e6%b7%bb%e5%8a%a0%e8%87%aa%e5%ae%9a%e4%b9%89op/</guid>
      <description>记录一些一个没有之前没有接触过caffe/caffe2的人为了添加自定义的op 到caffe2需要做的工作. 首先参考caffe2 tutoria</description>
    </item>
    
    <item>
      <title>caffe2 添加自定义op</title>
      <link>https://111qqz.com/2018/04/caffe2-%e6%b7%bb%e5%8a%a0%e8%87%aa%e5%ae%9a%e4%b9%89op/</link>
      <pubDate>Fri, 13 Apr 2018 03:08:19 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/04/caffe2-%e6%b7%bb%e5%8a%a0%e8%87%aa%e5%ae%9a%e4%b9%89op/</guid>
      <description>记录一些一个没有之前没有接触过caffe/caffe2的人为了添加自定义的op 到caffe2需要做的工作. 首先参考caffe2 tutoria</description>
    </item>
    
    <item>
      <title>非极大值抑制（Non-Maximum Suppression，NMS）</title>
      <link>https://111qqz.com/2018/03/%e9%9d%9e%e6%9e%81%e5%a4%a7%e5%80%bc%e6%8a%91%e5%88%b6%ef%bc%88non-maximum-suppression%ef%bc%8cnms%ef%bc%89/</link>
      <pubDate>Fri, 16 Mar 2018 02:56:14 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/03/%e9%9d%9e%e6%9e%81%e5%a4%a7%e5%80%bc%e6%8a%91%e5%88%b6%ef%bc%88non-maximum-suppression%ef%bc%8cnms%ef%bc%89/</guid>
      <description>NMS是为了在诸多CV任务如边缘检测，目标检测等，找到局部最大值 其主要思想是先设定一个阈值，然后计算检测框的IOU(所谓IOU，也就是int</description>
    </item>
    
    <item>
      <title>非极大值抑制（Non-Maximum Suppression，NMS）</title>
      <link>https://111qqz.com/2018/03/%e9%9d%9e%e6%9e%81%e5%a4%a7%e5%80%bc%e6%8a%91%e5%88%b6%ef%bc%88non-maximum-suppression%ef%bc%8cnms%ef%bc%89/</link>
      <pubDate>Fri, 16 Mar 2018 02:56:14 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/03/%e9%9d%9e%e6%9e%81%e5%a4%a7%e5%80%bc%e6%8a%91%e5%88%b6%ef%bc%88non-maximum-suppression%ef%bc%8cnms%ef%bc%89/</guid>
      <description>NMS是为了在诸多CV任务如边缘检测，目标检测等，找到局部最大值 其主要思想是先设定一个阈值，然后计算检测框的IOU(所谓IOU，也就是int</description>
    </item>
    
    <item>
      <title>reid 相关任务记录</title>
      <link>https://111qqz.com/2018/02/reid-%e7%9b%b8%e5%85%b3%e4%bb%bb%e5%8a%a1%e8%ae%b0%e5%bd%95/</link>
      <pubDate>Sat, 24 Feb 2018 04:34:02 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/reid-%e7%9b%b8%e5%85%b3%e4%bb%bb%e5%8a%a1%e8%ae%b0%e5%bd%95/</guid>
      <description>被师兄（同事？）普及了一番实验规范orz&amp;hellip; 我还是太年轻了 所谓的一个fc的版本是右边的．一个放着不动，另一个在sequence_</description>
    </item>
    
    <item>
      <title>reid 相关任务记录</title>
      <link>https://111qqz.com/2018/02/reid-%e7%9b%b8%e5%85%b3%e4%bb%bb%e5%8a%a1%e8%ae%b0%e5%bd%95/</link>
      <pubDate>Sat, 24 Feb 2018 04:34:02 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/reid-%e7%9b%b8%e5%85%b3%e4%bb%bb%e5%8a%a1%e8%ae%b0%e5%bd%95/</guid>
      <description>被师兄（同事？）普及了一番实验规范orz&amp;hellip; 我还是太年轻了 所谓的一个fc的版本是右边的．一个放着不动，另一个在sequence_</description>
    </item>
    
    <item>
      <title>分类评价指标之Cumulative Match Characteristi (CMC)曲线</title>
      <link>https://111qqz.com/2018/02/%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87%e4%b9%8bcumulative-match-characteristi-cmc%e6%9b%b2%e7%ba%bf/</link>
      <pubDate>Fri, 23 Feb 2018 08:20:55 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87%e4%b9%8bcumulative-match-characteristi-cmc%e6%9b%b2%e7%ba%bf/</guid>
      <description>CMC曲线全称是Cumulative Match Characteristic (CMC) curve，也就是累积匹配曲线，同ROC曲线Receiver Operating Characteristic (ROC) curve一样，是模式识别系统，</description>
    </item>
    
    <item>
      <title>分类评价指标之Cumulative Match Characteristi (CMC)曲线</title>
      <link>https://111qqz.com/2018/02/%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87%e4%b9%8bcumulative-match-characteristi-cmc%e6%9b%b2%e7%ba%bf/</link>
      <pubDate>Fri, 23 Feb 2018 08:20:55 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87%e4%b9%8bcumulative-match-characteristi-cmc%e6%9b%b2%e7%ba%bf/</guid>
      <description>CMC曲线全称是Cumulative Match Characteristic (CMC) curve，也就是累积匹配曲线，同ROC曲线Receiver Operating Characteristic (ROC) curve一样，是模式识别系统，</description>
    </item>
    
    <item>
      <title>Reid iLIDS-VID 数据集 切分</title>
      <link>https://111qqz.com/2018/02/reid-ilids-vid-%e6%95%b0%e6%8d%ae%e9%9b%86-%e5%88%87%e5%88%86/</link>
      <pubDate>Thu, 22 Feb 2018 08:12:58 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/reid-ilids-vid-%e6%95%b0%e6%8d%ae%e9%9b%86-%e5%88%87%e5%88%86/</guid>
      <description>ilids-vid数据集 150个人做trainning,150个人做 在training set里面，每个人有来自两个不同camera的两段vi</description>
    </item>
    
    <item>
      <title>Reid iLIDS-VID 数据集 切分</title>
      <link>https://111qqz.com/2018/02/reid-ilids-vid-%e6%95%b0%e6%8d%ae%e9%9b%86-%e5%88%87%e5%88%86/</link>
      <pubDate>Thu, 22 Feb 2018 08:12:58 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/reid-ilids-vid-%e6%95%b0%e6%8d%ae%e9%9b%86-%e5%88%87%e5%88%86/</guid>
      <description>ilids-vid数据集 150个人做trainning,150个人做 在training set里面，每个人有来自两个不同camera的两段vi</description>
    </item>
    
    <item>
      <title>end-to-end 神经网络</title>
      <link>https://111qqz.com/2018/02/end-to-end-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c/</link>
      <pubDate>Thu, 22 Feb 2018 02:53:01 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/end-to-end-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c/</guid>
      <description>所谓end-to-end 神经网络，更多是一种思想。 这种思想的核心是，比如对于图像处理，输入原始图像数据，输出的是直接有用的结果（有用取决于具</description>
    </item>
    
    <item>
      <title>end-to-end 神经网络</title>
      <link>https://111qqz.com/2018/02/end-to-end-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c/</link>
      <pubDate>Thu, 22 Feb 2018 02:53:01 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/end-to-end-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c/</guid>
      <description>所谓end-to-end 神经网络，更多是一种思想。 这种思想的核心是，比如对于图像处理，输入原始图像数据，输出的是直接有用的结果（有用取决于具</description>
    </item>
    
    <item>
      <title>Pose-driven Deep Convolutional Model for Person Re-identification 阅读笔记</title>
      <link>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 22 Feb 2018 02:25:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>1709.08325 Reid问题指的是判断一个probe person 是否在被不同的camera捕获的gallery person 中出现。 通常是如下情景：给出一个特定camera下某</description>
    </item>
    
    <item>
      <title>Pose-driven Deep Convolutional Model for Person Re-identification 阅读笔记</title>
      <link>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 22 Feb 2018 02:25:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/pose-driven-deep-convolutional-model-for-person-re-identification-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>1709.08325 Reid问题指的是判断一个probe person 是否在被不同的camera捕获的gallery person 中出现。 通常是如下情景：给出一个特定camera下某</description>
    </item>
    
    <item>
      <title>Deep Mutual Learning（相互学习） 阅读笔记</title>
      <link>https://111qqz.com/2018/02/deep-mutual-learning-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sun, 18 Feb 2018 10:46:35 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/deep-mutual-learning-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>原始论文 DNN在很多问题上效果很不错，但是由于深度和宽度过大，导致需要的执行时间和内存过大。我们需要讨论一些能快速执行并且对内存的需要不大的</description>
    </item>
    
    <item>
      <title>Deep Mutual Learning（相互学习） 阅读笔记</title>
      <link>https://111qqz.com/2018/02/deep-mutual-learning-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sun, 18 Feb 2018 10:46:35 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/deep-mutual-learning-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>原始论文 DNN在很多问题上效果很不错，但是由于深度和宽度过大，导致需要的执行时间和内存过大。我们需要讨论一些能快速执行并且对内存的需要不大的</description>
    </item>
    
    <item>
      <title>Similarity learning 和Metric learning</title>
      <link>https://111qqz.com/2018/02/similarity-learning-%e5%92%8cmetric-learning/</link>
      <pubDate>Sun, 18 Feb 2018 08:14:10 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/similarity-learning-%e5%92%8cmetric-learning/</guid>
      <description>Similarity_learning 相似性学习（Similarity learning ）有监督机器学习，它与回归和分类密切相关，但目标是从实例中学习一个相似函数，以衡量两个对象的相似程度或相</description>
    </item>
    
    <item>
      <title>Similarity learning 和Metric learning</title>
      <link>https://111qqz.com/2018/02/similarity-learning-%e5%92%8cmetric-learning/</link>
      <pubDate>Sun, 18 Feb 2018 08:14:10 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/similarity-learning-%e5%92%8cmetric-learning/</guid>
      <description>Similarity_learning 相似性学习（Similarity learning ）有监督机器学习，它与回归和分类密切相关，但目标是从实例中学习一个相似函数，以衡量两个对象的相似程度或相</description>
    </item>
    
    <item>
      <title>persion reid 论文列表</title>
      <link>https://111qqz.com/2018/02/persion-reid-%e8%ae%ba%e6%96%87%e5%88%97%e8%a1%a8/</link>
      <pubDate>Sat, 17 Feb 2018 07:17:49 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/persion-reid-%e8%ae%ba%e6%96%87%e5%88%97%e8%a1%a8/</guid>
      <description>Key: (1). Pose-driven, body part alignment, combine whole feature and body part feature, focus on alignment of part model, (2). Combine image label and human attributes classes, do classification with attributes and identity learning (3). Based on triplet loss, improve metric learning for an end to end learning (4). Post-process, re-ranking AlignedReID: Surpassing Human-Level Performance in Person Re-Identification Hydraplus-net: Attentive deep features for pedestrian analysis. Darkrank: Accelerating deep metric learning</description>
    </item>
    
    <item>
      <title>persion reid 论文列表</title>
      <link>https://111qqz.com/2018/02/persion-reid-%e8%ae%ba%e6%96%87%e5%88%97%e8%a1%a8/</link>
      <pubDate>Sat, 17 Feb 2018 07:17:49 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/persion-reid-%e8%ae%ba%e6%96%87%e5%88%97%e8%a1%a8/</guid>
      <description>Key: (1). Pose-driven, body part alignment, combine whole feature and body part feature, focus on alignment of part model, (2). Combine image label and human attributes classes, do classification with attributes and identity learning (3). Based on triplet loss, improve metric learning for an end to end learning (4). Post-process, re-ranking AlignedReID: Surpassing Human-Level Performance in Person Re-Identification Hydraplus-net: Attentive deep features for pedestrian analysis. Darkrank: Accelerating deep metric learning</description>
    </item>
    
    <item>
      <title>CUDA C Best Practices Guide 阅读笔记（二） Heterogeneous Computing</title>
      <link>https://111qqz.com/2018/02/cuda-c-best-practices-guide-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0%ef%bc%88%e4%ba%8c%ef%bc%89-heterogeneous-computing/</link>
      <pubDate>Tue, 13 Feb 2018 06:38:38 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-c-best-practices-guide-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0%ef%bc%88%e4%ba%8c%ef%bc%89-heterogeneous-computing/</guid>
      <description>CUDA 编程涉及到在不同的平台上同时运行代码:包含CPU的host 和包含GPU的device. 所以了解host和device的对性能优化是非常重要</description>
    </item>
    
    <item>
      <title>CUDA C Best Practices Guide 阅读笔记（二） Heterogeneous Computing</title>
      <link>https://111qqz.com/2018/02/cuda-c-best-practices-guide-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0%ef%bc%88%e4%ba%8c%ef%bc%89-heterogeneous-computing/</link>
      <pubDate>Tue, 13 Feb 2018 06:38:38 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-c-best-practices-guide-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0%ef%bc%88%e4%ba%8c%ef%bc%89-heterogeneous-computing/</guid>
      <description>CUDA 编程涉及到在不同的平台上同时运行代码:包含CPU的host 和包含GPU的device. 所以了解host和device的对性能优化是非常重要</description>
    </item>
    
    <item>
      <title>CMake Error at gpuxgboost_generated_updater_gpu_hist_experimental.cu.obj.Release.cmake:282  的解决办法</title>
      <link>https://111qqz.com/2018/02/cmake-error-at-gpuxgboost_generated_updater_gpu_hist_experimental-cu-obj-release-cmake282-%e7%9a%84%e8%a7%a3%e5%86%b3%e5%8a%9e%e6%b3%95/</link>
      <pubDate>Mon, 12 Feb 2018 07:19:45 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cmake-error-at-gpuxgboost_generated_updater_gpu_hist_experimental-cu-obj-release-cmake282-%e7%9a%84%e8%a7%a3%e5%86%b3%e5%8a%9e%e6%b3%95/</guid>
      <description>请教了同事&amp;hellip;果然是身经百战见得多啊 直接告诉我cuda 8.0.27 对应版本的thrust有bug. 解决办法是从 thrust 的github搞一份最新的</description>
    </item>
    
    <item>
      <title>CMake Error at gpuxgboost_generated_updater_gpu_hist_experimental.cu.obj.Release.cmake:282  的解决办法</title>
      <link>https://111qqz.com/2018/02/cmake-error-at-gpuxgboost_generated_updater_gpu_hist_experimental-cu-obj-release-cmake282-%e7%9a%84%e8%a7%a3%e5%86%b3%e5%8a%9e%e6%b3%95/</link>
      <pubDate>Mon, 12 Feb 2018 07:19:45 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cmake-error-at-gpuxgboost_generated_updater_gpu_hist_experimental-cu-obj-release-cmake282-%e7%9a%84%e8%a7%a3%e5%86%b3%e5%8a%9e%e6%b3%95/</guid>
      <description>请教了同事&amp;hellip;果然是身经百战见得多啊 直接告诉我cuda 8.0.27 对应版本的thrust有bug. 解决办法是从 thrust 的github搞一份最新的</description>
    </item>
    
    <item>
      <title>CUDA C Best Practices Guide 阅读笔记（1） 并行计算方法论(APOD)</title>
      <link>https://111qqz.com/2018/02/%e5%b9%b6%e8%a1%8c%e8%ae%a1%e7%ae%97%e6%96%b9%e6%b3%95%e8%ae%ba%ef%bc%88%e4%b8%80%ef%bc%89/</link>
      <pubDate>Mon, 12 Feb 2018 04:58:31 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%b9%b6%e8%a1%8c%e8%ae%a1%e7%ae%97%e6%96%b9%e6%b3%95%e8%ae%ba%ef%bc%88%e4%b8%80%ef%bc%89/</guid>
      <description>APOD指的是Assess, Parallelize, Optimize, Deploy 如图所示，APOD过程是一个循环的过程，每次只进行一部分，从A到P到O到D,然后再进行下一轮的APOD Assess 对</description>
    </item>
    
    <item>
      <title>CUDA C Best Practices Guide 阅读笔记（1） 并行计算方法论(APOD)</title>
      <link>https://111qqz.com/2018/02/%e5%b9%b6%e8%a1%8c%e8%ae%a1%e7%ae%97%e6%96%b9%e6%b3%95%e8%ae%ba%ef%bc%88%e4%b8%80%ef%bc%89/</link>
      <pubDate>Mon, 12 Feb 2018 04:58:31 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%b9%b6%e8%a1%8c%e8%ae%a1%e7%ae%97%e6%96%b9%e6%b3%95%e8%ae%ba%ef%bc%88%e4%b8%80%ef%bc%89/</guid>
      <description>APOD指的是Assess, Parallelize, Optimize, Deploy 如图所示，APOD过程是一个循环的过程，每次只进行一部分，从A到P到O到D,然后再进行下一轮的APOD Assess 对</description>
    </item>
    
    <item>
      <title>CUDA 7.5: 用指令级性能分析精确找到性能问题</title>
      <link>https://111qqz.com/2018/02/cuda-7-5-%e7%94%a8%e6%8c%87%e4%bb%a4%e7%ba%a7%e6%80%a7%e8%83%bd%e5%88%86%e6%9e%90%e7%b2%be%e7%a1%ae%e6%89%be%e5%88%b0%e6%80%a7%e8%83%bd%e9%97%ae%e9%a2%98/</link>
      <pubDate>Sun, 11 Feb 2018 08:06:14 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-7-5-%e7%94%a8%e6%8c%87%e4%bb%a4%e7%ba%a7%e6%80%a7%e8%83%bd%e5%88%86%e6%9e%90%e7%b2%be%e7%a1%ae%e6%89%be%e5%88%b0%e6%80%a7%e8%83%bd%e9%97%ae%e9%a2%98/</guid>
      <description>原文： CUDA 7.5: Pinpoint Performance Problems with Instruction-Level Profiling 主要是介绍了CUDA 7.5 以上的版本的 NVIDIA Visual Profiler 加入的新特性 可以细粒度到指令级，分析出性能的瓶颈（在这之前，只能分析到kern</description>
    </item>
    
    <item>
      <title>CUDA 7.5: 用指令级性能分析精确找到性能问题</title>
      <link>https://111qqz.com/2018/02/cuda-7-5-%e7%94%a8%e6%8c%87%e4%bb%a4%e7%ba%a7%e6%80%a7%e8%83%bd%e5%88%86%e6%9e%90%e7%b2%be%e7%a1%ae%e6%89%be%e5%88%b0%e6%80%a7%e8%83%bd%e9%97%ae%e9%a2%98/</link>
      <pubDate>Sun, 11 Feb 2018 08:06:14 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-7-5-%e7%94%a8%e6%8c%87%e4%bb%a4%e7%ba%a7%e6%80%a7%e8%83%bd%e5%88%86%e6%9e%90%e7%b2%be%e7%a1%ae%e6%89%be%e5%88%b0%e6%80%a7%e8%83%bd%e9%97%ae%e9%a2%98/</guid>
      <description>原文： CUDA 7.5: Pinpoint Performance Problems with Instruction-Level Profiling 主要是介绍了CUDA 7.5 以上的版本的 NVIDIA Visual Profiler 加入的新特性 可以细粒度到指令级，分析出性能的瓶颈（在这之前，只能分析到kern</description>
    </item>
    
    <item>
      <title>cuda c&#43;&#43;  基础算法库 thrust 学习笔记</title>
      <link>https://111qqz.com/2018/02/cuda-c-%e5%9f%ba%e7%a1%80%e7%ae%97%e6%b3%95%e5%ba%93-thrust-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sat, 10 Feb 2018 08:43:54 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-c-%e5%9f%ba%e7%a1%80%e7%ae%97%e6%b3%95%e5%ba%93-thrust-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>可以了解成并行版的STL(? 过了一遍nvidia的官方网文档 发现如果熟悉STL的话,thrust没什么太多好说的,看起来很简单&amp;hellip</description>
    </item>
    
    <item>
      <title>cuda c&#43;&#43;  基础算法库 thrust 学习笔记</title>
      <link>https://111qqz.com/2018/02/cuda-c-%e5%9f%ba%e7%a1%80%e7%ae%97%e6%b3%95%e5%ba%93-thrust-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Sat, 10 Feb 2018 08:43:54 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-c-%e5%9f%ba%e7%a1%80%e7%ae%97%e6%b3%95%e5%ba%93-thrust-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>可以了解成并行版的STL(? 过了一遍nvidia的官方网文档 发现如果熟悉STL的话,thrust没什么太多好说的,看起来很简单&amp;hellip</description>
    </item>
    
    <item>
      <title>cuda 学习笔记 术语篇</title>
      <link>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-%e6%9c%af%e8%af%ad%e7%af%87/</link>
      <pubDate>Fri, 09 Feb 2018 07:11:54 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-%e6%9c%af%e8%af%ad%e7%af%87/</guid>
      <description>最近在学习cuda,遇到的新词汇有点多,所以开一篇记录一下. 所记录的不一定和cuda有关,只是在学习cuda中遇到的陌生概念. * **SIMD(Single Instruction Multiple Data) 单指</description>
    </item>
    
    <item>
      <title>cuda 学习笔记 术语篇</title>
      <link>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-%e6%9c%af%e8%af%ad%e7%af%87/</link>
      <pubDate>Fri, 09 Feb 2018 07:11:54 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0-%e6%9c%af%e8%af%ad%e7%af%87/</guid>
      <description>最近在学习cuda,遇到的新词汇有点多,所以开一篇记录一下. 所记录的不一定和cuda有关,只是在学习cuda中遇到的陌生概念. * **SIMD(Single Instruction Multiple Data) 单指</description>
    </item>
    
    <item>
      <title>cuda error checking 学习笔记</title>
      <link>https://111qqz.com/2018/02/cuda-error-checking-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Fri, 09 Feb 2018 06:55:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-error-checking-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>由于发现cuda c++ 的 debug方式和c++ 差别很大,因此打算再开一篇,专门记录一些和error checking 以及debug有关的内容. Error checks in CUDA code can help catch CUDA</description>
    </item>
    
    <item>
      <title>cuda error checking 学习笔记</title>
      <link>https://111qqz.com/2018/02/cuda-error-checking-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Fri, 09 Feb 2018 06:55:00 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-error-checking-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>由于发现cuda c++ 的 debug方式和c++ 差别很大,因此打算再开一篇,专门记录一些和error checking 以及debug有关的内容. Error checks in CUDA code can help catch CUDA</description>
    </item>
    
    <item>
      <title>多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法</title>
      <link>https://111qqz.com/2018/02/%e5%a4%9a%e6%a0%87%e7%ad%be%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e7%9a%84%e8%af%84%e4%bb%b7%e6%96%b9%e6%b3%95-map/</link>
      <pubDate>Fri, 09 Feb 2018 03:53:09 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%a4%9a%e6%a0%87%e7%ad%be%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e7%9a%84%e8%af%84%e4%bb%b7%e6%96%b9%e6%b3%95-map/</guid>
      <description>参考资料: 多标签图像分类任务的评价方法-mAP wiki_Sensitivity and specificity False Positives和False Negative等含义 mean average precision（MAP）在</description>
    </item>
    
    <item>
      <title>多标签图像分类任务的评价方法-mean average precision(mAP) 以及top x的评价方法</title>
      <link>https://111qqz.com/2018/02/%e5%a4%9a%e6%a0%87%e7%ad%be%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e7%9a%84%e8%af%84%e4%bb%b7%e6%96%b9%e6%b3%95-map/</link>
      <pubDate>Fri, 09 Feb 2018 03:53:09 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/%e5%a4%9a%e6%a0%87%e7%ad%be%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1%e7%9a%84%e8%af%84%e4%bb%b7%e6%96%b9%e6%b3%95-map/</guid>
      <description>参考资料: 多标签图像分类任务的评价方法-mAP wiki_Sensitivity and specificity False Positives和False Negative等含义 mean average precision（MAP）在</description>
    </item>
    
    <item>
      <title>Non-local Neural Networks 阅读笔记</title>
      <link>https://111qqz.com/2018/02/non-local-neural-networks-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Mon, 05 Feb 2018 02:24:34 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/non-local-neural-networks-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>先粗略读了2遍orz.可能不够严谨，先写一些high-level的理解。 对于序列或者图片数据，如果想获得一个long-range的依赖，通常</description>
    </item>
    
    <item>
      <title>Non-local Neural Networks 阅读笔记</title>
      <link>https://111qqz.com/2018/02/non-local-neural-networks-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Mon, 05 Feb 2018 02:24:34 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/non-local-neural-networks-%e9%98%85%e8%af%bb%e7%ac%94%e8%ae%b0/</guid>
      <description>先粗略读了2遍orz.可能不够严谨，先写一些high-level的理解。 对于序列或者图片数据，如果想获得一个long-range的依赖，通常</description>
    </item>
    
    <item>
      <title>cuda 学习笔记</title>
      <link>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 01 Feb 2018 07:20:04 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>uodate:有毒吧。kernel中出问题原来是不会报错的。。。。 请教了组里的hust学长orz..、 学到了cuda-memcheck命令和</description>
    </item>
    
    <item>
      <title>cuda 学习笔记</title>
      <link>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 01 Feb 2018 07:20:04 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/02/cuda-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>uodate:有毒吧。kernel中出问题原来是不会报错的。。。。 请教了组里的hust学长orz..、 学到了cuda-memcheck命令和</description>
    </item>
    
    <item>
      <title>non-local means algorithm 学习笔记</title>
      <link>https://111qqz.com/2018/01/non-local-means-algorithm-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 25 Jan 2018 02:53:52 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/01/non-local-means-algorithm-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>终于忙完学校的事情可以干正事了orz 这里会记录一些第一遍看paper的过程中遇到的一些影响理解的概念，不过大多不会深究，只算做粗浅的理解。 1</description>
    </item>
    
    <item>
      <title>non-local means algorithm 学习笔记</title>
      <link>https://111qqz.com/2018/01/non-local-means-algorithm-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 25 Jan 2018 02:53:52 +0000</pubDate>
      
      <guid>https://111qqz.com/2018/01/non-local-means-algorithm-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>终于忙完学校的事情可以干正事了orz 这里会记录一些第一遍看paper的过程中遇到的一些影响理解的概念，不过大多不会深究，只算做粗浅的理解。 1</description>
    </item>
    
    <item>
      <title>tmp</title>
      <link>https://111qqz.com/2017/12/tmp-2/</link>
      <pubDate>Sun, 03 Dec 2017 13:14:40 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/12/tmp-2/</guid>
      <description>import pandas as pd from sklearn.datasets import fetch_mldata mnist = fetch_mldata(&amp;ldquo;MNIST original&amp;rdquo;)
X = mnist.data / 255.0 y = mnist.target #print X.shape, y.shape feat_cols = [ &#39;pixel&#39;+str(i) for i in range(X.shape[1]) ] df = pd.DataFrame(X,columns=feat_cols) # transform into some pandas DS named DataFrame df[&#39;label&#39;] = y # add a column named &#39;label&#39;,valued by variable y df[&#39;label&#39;] = df[&#39;label&#39;].apply(lambda i: str(i)) # df.apply (some funciton) X, y = None, None import matplotlib.</description>
    </item>
    
    <item>
      <title>tmp</title>
      <link>https://111qqz.com/2017/12/tmp-2/</link>
      <pubDate>Sun, 03 Dec 2017 13:14:40 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/12/tmp-2/</guid>
      <description>import pandas as pd from sklearn.datasets import fetch_mldata mnist = fetch_mldata(&amp;ldquo;MNIST original&amp;rdquo;)
X = mnist.data / 255.0 y = mnist.target #print X.shape, y.shape feat_cols = [ &#39;pixel&#39;+str(i) for i in range(X.shape[1]) ] df = pd.DataFrame(X,columns=feat_cols) # transform into some pandas DS named DataFrame df[&#39;label&#39;] = y # add a column named &#39;label&#39;,valued by variable y df[&#39;label&#39;] = df[&#39;label&#39;].apply(lambda i: str(i)) # df.apply (some funciton) X, y = None, None import matplotlib.</description>
    </item>
    
    <item>
      <title>PCA &#43; kmeans</title>
      <link>https://111qqz.com/2017/11/pca-kmeans/</link>
      <pubDate>Sun, 26 Nov 2017 11:05:50 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/11/pca-kmeans/</guid>
      <description>先记录一下PCA实战需要用到的安装包(arch下,python2环境) python2-scikit-learn python2-numpy python2-pandas python2-matplotlib python2-seaborn pandas.DataFrame pandas 数据结构介绍 几个和科学计算数据分析有关的重要的pytho</description>
    </item>
    
    <item>
      <title>PCA &#43; kmeans</title>
      <link>https://111qqz.com/2017/11/pca-kmeans/</link>
      <pubDate>Sun, 26 Nov 2017 11:05:50 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/11/pca-kmeans/</guid>
      <description>先记录一下PCA实战需要用到的安装包(arch下,python2环境) python2-scikit-learn python2-numpy python2-pandas python2-matplotlib python2-seaborn pandas.DataFrame pandas 数据结构介绍 几个和科学计算数据分析有关的重要的pytho</description>
    </item>
    
    <item>
      <title>Difference Between Deep Learning Training and Inference</title>
      <link>https://111qqz.com/2017/09/difference-between-deep-learning-training-and-inference/</link>
      <pubDate>Mon, 04 Sep 2017 09:36:15 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/09/difference-between-deep-learning-training-and-inference/</guid>
      <description>trainning的概念比较容易懂&amp;hellip;最近天天在说inference&amp;hellip;之前学习ML/DL 相关没有见过这个东西，有点</description>
    </item>
    
    <item>
      <title>Difference Between Deep Learning Training and Inference</title>
      <link>https://111qqz.com/2017/09/difference-between-deep-learning-training-and-inference/</link>
      <pubDate>Mon, 04 Sep 2017 09:36:15 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/09/difference-between-deep-learning-training-and-inference/</guid>
      <description>trainning的概念比较容易懂&amp;hellip;最近天天在说inference&amp;hellip;之前学习ML/DL 相关没有见过这个东西，有点</description>
    </item>
    
    <item>
      <title>k-means clustering 学习笔记</title>
      <link>https://111qqz.com/2017/08/k-means-clustering-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 03 Aug 2017 13:09:17 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/k-means-clustering-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>其实这算法巨简单。。。。让我想到了均分纸牌（noip200? 还是大致说一下： 对于有 features 但是 没有 labels 的数据，没办法用监督学习，但是可以使用非监督学</description>
    </item>
    
    <item>
      <title>k-means clustering 学习笔记</title>
      <link>https://111qqz.com/2017/08/k-means-clustering-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</link>
      <pubDate>Thu, 03 Aug 2017 13:09:17 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/08/k-means-clustering-%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0/</guid>
      <description>其实这算法巨简单。。。。让我想到了均分纸牌（noip200? 还是大致说一下： 对于有 features 但是 没有 labels 的数据，没办法用监督学习，但是可以使用非监督学</description>
    </item>
    
    <item>
      <title>几种梯度下降(GD)法的比较（转载）</title>
      <link>https://111qqz.com/2017/07/%e5%87%a0%e7%a7%8d%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgd%e6%b3%95%e7%9a%84%e6%af%94%e8%be%83%ef%bc%88%e8%bd%ac%e8%bd%bd%ef%bc%89/</link>
      <pubDate>Mon, 10 Jul 2017 01:49:04 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/%e5%87%a0%e7%a7%8d%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgd%e6%b3%95%e7%9a%84%e6%af%94%e8%be%83%ef%bc%88%e8%bd%ac%e8%bd%bd%ef%bc%89/</guid>
      <description>参考资料 机器学习中梯度下降（Gradient Descent， GD）算法只需要计算损失函数的一阶导数，计算代价小，非常适合训练数据非常大的应用</description>
    </item>
    
    <item>
      <title>几种梯度下降(GD)法的比较（转载）</title>
      <link>https://111qqz.com/2017/07/%e5%87%a0%e7%a7%8d%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgd%e6%b3%95%e7%9a%84%e6%af%94%e8%be%83%ef%bc%88%e8%bd%ac%e8%bd%bd%ef%bc%89/</link>
      <pubDate>Mon, 10 Jul 2017 01:49:04 +0000</pubDate>
      
      <guid>https://111qqz.com/2017/07/%e5%87%a0%e7%a7%8d%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgd%e6%b3%95%e7%9a%84%e6%af%94%e8%be%83%ef%bc%88%e8%bd%ac%e8%bd%bd%ef%bc%89/</guid>
      <description>参考资料 机器学习中梯度下降（Gradient Descent， GD）算法只需要计算损失函数的一阶导数，计算代价小，非常适合训练数据非常大的应用</description>
    </item>
    
  </channel>
</rss>